
D:


    RL2grid:
        



        
        D: 
        cd D:\grid2op\RL2Grid\RL2Grid
        conda activate rl2grid
        
        tensorboard --logdir="D:\grid2op\RL2Grid\RL2Grid\runs"  --reload_multifile=true






        python main.py --alg PPO --exp-tag my_   --additional-timesteps 500000 --total-timesteps 100 --action-type redispatch --env-id bus14_train  --n-threads 16  --n-envs 10 --wandb-mode offline --time-limit 4000 --checkpoint True --seed 42 --cuda True --verbose True --track True --norm-obs True --use-heuristic True
        python main.py --alg PPO --resume-run-name 
        # in PPO, total-timesteps has to be larger than n-steps?
        tensorboard --logdir="D:\grid2op\RL2Grid\RL2Grid\runs\runs"  --reload_multifile=true

        python main.py --alg DQN --total-timesteps 3000000 --exp-tag LAB_offline --wandb-mode offline  --n-threads 8 --checkpoint True --action-type topology --use-heuristic True --time-limit 100300 --n-steps 20000 --update-epochs 40  --critic-lr 0.0003 --n-envs 20 --max-grad-norm 10 --gamma 0.9 --n-minibatches 1563 --seed 42 --cuda True --verbose True --track True --norm-obs True

        python test_dqn.py --checkpoint-path   D:\grid2op\RL2Grid\RL2Grid\checkpoint\DQN_bus14_my_first_run_42_0_1742976773_41243.tar     --env-id bus14 --cuda True --seed 0 --difficulty 0 --action-type  topology --env-config-path scenario.json --n-threads 4 --norm-obs True --use-heuristic False
        python main_CURRI_topo_idle.py --alg PPO --total-timesteps 400000 --exp-tag   NEW_main_CURRI_topo_idle_MINIbatches_1563_nn_256_256_reLU   --additional-timesteps 100000   --n-minibatches 1563   --action-type topology --n-threads 16  --n-envs 10 --actor-layers 256 256   --critic-layers 256 256 --actor-act-fn relu  --critic-act-fn relu  --wandb-mode offline --use-heuristic True --time-limit 4000 --n-steps 20000 --update-epochs 40 --actor-lr 0.0003 --critic-lr 0.0003   --max-grad-norm 10 --gamma 0.9  --seed 42 --cuda True --verbose True --track True --norm-obs True 
            --actor-layers 256 256   --critic-layers 256 256 --actor-act-fn relu  --critic-act-fn relu
            




        runs:



            D: 
            cd D:\grid2op\learn2learn
            python -m venv venv_learn2_test_2
            D:\grid2op\venv_learn2_test_2\Scripts\activate

            D:\grid2op\venv_learn2learn\Scripts\activate
            python.exe -m pip install --upgrade pip setuptools wheel
            





            D: 
            cd D:\grid2op\RL2Grid\RL2Grid
            conda env create -f conda_env.yml

            conda activate rl2grid_cherry_3
            
            pip install .                      # you must do this, but why?

            pip install Cython

            cd D:\grid2op\pop_nhap\learn2learn
            pip install -e .     
                # --no-build-isolation
            
            python -c "import learn2learn; print(learn2learn.__version__)"
            pip install cherry-rl 
            pip install pandapower==2.14.11

            # base: gym==0.26.2   | gym==0.14.0 failed | should be higher than gym==0.17.2?
            # for cherry-rl: 'numpy>=1.15.4', 'gym>=0.10.9', 'torch>=1.0.0', gym<=0.21.0 (not sure about this gym version)
            # for learn2learn: 'numpy>=1.15.4', 'gym>=0.14.0', 'torch>=1.1.0', 'torchvision>=0.3.0',
            # for grid2op: gym>=0.17.2
            pip install gym==0.17.3
            pip install cloudpickle==3.0.0 
            # grid2op==1.10.1 is fine for pickling of AsyncVectorEnv, 1.10.5 is not, maybe should be smaller than 1.10.3? for 1.10.5 even when i still do experimental_read_from_local_dir=True like with 1.10.1, it won't work, another BUG?

            conda activate rl2grid_cherry_3

                test:
                    python D:\grid2op\pop_nhap\learn2learn\examples\rl\promp_grid2op.py  
                    python D:\grid2op\RL2Grid\RL2Grid\main.py --alg PPO  --total-timesteps 4000000  --exp-tag TEST   --reward_fn  LineSoftMaxRootMarginRewardUpgraded RedispRewardv1    --reward_factors 1.0 0.5   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 7  --reward_param_lsmrm_n_overflow 7   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 1 --n-steps 400  --n-threads 1   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
                    python D:\grid2op\pop_nhap\learn2learn\learn2learn\gym\envs\grid2op\grid2op_env_test.py
                    python D:\grid2op\pop_nhap\learn2learn\learn2learn\gym\envs\grid2op\grid2op_direction.py

                    
                    python D:\grid2op\pop_nhap\learn2learn\examples\rl\promp.py    







            D: 
            cd D:\grid2op\pop_nhap\learn2learn
            conda activate rl2grid_cherry
        
            D: 
            cd D:\grid2op\RL2Grid\RL2Grid
            conda activate rl2grid
        
        
            D: 
            cd D:\grid2op\RL2Grid\RL2Grid
            conda activate rl2grid_2

            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag AAAA_L2RPNRewardRegularized  --eval-freq 50000  --n-eval-episodes 100 --deterministic-action False  --eval-env-id bus14_val    --reward_fn L2RPNRewardRegularized   --reward_factors 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag AAAA_nonSoftMax_n_5_n_5  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 5   --reward_param_lsmrm_n_overflow 5   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  

            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag AAA_L2RPNRewardRegularized  --eval-freq 50000  --n-eval-episodes 100 --deterministic-action False  --eval-env-id bus14_val    --reward_fn L2RPNRewardRegularized   --reward_factors 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  

            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag NNN_L2RPNRewardRegularized_train_env  --eval-freq 50000  --n-eval-episodes 100 --deterministic-action False  --eval-env-id bus14_val    --reward_fn L2RPNRewardRegularized   --reward_factors 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  


            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=5_5__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 5  --reward_param_lsmrm_n_overflow 5   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=3_3__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=7_7__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 7  --reward_param_lsmrm_n_overflow 7   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=9_9__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 9  --reward_param_lsmrm_n_overflow 9   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=11_11__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 11  --reward_param_lsmrm_n_overflow 11   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=17_17__Deter=True --eval-freq 20000 --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 17  --reward_param_lsmrm_n_overflow 17   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=5_1__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 5  --reward_param_lsmrm_n_overflow 1   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=5_3__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 5  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_M_L2RPNRewardRegularized__Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn L2RPNRewardRegularized    --reward_factors 1.0   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 5  --reward_param_lsmrm_n_overflow 5   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
            python main.py --alg PPO  --total-timesteps 2000000  --exp-tag TEST_RRR_VENV2_MyReward_n=5_5_softmax_Deter=True  --eval-freq 20000  --n-eval-episodes 10 --deterministic-action True  --eval-env-id bus14_val  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0   --reward_param_lsmrm_use_softmax True  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 5  --reward_param_lsmrm_n_overflow 5   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  




            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag NNN_Real_FullCombo_Emarche_Deter=True   --eval-freq 50000  --n-eval-episodes 100 --deterministic-action True  --eval-env-id bus14_val  --reward_fn FlatReward RedispRewardv1 OverloadReward   --reward_factors 1.0 0.5 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  


            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag NNN_FullCombo_Emarche   --reward_fn FlatReward RedispRewardv1 OverloadReward   --reward_factors 1.0 0.5 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
                python main.py --alg PPO --resume-run-name  PPO_bus14_train_NNN_FullCombo_Emarche_42_0_1748955972_7119

            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag TEST_Softmax_n7_Cost_Mix   --reward_fn  LineSoftMaxRootMarginRewardUpgraded RedispRewardv1    --reward_factors 1.0 0.5   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 7  --reward_param_lsmrm_n_overflow 7   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  

            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag TEST_Softmax_Cost_Mix   --reward_fn  LineSoftMaxRootMarginRewardUpgraded RedispRewardv1    --reward_factors 1.0 0.5   --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  

            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag TEST_FullCombo   --reward_fn FlatReward RedispRewardv1 OverloadReward   --reward_factors 1.0 0.5 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 2 --n-steps 400  --n-threads 2   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  


            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag NNN_L2RPNRewardRegularized_train_env  --reward_fn L2RPNRewardRegularized   --reward_factors 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  


            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag NNN_nonSoftMax_n_safe_3_n_overflow_3_train_env  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 3  --reward_param_lsmrm_n_overflow 3   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  

            python main.py --alg PPO  --total-timesteps 4000000  --exp-tag NNN_nonSoftMax_n_safe_7_n_overflow_7_train_env  --reward_fn LineSoftMaxRootMarginRewardUpgraded   --reward_factors 1.0  --reward_param_lsmrm_use_softmax False  --reward_param_lsmrm_temp_softmax 1.0  --reward_param_lsmrm_n_safe 7  --reward_param_lsmrm_n_overflow 7   --use-heuristic True  --heuristic_type idle   --action-type topology   --additional-timesteps 100000   --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40   --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  



            python  test_PPO.py  --checkpoint-path "D:\grid2op\RL2Grid\RL2Grid\checkpoint\PPO_bus14_train_ZZZ_main_topo_idle_bsline_BASELINE_COMPLETE_MINIbatches_40_nn_256_128_64_tanh_n_steps_400_envs_50_train_env_42_0_1747631615_13472.tar"  --num-runner-episodes 100   --runner-output-dir "./Viz_92_minibatches_40_PPO_eval"  --seed 123    --eval-env-id "bus14_val"   --env-id "bus14_train"  --action-type "topology" --difficulty 0  --cuda True --env-config-path "scenario.json" --norm-obs True --use-heuristic True





        
            python main_topo_idle_SoftmaxRootReward.py --alg PPO  --total-timesteps 4000000  --exp-tag ZZZZ_main_topo_idle_SoftmaxRootReward_Standard_BASELINE_MINIbatches_4_nn_256_128_64_tanh_n_steps_400_envs_50_train_env --additional-timesteps 100000     --n-minibatches 4  --n-envs 50 --n-steps 400  --n-threads 16   --action-type topology   --gamma 0.9 --env-id bus14_train  --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40 --use-heuristic True --heuristic-type idle --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  
                python main_topo_idle_SoftmaxRootReward.py --alg PPO --resume-run-name  PPO_bus14_train_ZZZZ_main_topo_idle_SoftmaxRootReward_Standard_BASELINE_MINIbatches_4_nn_256_128_64_tanh_n_steps_400_envs_50_train_env_42_0_1748066870_26523

                
            python main_topo_idle_SoftmaxRootReward.py --alg PPO  --total-timesteps 1500000  --exp-tag ZZZZ_main_topo_idle_SoftmaxRootReward_BASELINE_MINIbatches_4_nn_256_128_64_tanh_n_steps_400_envs_1_train_env --additional-timesteps 100000     --n-minibatches 4  --n-threads 1   --action-type topology   --gamma 0.9 --env-id bus14_train --n-envs 1 --n-steps 400 --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40 --use-heuristic True --heuristic-type idle --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  


            python main_topo_idle_rootReward.py --alg PPO  --total-timesteps 1500000  --exp-tag ZZZZ_main_topo_idle_rootReward_BASELINE_MINIbatches_4_nn_256_128_64_tanh_n_steps_400_envs_50_train_env --additional-timesteps 100000     --n-minibatches 4  --n-threads 16   --action-type topology   --gamma 0.9 --env-id bus14_train --n-envs 50 --n-steps 400 --vf-coef 0.5 --actor-lr 0.00003 --norm-adv True --norm-obs True --anneal-lr True --clip-coef 0.2 --critic-lr 0.0003    --difficulty 0 --gae-lambda 0.95   --clip-vfloss True --actor-act-fn tanh --actor-layers 256 128 64 --entropy-coef 0.01 --optimize-mem False --critic-act-fn tanh --critic-layers 512 256 256 --max-grad-norm 10   --update-epochs 40 --use-heuristic True --heuristic-type idle --env-config-path scenario.json --th-deterministic True     --wandb-mode offline --time-limit 9000 --seed 42 --cuda True --verbose True --track True  








            python  test_PPO.py  --checkpoint-path "D:\grid2op\RL2Grid\RL2Grid\checkpoint\PPO_bus14_train_ZZZ_main_topo_idle_bsline_BASELINE_COMPLETE_MINIbatches_40_nn_256_128_64_tanh_n_steps_400_envs_50_train_env_42_0_1747631615_13472.tar"  --num-runner-episodes 100   --runner-output-dir "./Viz_92_minibatches_40_PPO_eval"  --seed 123    --eval-env-id "bus14_val"   --env-id "bus14_train"  --action-type "topology" --difficulty 0  --cuda True --env-config-path "scenario.json" --norm-obs True --use-heuristic True




            python  test_PPO_do_nothing.py  --checkpoint-path "D:\grid2op\RL2Grid\RL2Grid\checkpoint\PPO_bus14_train_ZZZ_main_topo_idle_bsline_BASELINE_MINIbatches_4_nn_256_128_64_tanh_n_steps_400_envs_50_train_env_42_0_1747133018_43591.tar"  --num-runner-episodes 100    --runner-output-dir "./TEST_do_nothing_PPO_evaluation_results"  --seed 123    --eval-env-id "bus14_val"   --env-id "bus14_train"  --action-type "topology" --difficulty 0  --cuda True --env-config-path "scenario.json" --norm-obs True --use-heuristic True
            python  test_PPO.py  --checkpoint-path "D:\grid2op\RL2Grid\RL2Grid\checkpoint\PPO_bus14_train_ZZZ_main_topo_idle_bsline_BASELINE_MINIbatches_4_nn_256_128_64_tanh_n_steps_400_envs_50_train_env_42_0_1747133018_43591.tar"  --num-runner-episodes 5   --runner-output-dir "./TEST_my_PPO_evaluation_results"  --seed 123    --eval-env-id "bus14_val"   --env-id "bus14_train"  --action-type "topology" --difficulty 0  --cuda True --env-config-path "scenario.json" --norm-obs True --use-heuristic True
            python  test_dqn.py  --checkpoint-path "D:\grid2op\pop_nhap\RL2Grid_Template\RL2Grid\checkpoint\DQN_bus14_my_first_run_so4_42_0_1744825212_37510.tar"  --num-runner-episodes 5   --runner-output-dir "./TEST_my_dqn_evaluation_results"  --seed 123    --eval-env-id "bus14_val"   --env-id "bus14_train"  --action-type "topology" --difficulty 0  --cuda True --env-config-path "scenario.json" --norm-obs True --use-heuristic True
            python c:/Users/Admin/pop_nhap/RL2Grid_Template/RL2Grid/test_dqn.py --checkpoint-path "D:\grid2op\pop_nhap\RL2Grid_Template\RL2Grid\checkpoint\DQN_bus14_my_first_run_so4_42_0_1744825212_37510.tar"   --num-runner-episodes 5   --runner-output-dir "./my_dqn_evaluation_results/test"  --seed 123    --eval-env-id "bus14_val"   --env-id "bus14_train"  --action-type "topology" --difficulty 0  --cuda True --env-config-path "scenario.json" --norm-obs True --use-heuristic True 












            python stable_bs_PPO_20000.py --env_id "bus14_train" --difficulty 0 --action_type "topology" --total_timesteps 1000000 --save_path "trained_STB_BSL_models/ppo_grid2op" --n_envs 1  
            python stable_bs_PPO_2000.py --env_id "bus14_train" --difficulty 0 --action_type "topology" --total_timesteps 1000000 --save_path "trained_STB_BSL_models/ppo_grid2op" --n_envs 10 
            python stable_bs_PPO_2000.py --env_id "bus14_train" --difficulty 0 --action_type "topology" --total_timesteps 1000000 --save_path "trained_STB_BSL_models/ppo_grid2op" --n_envs 1



            python stable_bs_PPO.py --env_id "bus14_train" --difficulty 0 --action_type "topology" --total_timesteps 1000000 --save_path "trained_STB_BSL_models/ppo_grid2op" --n_envs 10 


            D: 
            cd D:\grid2op\RL2Grid\RL2Grid
            conda activate rl2grid






            D: 
            cd D:\grid2op\RL2Grid\RL2Grid
            conda activate rl2grid
            tensorboard --logdir="D:\grid2op\RL2Grid\RL2Grid\runs"




        training time: 
            10 envs, 20k steps: 7mins 
            15 envs, 20k steps: 6mins 
            20 envs, 20k steps: 6mins
            30 envs, 20k steps: 5mins
            40 envs, 20k steps: 5mins 
            50 envs, 20k steps: 

    MARL:
        D:
        cd D:\grid2op\codebase
        conda activate marlbase






    Grid2Op:

        D:
        D:\grid2op\.venv\Scripts\activate

    grid2viz

        D:
        D:\grid2op\.venv_grid2viz\Scripts\activate
        
        
        grid2viz --agents_path C:\Users\HP\Grid2Op\getting_started\agentTest --env_path "C:\Users\HP\data_grid2op\l2rpn_case14_sandbox"






cd "C:\Users\admin\Grid2Op"

venv_grid2op\Scripts\activate




--------------------------------------------------------------------

docker start -ai gridOp

c:\Users\admin\Grid2Op\venv_grid2op\Scripts\python.exe -m grid2viz.main --agents_path c:\Users\admin\Grid2Op\getting_started\path_agents

c:\Users\admin\Grid2Op\venv_grid2op\Scripts\python.exe -m grid2viz.main --agents_path C:\Users\admin\Grid2Op\getting_started\agent_Thu_nghiem                  --env_path "C:\Users\admin\data_grid2op\l2rpn_case14_sandbox_test"




pip cache purge


all requirements:
    C:\Users\HP\Grid2Op\venv_g2op\Scripts\activate
    
    pip install grid2op
    pip install jyquickhelper
    pip install matplotlib
    pip install gym
    pip install gymnasium
    pip install lightsim2grid
    pip install stable_baselines3
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
    optional: 
        pip install plotly
        pip install numba==0.48.0
    l2 baseline:
        pip install l2rpn_baselines
        pip install tensorflow==2.8.0
        pip install protobuf==3.20.0
        pip install numpy==1.23.5
    
    pip install 
    pip install 
    pip install 




rl2grid env:

    pip install pandapower==2.14.1