A Survey of Meta-Reinforcement Learning 

Jacob Beck∗ 
jacob.beck@cs.ox.ac.uk University of Oxford 
Risto Vuorio∗ 
risto.vuorio@cs.ox.ac.uk University of Oxford 

arXiv:2301.08028v3 [cs.LG] 16 Aug 2024
Evan Zheran Liu 
evanliu@cs.stanford.edu Stanford University 
Zheng Xiong rd 
Abstract 
zheng.xiong@cs.ox.ac.uk University of Oxford 
Luisa Zintgraf† 
zintgraf@deepmind.com University of Oxford 

Chelsea Finn 
cbfinn@cs.stanford.edu Stanford University 
Shimon Whiteson 
shimon.whiteson@cs.ox.ac.uk University of Oxfo

While deep reinforcement learning (RL) has fueled multiple high-profile suc cesses in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the pres ence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner. 
1 Introduction 
Meta-reinforcement learning (meta-RL) considers a family of machine learning (ML) methods that learn to reinforcement learn. That is, meta-RL methods use sample-inefficient ML to learn sample efficient RL algorithms, or components thereof. As such, meta-RL is a special case of meta-learning [307, 122, 125], with the property that the learned algorithm is an RL algorithm. Meta-RL has been investigated as a machine learning problem for a significant period of time [263, 265, 301, 264]. Intriguingly, research has also shown an analogue of meta-RL in the brain [320]. 
Meta-RL has the potential to overcome some limitations of existing human-designed RL algorithms. While there has been significant progress in deep RL over the last several years, with success stories such as mastering the game of Go [276], stratospheric balloon navigation [27], or robot locomotion in challenging terrain [199]. RL remains highly sample inefficient, which limits its real-world appli cations. Meta-RL can produce (components of) RL algorithms that are much more sample efficient than existing RL methods, or even provide solutions to previously intractable problems. 
At the same time, the promise of improved sample efficiency comes with two costs. First, meta learning requires significantly more data than standard learning, as it trains an entire learning algo- 
∗Contributed equally 
†Now at DeepMind 
rithm (often across multiple tasks). Second, meta-learning fits a learning algorithm to meta-training data, which may reduce its ability to generalize to other data. The trade-off that meta-learning offers is thus improved sample efficiency at test time, at the expense of sample efficiency during training and generality at test time. 
Example application Consider, as a conceptual example, the task of automated cooking with a robot chef. When such a robot is deployed in somebody’s kitchen, it must learn a kitchen-specific policy, since each kitchen has a different layout and appliances. This challenge is compounded by the fact that not all items needed for cooking are in plain sight; pots might be tucked away in cabinets, spices could be stored on high shelves, and utensils might be hidden in drawers. Therefore, the robot needs not only to understand the general layout but also remember where specific items are once discovered. Training the robot directly in a new kitchen from scratch is too time consuming and potentially dangerous due to random behavior early in training. One alternative is to pre-train the robot in a single training kitchen and then fine-tune it in the new kitchen. However, this approach does not take into account the subsequent fine-tuning procedure. In contrast, meta-RL would train the robot on a distribution of training kitchens such that it can adapt to any new kitchen in that distribution. This may entail learning some parameters to enable better fine-tuning, or learning the entire RL algorithm that will be deployed in the new kitchen. A robot trained this way can both make better use of the data collected and also collect better data, e.g., by focusing on the unusual or challenging features of the new kitchen. This meta-learning procedure requires more samples than the simple fine-tuning approach, but it only needs to occur once, and the resulting adaptation procedure can be significantly more sample efficient when deployed in the new test kitchen. 
This example illustrates how, in general, meta-RL may be particularly useful when the need for effi cient adaptation is frequent, so the cost of meta-training is relatively small. This includes, but is not limited to, safety-critical RL domains, where efficient data collection is necessary and exploration of novel behaviors is prohibitively costly or dangerous. In many cases, a large investment of sample inefficient learning upfront (either with oversight, in a laboratory, or in simulation) is worthwhile to enable subsequent improved adaptation behavior. This example represents an aspirational appli cation for meta-RL. In practice, meta-RL is applied to more limited robotics tasks such as robotic manipulation [3, 361] and locomotion [283]. 
Survey scope This is a survey of the meta-RL topic in machine learning and leaves out research on meta-RL in other fields such as neuroscience. Research on closely related machine learning topics is discussed in Section 2.6. To capture the breadth and depth of machine learning research on meta-RL, we surveyed the proceedings of several major machine learning conferences, as well as specialized workshops from the time period between 2017 and 2022. We found that a major portion of the meta-RL literature emerged post-2016, with the lion’s share of contributions being concentrated in three conferences: NeurIPS, ICML, and ICLR. For a full list of conferences and workshops covered, see appendix A. While our survey primarily emphasizes these conferences and the specified timeframe, we also discuss a selection of relevant papers from outside this scope. From the proceedings of these conferences and workshops, we searched for papers that explicitly mention meta-RL as well those that do not make an explicit reference but that we judged to nevertheless fit the topic. Finally, we do not claim exhaustive coverage of meta-RL research included in our survey scope but rather a holistic overview of the most salient ideas and general directions. 
Survey overview The aim of this survey is to provide an entry point to meta-RL, as well as reflect on the current state of the field and open areas of research. In Section 2, we define meta-RL and the different problem settings it can be applied to, together with two example algorithms. 
In Section 3, we consider the most prevalent problem setting in meta-RL: few-shot meta-RL. Here, the goal is to learn an RL algorithm capable of fast adaptation, i.e., learning a task within just a handful of episodes. These algorithms are often trained on a given task distribution, and meta-learn how to efficiently adapt to any task from that distribution. A toy example to illustrate this setting is shown in Figure 1. Here, an agent is meta trained to learn how to navigate to different (initially unknown) goal positions on a 2D plane. At meta-test time, this agent can adapt efficiently to new tasks with unknown goal positions. 
In Section 4, we consider many-shot settings. The goal here is to learn general-purpose RL algo rithms not specific to a narrow task distribution, similar to those currently used in practice. There are two flavors of this: training on a distribution of tasks as above, or training on a single task but meta-learning alongside standard RL training. 
2
(a) Meta-Training Tasks (b) Rollout at Meta-Test Time 
Figure 1: Example of the fast adaptation meta-RL problem setting discussed in Section 3. The agent (A) is meta-trained on a distribution of meta-training tasks to learn to go to goal position (X) located on a unit circle around its starting position (Fig 1a). At meta-test time, the agent can adapt quickly (within a handful of episodes) to new tasks with initially unknown goal positions (Fig 1b). In contrast, a standard RL algorithm may need hundreds of thousands of environment interactions when trained from scratch on one such task. 
Next, Section 5 presents some applications of meta-RL such as robotics. To conclude the survey, we discuss open problems in Section 6. These include generalization to broader task distributions for few-shot meta-RL, optimization challenges in many-shot meta-RL, and reduction of meta-training costs. 
To provide high-level summaries of all meta-RL research cited in this survey, we collect all papers discussed in each Section in a summary table presented within the Section. 
2 Background 
Meta-RL can be broadly described as learning a part or all of an RL algorithm. In this section we define and formalize meta-RL. In order to do so, we start by defining RL. 
2.1 Reinforcement learning 
An RL algorithm learns a policy to take actions in a Markov decision process (MDP), also referred to as the agent’s environment. An MDP is defined by a tuple M = ⟨S, A, P, P0, R, γ, T⟩, where S is the set of states, A the set of actions, P(st+1|st, at) : S × A × S → R+ the probability of transitioning from state st to state st+1 after taking action at, P0(s0) : S → R+ is a distribution over initial states, R(st, at) : S × A → R is a reward function, γ ∈ [0, 1] is a discount factor, and T is the horizon. We give the definitions assuming a finite horizon for simplicity, though many of the algorithms we consider also work in the infinite horizon setting. A policy is a function π(a|s) : S × A → R+ that maps states to action probabilities. The interaction of the policy with the MDP takes place in episodes, which start from initial states sampled from P0 followed by T transitions between states where the actions are sampled from the policy π and states from the dynamics P. After the T transitions, a new episode begins starting from a freshly sampled initial state. The reward for each transition is defined by the reward function rt = R(st, at). This defines a distribution over episodes 

P(τ ) = P0(s0)YT t=0 
π(at|st)P(st+1|st, at). (1) 

Sometimes we refer to the data τ = {st, at, rt, st+1}Tt=0 collected during an episode as a trajectory. The objective of RL is to learn a policy that maximizes the expected discounted return within an 

episode, 
J(π) = Eτ∼P (τ) 
"XT t=0 
γtrt 
# 
, (2) 

where rt are the rewards along the trajectory τ . In the process of optimizing this objective, multiple episodes are gathered. If H episodes have been gathered, then D = {τh}Hh=0 is all of the data collected in the MDP. An RL algorithm is a function that maps the data to a policy. This includes choosing the policies used for data collection during training. These intermediate policies are not necessarily greedy with respect to the RL objective but may take exploratory actions instead. In this 
3

Figure 2: The relationship between the inner-loop and outer-loop in a meta-RL algorithm. The pol icy parameterized by ϕiinteracts with each MDP producing trajectories τ . All of the data collected during a single trial constitutes a meta-trajectory Di. The inner-loop fθ computes adapted policy parameters for each MDP based on the meta-trajectories. The outer-loop computes updated meta parameters using all of the meta-trajectories collected in all of the trials. 
survey, we mostly consider parameterized policies with parameters ϕ ∈ Φ. Therefore, we define an RL algorithm as the function f(D) : ((S × A × R)T)H → Φ. In practice, the data may include a variable number of episodes of variable length. 
2.2 Meta-RL definition 
RL algorithms are traditionally designed, engineered, and tested by humans. The idea of meta-RL is instead to learn (parts of) an algorithm f using machine learning. Where RL learns a policy, meta-RL learns the RL algorithm f that outputs the policy. This does not remove all of the human effort from the process, but shifts it from directly designing and implementing the RL algorithms into developing the training environments and parameterizations required for learning parts of them in a data-driven way. 
Due to this bi-level structure, the algorithm for learning f is often referred to as the outer-loop, while the learned f is called the inner-loop. The relationship between the inner-loop and the outer-loop is illustrated in Figure 2. Since the inner-loop and outer-loop both perform learning, we refer to the inner-loop as performing adaptation and the outer-loop as performing meta-training, for the sake of clarity. The learned inner-loop, that is, the function f, is assessed during meta-testing. We want to meta-learn an RL algorithm, or an inner-loop, that can adapt quickly to a new MDP. This meta training requires access to a set of training MDPs. These MDPs, also known as tasks, come from a distribution denoted p(M). In principle, the task distribution can be supported by any set of tasks. However, in practice, it is common for S and A to be shared between all of the tasks and the tasks to only differ in the reward R(s, a) function, the dynamics P(s′|s, a), and initial state distributions P0(s0). Meta-training proceeds by sampling a task from the task distribution, running the inner loop on it, and optimizing the algorithm to improve the policies it produces. The interaction of the inner-loop with the task, during which the adaptation happens, is called a lifetime or a trial and is illustrated in Figure 3. A trial can consist of multiple episodes. After an episode ends, a new one begins with the initial state of the new episode sampled from the initial state distribution P0(s0). 
Parameterization We generally parameterize the inner-loop fθ with meta-parameters θ, and learn these parameters to maximize a meta-RL objective. Hence, fθ outputs the parameters of πϕ directly: ϕ = fθ(D). We refer to the policy πϕ as the base policy. Frequently, some of the policy parameters are adapted by the inner-loop, and some are meta-parameters updated only in the outer-loop: πϕ,θ. We specifically call the policy parameters adapted by the inner-loop, i.e., the output of the inner-loop (ϕ), adapted policy parameters, or the task parameters, since they are adapted to each task. Here, D is a meta-trajectory that may contain multiple episodes. We use subscripts of D to refer to indices 
4
Meta-Training 
S0 ... 

MDP 1 MDP 2 
S0 S1 S2 S0 S1 S1 S0 S1 
MDP 1 S0 
S0 S1 
Episode Episode 
Episode 
Episode Episode Episode 

Trial 1 Trial 2 
Trial 3 

Figure 3: Meta-training consists of trials (or lifetimes), each broken up into multiple episodes from a single task (MDP). In this example, each trial consists of two episodes (H = 2). 
of trajectories within the meta-trajectory. Some meta-RL algorithms use fθ to map D to ϕ at every MDP step, while others do so less frequently. 
Meta-RL objective In standard RL, a Markov policy suffices to maximize the RL objective given by Equation 2. In contrast, in meta-RL, the inner-loop is an entire RL algorithm that outputs pa rameters of policies as a function of the meta-trajectory. The performance of a meta-RL algorithm is measured in terms of the returns achieved by the policies πϕ produced by the inner-loop during a trial on tasks M drawn from the task distribution. Depending on the application, slightly different objectives are considered. For some applications, we can afford a free exploration or adaptation period, during which the performance of the policies produced by the inner-loop is not important as long as the final policy found by the inner-loop solves the task. The episodes during this phase can be used by the inner-loop for freely exploring the task. For other applications, a free exploration period is not possible and correspondingly the agent must maximize the expected return from the first timestep it interacts with the environment. Maximizing these different objectives leads to dif ferent learned exploration strategies: a free exploration period enables more risk-taking at the cost of wasted training resources when the risks are realized. Formally, the meta-RL objective for both settings is: 

J (θ) = EMi∼p(M) 
  
ED 
  X τ∈DK:H 
G(τ ) 
    fθ,Mi  , (3) 

where G(τ ) = PTt=0 γtrt is the discounted return in the MDP Mialong the trajectory τ , H is the length of the trial, or the task-horizon, and the subscript K : H in DK:H refers to the indices of the trajectories in the meta-trajectory. The exploration period is captured by K, also called the shot, which is the index of the first episode during the trial in which return counts towards the objective. K = 0 corresponds to no free exploration episodes. The episodes τ are sampled from the MDP Mi with the policy πϕ, whose parameters are produced by the inner-loop fθ(D). 
The meta-RL objective defined by Equation 3 is evaluated in expectation over samples from the task distribution, p(M). During meta-training, samples are taken from p(M), generally without assuming access to the true distribution. This results in a generalization problem in meta-testing, as testing tasks may not match the training tasks. This problem is made worse when the meta-testing task distribution does not share support with the training task distribution. Such out-of-distribution cases are often studied in meta-RL. 
2.3 POMDP Formalization 
The formal problem setting defining meta-RL, posed by Equation 3, can additionally be viewed as a special case of a partially observable Markov decision process (POMDP) [62, 126]. We specify the POMDP as a tuple of P = ⟨S, A, Ω, P, P0, R, O, γ⟩. Here, Ω is the set of observations, O is the observation function, and the rest are defined as in an MDP. Where ambiguous, we use a superscript to indicate whether the function belongs to an MDP or POMDP, e.g., RP , for a POMDP. The POMDP states, actions, and transitions function similarly to an MDP. However, unlike in an MDP, the agent does not observe the current state, sPt, which is hidden from the agent. Instead, the agent 
5
observes an observation, ot ∈ Ω, with its probability defined by O(ot|sPt, at) : Ω × SP × A → R+. The POMDP policy is then a function that maps a history of observations to action probabilities: π(a|τt) : A × Ωt → R+. 
Given a distribution of MDPs, the POMDP corresponding to the meta-RL problem has a specific structure. In particular, it maintains the MDP identity, i.e., the reward function and the dynamics distribution, in its hidden state. Therefore, the state of the POMDP is the identity of the MDP and the MDP state in it: sPt = (Mi, st). The observation function returns the state of the underlying MDP, along with the previous action, and the reward: ot = (st, at−1, rt−1). In meta-RL, the unknown MDP identity includes an unknown reward function. In order for the inner-loop to learn an optimal policy for the MDP, it requires information about the unknown reward function. Therefore, the observation includes the sampled reward here, rt−1, unlike in other observation functions for POMDPs considered outside of meta-RL. Together, the sequence of these observations produces the dataset D, needed for learning. The reward and dynamics functions of the POMDP are those of the sampled MDP stored in the hidden state. Additionally, at the terminal states of the MDP, the dynamics function of the POMDP must be constructed so that it transitions to initial states sampled from P0. A single episode in this particular type of POMDP thus consists of one entire trial, itself comprised of many episodes, in the sampled MDP. When the task-horizon, H, is reached, the POMDP episode terminates, and a new MDP is sampled at the beginning of the next POMDP episode. (This formalism can also be cast as a Contextual MDP, or CMDP [112], where the context is not shown to the agent.) This perspective leads to the following insights into the meta-RL problem. 
First, from the theory of POMDPs, we know that the optimal policy is either history-dependent or dependent on a sufficient statistic, or information state for the POMDP [287]. In the case that the inner-loop, fθ, approximates a general function of history, then the inner-loop fθ and policy πθ,ϕ, can be viewed together as a single object, forming a history-dependent policy. While such history dependent policies do not take into account the specific structure of the meta-RL problem beyond that of a more general POMDP, they are sufficient for solving the meta-RL problem. In the case that the inner-loop approximates a posterior over the hidden state for this POMDP, or a belief state [287], then the combined object can be seen as a policy dependent on approximations of a sufficient statistic. The form of these sufficient statistics can be more specific than in the general POMDP, since the hidden state has a specific form in meta-RL. In particular, one sufficient statistic for meta RL is the posterior distribution over tasks, which we discuss further in Section 3.5. This dichotomy in POMDP methods, between history-dependent and belief-dependent, leads to two different meta RL methods: black box methods, discussed in Section 3.2, and task inference methods, discussed in Section 3.3, respectively. 
Second, this perspective enables us to draw connections to Bayesian RL [63, 94]. In particular, Bayesian RL solves this POMDP by explicitly maintaining a posterior over tasks, and updating it using Bayesian inference. The Bayesian framework provides a convenient method for incorporating prior knowledge, and explicitly maintaining uncertainty. Using this framework, you can construct a new MDP, where the state includes the posterior over POMDP hidden states, or equivalently, the MDP state and a posterior over tasks. In this case, the resulting MDP is called called a Bayes adaptive Markov decision process (BAMDP). This construction allows you to learn a Markovian policy to solve the meta-RL problem, resulting in optimal exploration. BAMDPs and Bayes-optimal policies are discussed in Section 3.5. However, since Bayesian RL methods must explicitly model and update a distribution over MDPs, they are tractable only in simple domains without strong approximations. For example, even scalable Bayesian RL methods may be limited to discrete-space MDPs [103]. Instead of engineering approximations for the Bayesian posterior, meta-RL methods may learn to model these components as needed. For example, meta-RL methods only require sample access to the prior instead of the prior being explicitly known. The meta-RL agent may learn, from samples, to implicitly model the prior over tasks, as needed. 
Finally, while Meta-RL generally considers a distribution over MDPs, it is also possible to consider a distribution over POMDPs. In this case, each task is itself partially observable. This forms yet another type of POMDP, called a meta-POMDP [4]. The meta-POMDP can be written as a POMDP where only a part of the hidden state remains constant throughout a trial, and it is possible to adapt existing methods to accommodate this structure [4]. 
6

Figure 4: MAML in the problem setting (left) and conceptually (right). The meta-parameters θ are the initial parameters of the inner-loop policies ϕ0. The inner-loop computes new parameters ϕ1 for using a policy gradient algorithm. The outer-loop updates the meta-parameters to optimize the performance of the policies parameterized by ϕ1. 
2.4 Example algorithms 
We now describe two canonical meta-RL algorithms that optimize the objective given by Equation 3: Model-Agnostic Meta-Learning (MAML), which uses meta-gradients [73], and Fast RL via Slow RL (RL2), which uses a history dependent policy [62, 321]. Many meta-RL algorithms build on concepts and techniques similar to those used in MAML and RL2, which makes them excellent entry points to meta-RL. 
MAML Many designs of the inner-loop algorithm fθ build on existing RL algorithms and use meta-learning to improve them. MAML [73] is an influential design following this pattern. Its inner-loop is a policy gradient algorithm whose initial parameters are the meta-parameters ϕ0 = θ. The key insight is that such an inner-loop is a differentiable function of the initial parameters, and therefore the initialization can be optimized with gradient descent to be a good starting point for learning on tasks from the task distribution. When adapting to a new task, MAML collects data using the initial policy and computes an updated set of parameters by applying a policy gradient step for a task Mi ∼ p(M): 
ϕi1 = f(Di0, ϕ0) = ϕ0 + α∇ϕ0 Jˆ(Di0, πϕ0), 
where Jˆ(Di0, πϕ0) is an estimate of the returns of πϕ0in task Micomputed on data Di0collected using πϕ0. To update the initial parameters ϕ0 in the outer-loop, MAML collects a second batch of data Di1 using the policy πϕi1and computes the gradient of the returns of the updated policy with respect to the initial parameters: 

ϕ′0 = ϕ0 + β∇ϕ0X Mi∼p(M) 
Jˆ(Di1, πiϕ1), 

where πiϕ1is the policy for task i updated once by the inner-loop, β is a learning rate, and ∇ϕ0 Jˆ(Di1, πiϕ1) is the gradient of the returns of the updated policy computed w.r.t. the initial policy parameters. Such a gradient through an RL inner-loop is often referred to as a meta-gradient. De scending the meta-gradient corresponds to optimizing the outer-loop objective given by Equation 3. The version of MAML described here considers only a single update in the inner-loop but for harder problems, the inner-loop can compute multiple updates. The additional updates do not change the meta-gradient computation. The algorithm is illustrated in Algorithm 1 and Figure 4. 
Typically in MAML, the shot K and task-horizon H are chosen such that the outer-loop objective only considers the returns of the last policy produced by the inner-loop. In the simplest case, this would mean setting K = 1 and H = 2, i.e., using one episode for computing the inner-loop update 
7

Figure 5: RL2in the problem setting (left) and conceptually (right). The inner-loop algorithm is implemented by an RNN parameterized by the meta-parameters θ. The RNN takes as input the states, actions, and rewards from the environment. The RNN hidden state ϕi defines the task parameters at each timestep, which are passed as input to the MLP policy. The hidden state is not reset during a trial and instead carries over across episode boundaries. The outer-loop is a standard RL algorithm. 
and another for computing the outer-loop objective. To accommodate multiple updates in the inner loop and sampling multiple episodes with each policy for variance reduction, higher values of K and H are often used. 
Algorithm 1 MAML for Reinforcement Learning 
1: Initialize meta-parameters ϕ0 = θ 
2: while not done do 
3: Sample tasks Mi ∼ p(M) 
4: for each task Mi do 
5: Collect data Di0 using the initial policy πϕ0 
6: Adapt policy parameters using a policy gradient step: ϕi1 = ϕ0 + α∇ϕ0 Jˆ(Di0, πϕ0) 7: end for 
8: Update ϕ0 using the meta-gradient: ϕ′0 = ϕ0 + β∇ϕ0PMi∼p(M)Jˆ(Di1, πiϕ1). 9: end while 
RL2 Another popular approach to meta-RL is to represent the inner-loop as a policy that is de pendent on the entire history of its interaction with the environment and train it on tasks from the task distribution end-to-end with RL. This history includes all of the states, actions, and rewards the policy encounters during a trial. This results in learning an adaptive policy that changes its behavior as it gathers more information about the environment. As discussed in Section 2.3, this is the same as applying a general POMDP method to meta-RL. A similar idea has been explored for supervised learning, for example by Hochreiter et al. [121]. For meta-RL specifically Duan et al. [62] and Wang et al. [321] propose a method called RL2, where the history dependent policy is represented as a re current neural network (RNN). While in this example we focus on RNNs, any history dependent policy could be used. 
In RL2, the outer-loop objective is the expected discounted sum of rewards across the entire trial, which may consist of multiple episodes, instead of the standard RL objective that is the expected return in a single episode. This corresponds to the objective given by Equation 3 with K = 0 up to differences in the discounting: 

JRL2 (θ) = EMi∼p(M) 
  
Eτ∼p(τ|Mi,θ) 8
 X HT 
t=0 
γtrt 
  
. (4) 

To optimize this objective, RL2treats the trial as a single continuous sequence, during which the RNN hidden state is not reset, even if the trial spans multiple episodes in the underlying MDP. The meta-parameters θ are the parameters of the RNN and other neural networks used in processing the inputs and outputs of fθ. The task parameters ϕ are the ephemeral hidden states of the RNN, which may change after every timestep. The operation of the algorithm is illustrated in Algorithm 2 and Figure 5. 
Algorithm 2 RL2for Meta-Reinforcement Learning 
1: Initialize meta-parameters θ (RNN and other neural network parameters) 2: while not done do 
3: Sample tasks Mi ∼ p(M) 
4: for each task Mi do 
5: Initialize RNN hidden state ϕ0 
6: Run a continuous trial consisting of multiple episodes 
7: for each timestep t in the trial do 
8: Observe current state st, previous action at−1, and previous reward rt−1 9: Update RNN hidden state with input [st, at−1, rt−1] and parameters θ: 
ϕt = fθ(st, at−1, rt−1, ϕt−1) 
10: Sample action at ∼ πθ(·|st, ϕt) 
11: Execute action at and receive reward rt 
12: end for 
13: end for 
14: Update meta-parameters θ by optimizing the expected discounted sum of rewards across the entire trial (4). 
15: end while 
These two distinct approaches to meta-RL, MAML and RL2, each bring their unique advantages and disadvantages. On one hand, the generality of MAML’s policy gradient algorithm in its inner loop allows it, under specific conditions, to learn a policy starting from its initial state for any given task, including those outside the task distribution. On the other hand, RL2 directly approximates the optimal policy of the meta-RL objective, given by equation 3. This policy, known as Bayes-optimal, is the best policy for the task distribution and is further discussed in Section 3.5. Bayes-optimal policies always choose actions that maximize the expected return under uncertainty about the MDP identity, whereas the policy-gradient dependent MAML can only update when full episodes have been collected in the inner-loop. However, a drawback of RL2is that it faces a challenging general ization problem when tested on tasks outside the task distribution. The end-to-end RL training on a narrow task distribution may not result in a policy that generalizes well outside the task distribution. 
2.5 Problem Categories 
While the given problem setting applies to all of meta-RL, distinct clusters in the literature have emerged based on two dimensions: whether the task-horizon H is short (a few episodes) or long (hundreds of episodes or more), and whether the task distribution p(M) contains multiple tasks or just one. This creates four clusters of problems, of which three yield practical algorithms, as shown in Table 1. We illustrate how these categories differ in Figure 6. 
The first cluster in Table 1 is the few-shot multi-task setting. In this setting, an agent must quickly – within just a few episodes – adapt to a new MDP sampled from the task distribution it was trained on. This requirement captures the central idea in meta-RL that we want to use the task distribution to train agents that are capable of learning new tasks from that distribution using as few environment interactions as possible. The shots in the name of the setting echo the shots in few-shot classifica tion [313, 73, 280], where a model is trained to recognize new classes given only a few samples from each. In meta-RL, the number of exploration episodes K is more or less analogous to the shots in classification. This setting is discussed in detail in Section 3. 
While few-shot adaptation directly tackles the motivating question for meta-RL, sometimes the agent faces an adaptation problem so difficult that it is unrealistic to hope for success within a small number of episodes. This might happen, for example, when adapting to tasks that do not have support in the 
9

Few-Shot Meta-RL 
Multi-Task 
Adaptation 
Exploration Evaluation 

Goal 
Learn new tasks within a few 
Zero-Shot 
Perform well from start 
Meta-Learning 
MDP 1 
MDP 2 
...Tasks 

steps/episodes Methods: Rl2, L2RL, VariBAD 
MDP 3 

Meta-Learning 
Over a distribution of tasks 
Few-Shot 
Free exploration phase 
Methods: 
MAML, DREAM 
MDP 1 
MDP 2 
...Tasks MDP 3 
Goal 
Learn new tasks 
better than standard 
Many-Shot Meta-RL Multi-Task 
Solutions 
Adaptation 
RL algorithms Methods: 
Meta-Learning 
MDP 1 

Meta-Learning Over a more diverse distribution of tasks. I.e., more degrees of freedom 
Goal 
Accelerate standard RL algorithms 
LPG, MetaGenRL 
Single-Task Solutions 
MDP 2 
MDP 3 ... 
Meta-Learning 
Tasks 
Meta-Learning 
Over windows in a 
single task. (No reset) 
Methods: 
STACX, FRODO 
Adaptation 
Adaptation Adaptation MDP 1 

Figure 6: Illustration of the different meta-RL settings considered in this survey. Each setting is summarized in terms of its motivating goal, task distribution, and the defining features of its algo rithms. The small embedded figures on the right illustrate the adaptation over task distributions in each setting. The tasks are shown as bars depicting trials with the shot and task-horizon depicted by the lengths of the green and red bars. 
10
Multi-task Single-task 
Few-Shot RL2[62, 321], MAML [73] - 
Many-Shot LPG [220], MetaGenRL [146] STAC [355], FRODO [340] 
Table 1: Example methods for the three categories of meta-RL problems. The categorization is based on whether they consider multi-task task distributions or a single task and few or many shots. 
task distribution. In such cases, the inner-loop may require thousands of episodes or more to produce a good policy for a new task, but we would still like to use meta-learning to make the inner-loop as data-efficient as possible. We discuss this many-shot multi-task setting in Section 4. 
Meta-RL is mostly concerned with the multi-task setting, where the inner-loop can exploit similar ities between the tasks to learn a more data-efficient adaptation procedure. In contrast, in standard RL, agents are often trained to tackle a single complex task over many optimization steps. Since this is often highly data-inefficient, researchers have investigated whether meta-learning methods can improve efficiency even without access to a distribution of related tasks. In such cases, the ef ficiency gains must come from transfer within the single lifetime of the agent or from adaptation to the local training conditions during training. Methods for this many-shot single-task setting tend to resemble those in the many-shot multi-task setting and are therefore also discussed in Section 4. 
The fourth cluster is the few-shot single-task setting, where a hypothetical meta-learner would accel erate learning on a single task within a few episodes without transfer from other related tasks. The short lifetime of the agent is unlikely to leave enough time for the inner-loop to learn a data-efficient adaptation procedure and produce a policy using it. Therefore, there is to our knowledge no research targeting this setting. 
2.6 Related Work 
Related Fields The objective of this survey is to introduce, summarize, and highlight research in the field of meta-reinforcement learning. Several existing surveys [301, 122, 325, 227, 142] include some meta-RL methods, but differ in a few key ways. Thrun et al. [301] and Hospedales et al. [122] both include meta-RL methods, but focus on meta-learning more broadly, including meta-supervised learning. Wang et al. [325] survey methods for few-shot learning, but meta-RL is broader than few shot learning, since meta-RL also includes many-shot problems, and meta-RL additionally requires a task-distribution for learning in the few-shot setting. Parker-Holder et al. [227] survey methods for AutoRL, but also include non-learned algorithms for adapting to new MDPs. Kirk et al. [142] survey methods for generalization in RL, which includes meta-RL methods, but also includes both non-learned algorithms and problems that do not require adaptation, i.e., problems where a single policy can perform optimally without specialization to the given task. Of all the related machine learning fields, meta-supervised learning (meta-SL) and multi-task RL, are the most closely related. 
Meta-Supervised Learning There are many points of similarity between meta-RL and meta-SL research. Like meta-RL, meta-SL considers a distribution of tasks. However, whereas the tasks in meta-RL are defined by MDPs, the tasks in meta-SL are defined by fixed datasets. Some algorithms have been proposed as methods for both meta-SL and meta-RL [73, 201]. Moreover, many algo rithmic choices have analogues in both settings. For example, in the few-shot setting, the use of attention over prior states in meta-RL [254, 81, 298], has analogues in the kernel-based methods of supervised meta-SL [262, 313, 288]. These methods are covered later in Section 3.3. Additionally, in the many-shot setting, meta-learning optimizers and objective functions has been covered in both meta-RL [123, 146, 220, 20, 181, 154] and meta-SL [12, 164, 20]. These methods are covered in Section 4.3. Finally, imitation learning is considered a closely related pursuit to RL, since both consider sequential decision making problems. Therefore, we discuss meta-supervised learning for imitation learning in Section 3.6. 
In addition to similarities in the problem settings and methods, meta-RL and meta-SL have some key differences. In particular, meta-SL generally considers fixed datasets while meta-RL does not. In meta-RL, the adapted policy is responsible for collecting more data, which is fed back into the dataset for adaptation. This introduces a problem of data collection for adaptation to the given task. Moreover, since the meta-RL agent must adapt quickly, this requires that the agent also explores quickly. Fast exploration and the resulting challenges are unique to meta-RL and are a key theme discussed at length in Section 3. 
11
Multi-Task RL Multi-task RL, like meta-RL, considers a distribution of MDPs. However, whereas a meta-RL agent must identify the MDP that it encounters, in multi-task RL, the agent has access to a ground-truth representation of the task. While some solutions from multi-task RL, such as PopArt [119], are immediately applicable to meta-RL [101], differences in evaluation also prevent some methods from being applicable. For example, multi-task RL is often evaluated over a finite set of discrete tasks, whereas meta-RL agents often encounter new tasks at test time [299, 351]. This allows multi-task methods to train separate networks for each task [299], whereas such approaches are not immediately applicable in meta-RL. 
Generally, multi-task RL can be seen as an easier version of meta-RL, in the sense that the MDP representation is known, so there is no need to learn it from data, nor to explore to get data for adaptation. In fact, an entire category of meta-RL methods tries to explicitly infer the MDP identity in order to reduce the meta-RL problem to the easier multi-task RL problem, which is discussed in Section 3.3. However, there can also be cases in which meta-RL is possible and multi-task RL is not. Since generalization to different tasks in multi-task RL occurs without conditioning on data at test time, it is always zero-shot generalization. Such generalization may not be possible, in practice, if the task representation is not sufficiently informative. For example, if the task is represented as a one-hot categorical vector, then it may be impossible to generalize to categories not seen during training. However, while multi-task RL would fail, meta-learning may still be viable in this case, if a more informative representation of the task can be inferred from data, or if additional learning at test time can compensate for sparsity in the training distribution. 
3 Few-Shot Meta-RL 
In this section, we discuss few-shot adaptation, where the agent meta-learns across multiple tasks and at meta-test time must quickly adapt to a new, but related task in a few timesteps or episodes. 
As a concrete example, recall the robot chef learning to cook in home kitchens. Training a new policy to cook in each user’s home using reinforcement learning from scratch would require many samples in each kitchen, which can be wasteful as general cooking knowledge (e.g., how to use a stove) transfers across kitchens. Wasting many data samples in the homes of customers who purchased a robot may be unacceptable, particularly if every action the robot takes risks damaging the kitchen. Meta-RL can automatically learn a procedure, from data, for adapting to the differences that arise in new kitchens (e.g., the location of the cutlery). During meta-training, the robot may train in many different kitchens in simulation or a setting with human oversight and safety precautions. Then, during meta-testing, the robot is sold to a customer and deployed in a new kitchen, where it must quickly learn to cook in it. However, training such an agent with meta-RL involves unique challenges and design choices. 
In particular, here we summarize the literature in terms of the inner-loop, the exploration, the su pervision, and whether the RL algorithm is model-based or model-free. We categorize by the type of inner-loop, since the majority of research considers different inner-loop parameterizations. We consider exploration, since learning exploration is unique to meta-RL compared to meta-SL, as dis cussed in Section 2.6. We classify closely related problem settings, which utilize imitation learning, and some other forms of supervision, by the supervision available. Finally, we survey model-based approaches to meta-RL, since they present different trade-offs to model-free methods. However, we only discuss model-based methods briefly, as most meta-learning literature concerns the model-free setting. 
Meta-parameterization In this section, we first discuss three common categories of methods in the multi-task few-shot setting. Methods in these categories are further classified in Table 2. Recall that meta-RL itself learns a learning algorithm fθ. This places unique demands on fθ and suggests particular representations for this function. We call this design choice the meta-parameterization, with the most common ones being the following: 
• Parameterized policy gradient methods build the structure of existing policy gradient algorithms into fθ. We review these in Section 3.1. 
• Black box methods impose little to no structure on fθ. We review these in Section 3.2. 
• Task inference methods structure fθ to explicitly infer the unknown task. We review these in Section 3.3. 
12

MAML-like 
Distributional MAML 
Meta-gradient estimation 
Alternative 
Parameterized Policy Gradients 
Finn et al. [73], Li et al. [168], Park et al. [225], Vuorio et al. [315], Zintgraf et al. [368], Flennerhag et al. [77], Raghu et al. [237], Song et al. [282], Lou et al. [180], and Tack et al. [294] 
Gupta et al. [108], Yoon et al. [348], Wang et al. [326], Zou et al. [374], and Ghadirzadeh et al. [94] 
Foerster et al. [80], Al-Shedivat et al. [273], Stadie et al. [284], Farquhar et al. [71], Liu et al. [179], Mao et al. [189], Rothfuss et al. [259], Fallah et al. [70], Gao et al. [88], Fallah et al. [69], Tang et al. [297], Vuorio et al. [314], Liu et al. [175, 176], Ren et al. [245], Tack et al. [294], and Tang [296] 

outer-loop algorithms Sung et al. [289], Mendonca et al. [194], and Song et al. [282] 
Black Box 
RNN inner-loop Heess et al. [117], Duan et al. [62], Wang et al. [321], and Fakoor et al. [68] Oh et al. [219], Mishra et al. [201], Ritter et al. [255], Fortunato et al. [81], 

Attention 
AlKhamissi et al. [8], Emukpere et al. [65], Ritter et al. [253], Wang et al. [319], Melo [192], Xu et al. [339], and Team et al. [298] 

Hebbian Learning Miconi et al. [198, 197], Najarro et al. [213], Chalvidal et al. [39], and Rohani et al. [256] 
Spiking neurons Bellec et al. [26] 
Gradient updates to 
MLP Munkhdalai et al. [207] 
Q-Learning 
Outer-Loop Fakoor et al. [68] 
PPG and black-box Vuorio et al. [315], Xiong et al. [337], and Ren et al. [245] Hypernetworks Beck et al. [22] and Irie et al. [128] 
Task Inference 

Multi-task 
pre-training 
Latent trained by 
Humplik et al. [126], Kamienny et al. [137], Raileanu et al. [238], Liu et al. [177], and Peng et al. [230] 

value-function Rakelly et al. [241] and Wen et al. [328] 

Latent for reward or dynamics 
Zhou et al. [367], Raileanu et al. [238], Zintgraf et al. [372], Akuzawa et al. [4], Zhang et al. [358], Zintgraf et al. [369], Zintgraf et al. [371], Beck et al. [22], He et al. [115], Imagawa et al. [127], and Rimon et al. [252] 

Contrastive learning Guo et al. [105], Fu et al. [83], Mu et al. [206], and Choshen et al. [46] Variance reduction Luo et al. [187] 
Policy latent Raileanu et al. [238] 
Q-Learning 
outer-loop Liu et al. [177] and Zhang et al. [357] Sub-Tasks Sohn et al. [281] and Zhang et al. [357] 

Permutation invariance 
Rakelly et al. [241], Korshunova et al. [147], Raileanu et al. [238], Imagawa et al. [127], and Beck et al. [23] 

Test-time fine-tuning Lan et al. [153], Xiong et al. [337], and Imagawa et al. [127] Human preferences Ren et al. [246] 
Known Dynamics Lee et al. [158] 
Hypernetworks Peng et al. [230] and Beck et al. [22, 24] 
Table 2: Few-shot meta-RL research categorized by method. The majority of methods fall into one of three clusters: Parameterized policy gradient, black box, or task inference. These categories de termine how the inner-loop is parameterized. Within each cluster, we further categorize the methods. Explanations of these methods can be found in Sections 3.1, 3.2, and 3.3, respectively. 
13
While each of these categories represents a discrete cluster of research, other authors cluster the research diffently and may use different names to refer to these clusters. For example, parameter ized policy gradient methods are sometimes referred to as gradient-based methods [73, 241], or are referred to as part of a larger category of methods in meta-SL called optimization-based meth ods [122]. Additionally, black box methods and task-inference methods are sometimes referred to collectively as context-based methods [241]. 
Along with the parameterization of the inner-loop, related design choices include the representation of the base policy and the choice of the outer-loop algorithm. Each method must additionally spec ify which base policy parameters are adapted by the inner-loop, and which are meta-parameters. We call this design choice the adapted policy parameters. For each of the three types of meta parameterizations in this section, we discuss the inner-loop, the outer-loop, and the adapted policy parameters. 
Exploration While all these methods are distinct, they also share some challenges. One such challenge is that of exploration, the process of collecting data for adaptation. In few-shot learning, exploration determines how an agent takes actions during its few shots. Subsequently, an agent must decide how to adapt the base policy using this collected data. For the adaptation to be sample effi cient, the exploration must be efficient as well. Specifically, the exploration must target differences in the task distribution (e.g., different locations of cutlery). In Section 3.4, we discuss the process of exploration, along with ways to add structure to support it. While all few-shot methods must learn to explore an unknown MDP, there is an especially tight relationship between exploration methods and task inference methods. In general, exploration may be used to enable better task inference, and conversely, task inference may be used to enable better exploration. One particular way in which task inference may be used to enable better exploration is by quantifying uncertainty about the task, and then choosing actions based on that quantification. This method may be used in order to learn optimal exploration. We discuss this in Section 3.5. 
Supervision Meta-RL methods also differ in the assumptions that they make about the available supervision. In the standard meta-RL problem setting, rewards are available during both meta training and meta-testing. However, providing rewards in each phase presents challenges. For ex ample, it may be difficult to manually design an informative task distribution for meta-training, and it may be impractical to measure rewards with expensive sensors during deployment for meta testing. In this case, unique methods must be used, such as automatically designing rewards for the outer-loop, or creating an inner-loop that does not need to condition on rewards. Alternatively, super vision that is more informative than rewards can be provided. We review such settings, challenges, and methods in Section 3.6. 
Model-Based Meta-RL Some meta-RL methods explicitly learn a model of the MDP dynamics and reward function. Such methods are called model-based methods, in contrast to model-free methods that do not explicitly learn a model of the environment. Model-based methods confer advantages such as sample-efficient and off-policy meta-training. Moreover, using an off-the-shelf planning algorithm with the learned model can be easier for some task distributions than learning a complicated policy directly. However, model-based methods generally require the implementation of additional components and can have lower asymptotic performance. We discuss these trade offs and the applications of model-based RL to meta-RL in Section 3.7. We keep discussion in this section brief, since most meta-RL literature considers mode-free RL, the relevant trade-offs are similar in meta-RL and RL at large, and most model-based meta-RL methods have an analogous model-free method that will have already been discussed by that point. 
Theory of Meta-RL Finally, we consider theoretical research on meta-RL. While meta-RL is a relatively new area of research, several studies have found interesting theoretical challenges in it. These insights relate well to the empirical findings, which constitute the majority of the research in meta-RL. In Section 3.8, we survey important theoretical works in meta-RL. We believe there is a lot more to explore in the theory of meta-RL and therefore hope this survey can motivate future research. 
3.1 Parameterized Policy Gradient Methods 
Meta-RL learns a learning algorithm fθ, the inner-loop. This inner-loop is learned to maximize the meta-RL objective given by Equation 3, over samples from a task distribution, p(M). However, during training, we generally only assume access to samples from this distribution. These samples 
14
may be limited, and the distribution over which we want to generalize may be broad. Moreover, the distribution on which the meta-RL is evaluated for meta-testing may differ from the distribution on which the meta-RL agents is meta-trained. Each of these motivates the need for learning to take place in the inner-loop algorithm fθ. This section discusses methods for building such structure into the inner-loop itself. 
We call the parameterization of fθ the meta-parameterization. In this section, we discuss one way of parameterizing the inner-loop that builds in the structure of existing standard RL algorithms. Pa rameterized policy gradients (PPG) are a common class of methods which parameterize the learning algorithm fθ as a policy gradient algorithm. These algorithms generally have an inner-loop of the 

form 
ϕj+1 = fθ(Dj , ϕj ) = ϕj + αθ∇ϕj Jˆθ(Dj , πϕj), 

where Jˆθ(Dj , πϕj) is an estimate of the returns of the policy πϕj. For example, recall the MAML algorithm discussed in Section 2.4. In MAML, θ = ϕ0, and so the initialization is the meta-learned component. It is also possible to add additional pre-defined components to the inner-loop, such as sparsity-inducing regularization [180]. In general, whatever structure is not predefined, is a param eter in θ that is meta-learned. In addition to initialization, the meta-learned structure can include components such as hyper-parameters [168]. Some methods also meta-learn a preconditioning ma trix to approximate the curvature of the objective, inspired by second-order optimization methods. These methods generally have the form ϕj+1 = ϕj + αθMθ∇ϕj Jˆθ(Dj , πϕj) [225, 77]. While a value based-method could be used to parameterize the inner-loop instead of a policy gradient [373], value based-methods generally require many more steps to propagate reward information [202], and so are typically reserved for the many-shot setting, discussed in Section 4. In this section, we focus on PPG methods. We begin by discussing different parameters of the base policy that the inner-loop may adapt. Then, we discuss options for outer-loop algorithms and optimization. We conclude with a discussion of the trade-offs associated with PPG methods. 
Adapted policy parameters PPG algorithms commonly meta-learn an initialization, and then adapt that initialization in the inner-loop. Instead of adapting a single initialization, several PPG methods learn a full distribution over initial policy parameters, p(ϕ0) [108, 348, 326, 374, 94]. This distribution allows for modeling uncertainty over policies. The distribution over initial parameters can be represented with a finite number of discrete particles [348], or with a Gaussian approximation fit via variational inference [108, 94]. Moreover, the distribution itself can be updated in the inner loop, to obtain a posterior over (a subset of) model parameters [108, 348]. The updated distribution may be useful both for modeling uncertainty [348], and for temporally extended exploration, if policy parameters are resampled periodically [108]. 
Alternately, some PPG methods adapt far fewer parameters in the inner-loop. Instead of adapting all policy parameters, they adapt a subset [368, 237]. For example, one method adapts only the weights and biases of the last layer of the policy [237], while leaving the rest of the parameters constant throughout the inner-loop. Another method adapts only a vector, called the context vector, on which the policy is conditioned [368]. In this case, the input to the policy itself parameterizes the range of possible behavior. We write the policy as πθ(a|s, ϕ), where ϕ is the adapted context vector. The weights and biases of the policy, as well as the initial context vector, constitute the meta-parameters that are constant in the inner-loop. We visualize the use of a context vector in Figure 7. 
Consider the algorithm Context Adaptation via Meta-learning (CAVIA) [368]. CAVIA is similar to MAML but it only adapts a context vector, which is initialized to the zero vector: 
ϕ0 = 0, 
ϕj+1 = ϕj + αθ∇ϕj Jˆθ(Dj , πθ(·|ϕj )). 
While the update to ϕ is the same as in MAML, here, ϕ represents a vector passed as an input to the policy network, while all the weights and biases of the network remain fixed throughout adaptation. One benefit of adapting a subset of parameters in the inner-loop is that it may mitigate overfitting in the inner-loop, for task distributions where only a small amount of adaption is needed [368]. 
Meta-gradient estimation in outer-loop optimization Effective learning in the outer-loop of PPG algorithms requires access to estimates of meta-gradients, i.e., gradients of the outer-loop ob jective with respect to the meta-parameters. Most commonly, the meta-gradients are computed by directly optimizing objective 3 with a policy gradient algorithm. However, naively applying a policy 
15

Figure 7: Illustration of meta-RL without (left) and with (right) a context vector. The context vector can be updated with backpropagation in a PPG method or by a neural network in a black box method. When using a context vector, some of the policy parameters are not adapted by the inner-loop, and are instead meta-parameters set by the outer-loop. 
gradient method in the outer-loop can lead to a suboptimal bias-variance trade-off. Significant re search has considered how to improve this trade-off when estimating meta-gradients for PPG meth ods [80, 284, 179, 189, 259, 70, 69, 297, 314, 175, 294, 296]. Next, we discuss the bias-variance trade-offs in meta-gradient estimation for PPG methods and the algorithms arising from choosing different points in the trade-off. 
The unbiased gradient of the meta-RL objective given by Equation 3 with respect to the meta parameters θ can be computed as follows. Since the distribution of tasks does not depend on the meta-parameters, we can estimate the meta-gradient with respect to each task separately and then combine. The expression for the meta-gradient for a single task is given by 

∇θED 
  X τ∈DK:H 
G(τ ) 
    fθ = ∇θXH k=K 
Z 
G(τk)Yk i=0 
p(τi; ϕi)dτi. 

Here, we take into account that the returns from previous trajectories are not influenced by the policies of future trajectories. Therefore, we restricted the product of trajectory probabilities to range only from i = 0 to k. Since the probabilities of the trajectories depend on θ, we use the log-derivative trick to get the gradient of the product 

=XH k=K 
Z 
G(τk)Yk i=0 
p(τi; ϕi)∇θ logYk i=0 
p(τi; ϕi)dτi. 

Finally, we use the chain rule to get the meta-gradient: 

=XH k=K 
Z 
G(τk)Yk i=0 
p(τi; ϕi)Xk j=0 
∇θϕj∇ϕj log p(τj ; ϕj )dτi 

Naively applying a policy gradient algorithm to the returns PHk=K G(τk) does not compute this meta-gradient. To see why, we reorganize the terms to get 

=XH k=K 
ED 
  
G(τk) 
  
∇θϕk∇ϕklog p(τ ; ϕk) + 
kX−1 j=0 
∇θϕj∇ϕj log p(τj ; ϕj ) 
  
, 

where the first gradient term is the regular policy gradient on the kth trajectory w.r.t. the meta parameters, and the gradient terms in the sum are the gradients of the policies on the jth trajectories for 0 ≤ j ≤ k − 1 w.r.t. the meta-parameters. These latter terms, sometimes called sampling correction terms, are often omitted in practice, yielding biased meta-gradient estimates [273, 284]. 
Recall that PPG methods produce a sequence of policies, and generally optimize Equation 3 with K = H − n, where n is the number of episodes collected by the final policy. Actions taken by any one policy affect the data seen by the next, and thus these actions affect the expected return of the later policies. The sampling-correction terms account for this dependence. Ignoring them 
16
amounts to ignoring actions from earlier policies when training the inner-loop to optimize the meta RL objective and thus introduces bias to the meta-gradient estimation [273]. This bias can sometimes be detrimental to the meta-learning performance but the high variance of the unbiased estimator often dominates [284, 70, 314]. 
Significant research has considered a meta-gradient estimator derived originally for computing higher-order meta-gradients [80, 71, 179, 189, 259, 297, 176, 296]. In practice, PPG algorithms use a sample-based policy gradient algorithm for updating the policy parameters in the inner-loop. The higher-order meta-gradients of the sample-based inner-loop are different from those of an inner loop that uses the expected policy gradient. Foerster et al. [80] and Rothfuss et al. [259] argue that the higher-order meta-gradients of the sample-based inner-loop should approximate those of the ex pected inner-loop and derive alternative sample-based inner-loop update functions for that purpose. While these meta-gradient estimators are still biased, their variance has a more benign dependence on the inner-loop sample size than the unbiased meta-gradient estimator and therefore can achieve a better point in the bias-variance trade-off than either the naive or the unbiased meta-gradient esti mators [297, 176]. To further reduce the variance of this class of meta-gradient estimators, [71, 179, 189, 259] propose to ignore certain high-variance terms in the estimator and introduce baselines. 
Alternatively, some PPG methods do not require higher-order meta-gradients. For example, [73] use a first-order approximation of the meta-gradient, whereas [282] use gradient-free optimization. (See Nichol et al. [217] for another first-order approximation proposed for supervised meta-learning, and recently used in meta-RL [245].) Furthermore, when meta-learning an initialization as in MAML, for a limited number of tasks or limited amount of data in the inner-loop, it may even be preferable to set the inner-loop to take no steps at all during meta-training, i.e., without adapting to each task [88]. In this case, task adaptation occurs only through fine-tuning at test-time. The lack of explicit meta-learning can be seen as a limitation of the meta-learning approach and is discussed further in Section 6. Additionally, it is possible to use a value-based algorithm in the outer-loop, instead of a policy gradient algorithm, which avoids meta-gradients altogether [289]. 
Outer-loop algorithms While most PPG methods use a policy-gradient algorithm in the outer loop, other alternatives are possible [289, 194]. For example, one can train a critic, Qθ(s, a, D), using-TD error in the outer-loop, then reuse this critic in the inner-loop [289]. Alternatively, instead of estimating meta-gradients via backpropagation, evolution strategies (ES) [244, 329, 261] may be used [282]. Additionally, one can train task-specific experts and then use these for imitation learn ing in the outer-loop. While neither directly learn exploratory behavior by optimizing Equation 3, they can work in practice. We provide a more complete discussion of outer-loop supervision in Section 3.6. 
PPG trade-offs One of the primary benefits of PPG algorithms is that they produce an inner-loop that converges to a locally optimal policy, even when relatively few samples are available for meta training or when the task distribution differs from meta-training to meta-testing. PPG inner-loops are generally guaranteed to converge under the same assumptions as standard policy gradient methods. For example, the MAML inner-loop converges with the same guarantees as REINFORCE [330], since it simply runs REINFORCE from a meta-learned initialization. Even convergence bounds are possible [69]. However, as with any policy gradient algorithm that uses estimated gradients, the guarantees given by REINFORCE are rather weak, and in some cases a step of REINFORCE can even make the policy worse on the task [55]. Having a learning algorithm that eventually adapts to a novel task is desirable, since it reduces the dependence on seeing many relevant tasks during meta-training. 
Although parameterizing fθ as a policy gradient method may ensure that adaptation generalizes, this structure also presents a trade-off. Typically the inner-loop policy gradient has high variance and requires a value estimate covering the full episode, so estimating the gradient generally requires many episodes. Hence, PPG methods are generally not well-suited to few-shot problems that require stable adaption at every timestep or within very few episodes in the inner-loop. Moreover, PPG methods are often sample-inefficient during meta-training as well, because the outer-loop generally relies on on-policy evaluation, rather than an off-policy method that can reuse data efficiently. 
In general, there is a trade-off between generalization to novel tasks and specialization over a given task distribution. How much structure is imposed by the parameterization of fθ determines where each algorithm lies on this spectrum. The structure of PPG methods places them near the end of the spectrum that ensures generalization. This is visualized in Figure 8. While this spectrum 
17
PPG Method Black Box Method 

Generalization Specialization 
Generalization Specialization 

MAML RL2 Structure Structure 

Inductive bias in structure 
Inductive bias from data 

Figure 8: The meta-parameterization spectrum. On the left are methods like MAML that hard-code the structure of policy gradients into the inner-loop. Such methods generalize better. On the right are black box methods, which learn all of the structure of the inner-loop from data. Such methods can specialize to the task distribution better. 
summarizes current methods, the trade-off is not necessarily inherent to the problem setting, and future work could investigate methods that achieve the best of both worlds. In the next section, we discuss methods at the other end of the spectrum. 
3.2 Black Box Methods 
In contrast to PPG methods, black box methods are near the other end of the spectrum. In principle, they can learn any arbitrary learning procedure, since they represent fθ with a neural network as a universal function approximator. This places fewer constraints on the function fθ than with a PPG method. Since fθ represents an arbitrary function of history, the modified policy can represent an arbitrary history-dependent policy. As discussed in Section 2, such a policy is sufficient for learning an optimal policy for a POMDP, and thus for the meta-RL problem as well. Recall the RL2 algorithm [62] from Section 2.4. In this case, fθ is represented by a recurrent neural network, whose outputs are an input vector to the base policy. While the use of a recurrent network to output a context vector is common, black box methods may also structure the base policy and inner-loop in other ways. In this section we survey black box methods. We begin by discussing architectures that are designed for different amounts of diversity in the task distribution. Each of these architectures has different adapted policy parameters. Then, we discuss distinct ways to structure the inner-loop and alternative outer-loop algorithms. Finally, we conclude with a discussion of the trade-offs associated with black box methods. 
Adapted policy parameters On one hand, some task distributions may require little adaptation for each task, as discussed in Section 3.1. For example, if a navigation task varies only by goal location, then it may be that not many policy parameters need to be adapted. Many black box methods have been applied to such task distributions [321, 126, 372]. In such a setting, it is sufficient to only adapt a vector, used as an input to the base policy, instead of all parameters of the base policy [62, 321, 126, 368, 68, 372, 177]. This vector, ϕ, is called a context vector, just as in PPG methods. This distinction is visualized in Figure 7. The context is produced by a recurrent neural network, or any other network that conditions on history, such as a transformer or memory-augmented network. Using a recurrent neural network, the inner-loop can be written fθ(D) = RNNθ(D). In this case, the inner-loop (fθ) and base policy (πθ(a|s, ϕ)) together, can also be seen as a single object, forming a history-dependent policy. This is a common architecture for few-shot adaptation methods [62, 321, 126, 68, 372, 177]. 
On the other hand, some task distributions may require significant differences in behavior between the optimal policies. By conditioning a policy on a context vector, all of the weights and biases of π must generalize between all tasks. However, when significantly distinct policies are required for dif ferent tasks, forcing base policy parameters to be shared may impede adaptation [22]. Alternatively, just as in PPG methods, there may be no context vector, but the inner-loop may adapt the weights and biases of the base policy, directly [22]. The weights and biases here are the task parameters, ϕ, output by the inner-loop. The inner-loop may produce all of the parameters of a feed-forward base policy [22], or may modulate the weights of a recurrent base learner [77]. In these cases, using one network to map all data in the trial, Dj , to weights and biases of another network defines a 
18

φ 
πφ(a|s) s 
πθ(a|s,φ) s φ 

Figure 9: Illustration of black box meta-RL with hypernetwork (left) and with a context vector (right). When the inner-loop is a recurrent neural network, generating all the parameters of the policy, including its weights and biases, results in a hypernetwork. Alternatively, the RNN can output a context vector on which the policy is conditioned. 
hypernetwork [110]. The architecture can be written as πϕ(a|s), where ϕ = hθ(RNNθ(D)), h is a hypernetwork, and ϕ are the weights and biases of the policy. See Figure 9 for an illustration. 
Inner-loop representation While many black box methods simply use recurrent neural networks as the inner-loop representation, [117, 62, 321, 68], alternative representations have also been in vestigated. These representations include networks of spiking neurons [26], a type of biologically inspired neuron; Hebbian learning [198, 197, 213, 39, 256], another type of biologically inspired learning covered in Section 3.6; networks using attention and convolution [201]; recurrent hyper networks that update their own weight matrices [128]; storage of memories in the weights of a feed forward network [207]; the external storage of memory states when tasks change in a non-stationary environment [255, 8]; and the use of differentiable neural dictionaries [235] and successor features with generalized policy improvement [18] for meta-training [65]. However, most alternative repre sentations use some form of attention [98, 308] to parameterize the inner-loop [219, 201, 255, 81, 65, 253, 319, 192, 339, 298]. 
Attention can be thought of as a soft form of a key-value lookup in a dictionary. Specifically, it is a mechanism for combining different vectors based on the similarity of their associated key vectors to a given query vector. Given a query vector, q, a matrix, V , where each row is one of n value vectors over which to attend, and a matrix K, where each row is one of n key vectors, then attention can be written 
i=n 

Attention(q, K, V ) = X i=0 
wi(q, Ki)Vi 

where w ∈ ∆n is a weight vector defining the convex combination of value vectors [98]. Gener ally, attention computes the weights as w = softmax(Kq), leading to an attention mechanism that simplifies to 
Attention(q, K, V ) = VTsoftmax(Kq) = (softmax(qT KT)V )T. 
Additionally, computing multiple queries, with each as a row vector in Q, we can write this as Attention(Q, K, V ) = softmax(QKT)V. 
Some methods combine attention with convolution [201], or use attention over past recurrent states [219, 255, 81, 298], while others use self-attention alone [253, 319, 298]. For example, to attend to past recurrent states, q may be a function of the current hidden state of a recurrent network, while K and V may be (two different linear projections of) all prior hidden states computed over D. A generalization of such methods can be seen in Figure 10. In contrast, self-attention models Q, K, and V all as linear projections of the inputs in D first, then as linear projections of the previous attention layer, for multiple attention layers in a row. 
19
softmax 
... ... 
Figure 10: Attention over past recurrent states in the inner-loop of black box methods. One approach is to use the current hidden state as a query in attention over past recurrent states. The keys (K) and values (V ) are all linear projections of the past recurrent states. The output, ϕ, is a a convex combination of the projected hidden states (V ), where the combination is specified by this weight vector (w), computed from the similarity between the keys (K) and query (q). Components in green are optional. In order to provide long-term context to the RNN, the output from attention over past hidden states, VT w, can be passed as an input to the RNN at the next timestep. Additionally, the output, ϕ, can be VT w, h, or a concatenation of both. Passing the RNN state, h, as an output, with VT w as an input, allows multiple steps of attention to be integrated into the final output. 
Attention mechanisms seem to aid in generalization to novel tasks outside of the distribution, p(M), [81, 192] and self-attention may be useful for complex planning [253]. Still, attention is computa tionally expensive: whereas recurrent networks use O(1) memory and compute per timestep, atten tion generally requires O(t2) memory and compute per timestep t, which may cross many episodes. While fast approximations of attention exist [138], solutions in meta-RL often simply maintain only memory of a fixed number of the most recent transitions [201, 81]. Nonetheless, both transform ers, which use self-attention, and recurrent neural networks can still struggle to meta-learn simple inductive biases, particularly for complex task distributions generated by simple underlying rules in low-dimensional spaces [149, 40]. 
Outer-loop algorithms While many black box methods use on-policy algorithms in the outer loop [62, 321, 372], it is straightforward to use off-policy algorithms [241, 68, 177], which bring increased sample efficiency to RL. For discrete actions, it is also straightforward to use Q-learning [68, 177, 357], in which case the inner-loop must change as well. In this case, the inner-loop estimates Q-values instead of directly parameterizing a policy. The policy can then act greedily with respect to these Q-values at meta-test time. This approach can be thought of as modifying recurrent Q-networks [114] to fit the meta-RL setting, which compares favorably compared to other state-of-the-art meta-RL methods [68]. 
Black box trade-offs One key benefit of black box methods is that they can rapidly alter their policies in response to new information, whereas PPG methods generally require multiple episodes of experience to get a sufficiently precise inner-loop gradient estimate. For example, consider an agent that must learn which objects in a kitchen are hot at meta-test time. While estimating a policy gradient, a PPG method may touch a hot stove multiple times before learning not to. In contrast, a black box method may produce an adaptation procedure that never touches a hot surface more than once. Black box methods can learn such responsive adaptation procedures because they represent the inner-loop as an arbitrary function that maps from the cumulative task experience to the next action. 
20
However, black-box methods also present a trade-off. While black-box methods can tightly fit their assumptions about adaptation to a narrow distribution of data, p(M), increasing specialization, they often struggle to generalize outside of p(M) [321, 74, 81, 337]. Consider the robot chef: while it may learn to not touch hot surfaces, it is unlikely a black box robot chef will learn a completely new skill, such as how to use a stove, if it has never seen a stove during meta-training. In contrast, a PPG method could still learn such a skill at meta-test time with sufficient data. While there have been efforts to both broaden the set of meta-training tasks and manually add inductive biases to black-box methods for generalization, which we discuss in Section 4, whether to use a black-box method or PPG method generally depends on the amount of generalization and specialization in the problem to be solved. 
Additionally, there exist trade-offs in outer-loop optimization challenges between PPG and black box methods. On one hand, as discussed in Section 3.1, PPG methods often estimate a meta-gradient, which is difficult to compute [273], especially for long horizons [334]. On the other hand, black box methods do not have the structure of optimization methods build into them, so they can be harder to train from scratch, and have associated outer-loop optimization challenges even for short horizons. Black box methods generally make use of recurrent neural networks, which can suffer from vanish ing and exploding gradients [228]. Moreover, the optimization of recurrent neural networks can be especially difficult in reinforcement learning [21], while transformers can be even more problematic to train [224, 192]. While large transformers have shown some notable success in meta-RL, such solutions require the use of curriculum learning and distillation to train stably [298]. Thus, the rapid updates of black box methods in meta-RL, while enabling fast learning, also present challenges for meta-learning. 
Some methods use both PPG and black-box components [315, 337, 245]. In particular, even when training a fully black-box method, the policy or inner-loop can be fine-tuned with policy gradients at meta-test time [153, 337, 127]. 
3.3 Task Inference Methods 
Closely related to black box methods are task inference methods, which often share the same pa rameterization as black box methods and thus can be considered a subset of them. However, pa rameterizations of the inner-loop may be specific to task inference methods [241, 147, 372], which generally train the inner-loop to perform a different function by optimizing a different objective. 
Task inference methods generally aim to identify the MDP, or task, to which the agent must adapt, in the inner-loop. In meta-RL, the agent must repeatedly adapt to an unknown MDP whose representa tion is not given as input to the inner-loop. While the agent ultimately acts to maximize reward, the entire purpose of the inner-loop can be described as identifying the task. The agent’s belief about what it should do can be represented as a distribution over tasks. As discussed in Section 2, this posterior distribution constitutes a sufficient statistic, or information state [287], for the meta-RL problem. Since we already know the form of this sufficient statistic, the inner-loop can model it di rectly, instead of learning a mapping from history to action end-to-end. Task inference is the process of inferring this posterior distribution over tasks, conditional on what the agent has seen so far. 
Consider the case where the agent has uniquely identified the task. Then, at this point, the agent knows the MDP and could in fact use classical planning techniques, such as value iteration, to com pute the optimal policy directly. In this scenario, no further learning or data collection is required. More practically, if the task distribution is reasonably small and finite, we can avoid even having to explicitly plan, by learning a mapping from the task to the optimal policy directly, during meta training. In fact, training a policy over a distribution of tasks, with the policy conditioned on the true task, can be taken as the definition of multi-task RL [351]. In the multi-task case, a mapping is learned from a known task to a policy. In meta-RL the only difference is that the task is not known. Thus task inference can be seen as an attempt to move a meta-learning problem into the easier multi-task setting. 
When uncertainty remains in the distribution, instead of mapping a task to a base policy, task infer ence methods generally map a task distribution, given the current data, to a base policy. This can be seen as learning a policy conditioned on a (partially) inferred task. In this case, learning becomes the process of reducing uncertainty about the task. The agent must collect data that enables it to identify the task. That is, the agent mustexplore to reduce uncertainty in the posterior given by task inference. Task inference is therefore a useful way to frame exploration, and many task inference 
21

Figure 11: Illustration of normal meta-RL (left), task inference using privileged information (mid dle), task inference using multi-task training (right). Solid arrows represent forward propagation, and dashed arrows represent backpropagation. Task inference with privileged information uses a true task representation, c, and then backpropagates that inference to train ϕ. Task inference with multi-task training learns an encoding of the task, g(c), to maximize the return of an informed pol icy, πmulti, then uses inference of this learned encoding to train ϕ. 
methods are framed as tools for exploration, which we discuss in Section 3.4. Optimally handling uncertainty in the task distribution is difficult, and is discussed in Section 3.5. 
In this section we discuss two methods for task inference that use supervised learning but also require assumptions about the information available for meta-training. We then discuss alternative methods without such assumptions, and how the inner-loops are usually represented. Finally, we conclude with a discussion of the trade-offs concerning task inference methods. Table 2 summarizes these categories and task inference methods. 
Task inference with privileged information A straightforward method for inferring the task is to add a supervised loss so that a black box fθ predicts some estimate of the task, cˆM, given some known representation of the task, cM [126]. For example, a recurrent network may predict the task representation conditional on all data collected so far. Recall that ϕ, the task parameters, are the adapted policy parameters output by the inner-loop. Most commonly, ϕ is passed directly to the policy as an input vector: πθ(a|s, ϕ). In task inference, the vector ϕ is generally the predicted task estimate: πθ(a|s, ϕ = ˆcM). For computing the supervised loss, this task representation must be known during meta-training, and so constitutes a form of privileged information. The representation may, for instance, be a one-hot representation of a task index, if the task distribution is discrete and finite. Or, it may be some parameters defining the MDP. For example, if kitchens differ in the location of the stove and refrigerator, the task representation, cM, could be a vector of all these coordinates. In this case, fθ would predict these coordinates. The representation may even contain sub-tasks and their hierarchies, or human preferences [246], when such information is known [281, 357], or make use of known transition functions to explicitly approximate a Bayesian posterior over the tasks [158]. Commonly, the task-inference objective, denoted here as Jinfer, is given by the maximum likelihood estimate: 
Jinfer(θ) = EM[ED|π[log pθ(cM)]]. 
When passing the task to the policy, there are a few important representation choices. First, instead of conditioning the policy on the task representation directly, we can pass a representation with more information about task uncertainty. This can be accomplished, for instance, by passing to the policy the penultimate layer when predicting cˆM [126]. Then the task parameters, ϕ, passed to the policy, are trained by inferring cˆM, but only after a subsequent linear transformation: cˆM = Lθ(ϕ). This hidden layer may contain more information than cˆM, and cˆM can always be computed from it by the policy. Second, it can be useful to add a stop-gradient to ϕ before passing it to the policy [126, 371], to prevent conflicting gradients. Finally, when training fθ with Jinfer, as a supervised objective, the dataset D may contain off-policy data without bias, which may given a particular advantage in the off-policy setting, as compared to Jmeta given by Equation 3 [126]. 
Task inference with multi-task training Some research uses the multi-task setting to improve task inference with privileged information [126, 137, 238, 177, 230]. The task representation may contain little task-specific information (e.g., if it is one-hot representation) or task-specific informa 
22
tion that is irrelevant to the policy (e.g the amount of oxygen in the kitchen). For example, consider the more concrete task of navigation to a goal on the perimeter of a circle, as discussed in Section 1. In this case, if the task representation is one-hot, it may be useful to instead have access to the (xgoal, ygoal) coordinates of the goal. Additionally, if the task representation contains the location of an additional irrelevant object, (xgoal, ygoal, xobject, yobject), then if may be useful to instead have access to a more parsimonious task descriptor, (xgoal, ygoal), that contains the goal location alone. In general, the entire MDP does not need to be uniquely identified. The agent only needs to identify the variations between MDPs that occur in the task distribution, p(M). Even more specifically, the agent only needs to identify the subset of those variations that change the optimal policy. 
To address uninformative and irrelevant task information, representations can be learned by pre training in the multi-task setting [126, 177]. Let gθ(cM) be a function that encodes the task rep resentation. First an informed policy, πmulti 
θ(a|s, gθ(cM)), can be trained. This is the multi-task 
phase, and enables the learning of gθ. Since this representation, gθ(cM), is learned end-to-end, it contains the information relevant for solving the task. For example, gθ could transform cM from (xgoal, ygoal, xobject, yobject) to (xgoal, ygoal). Often, an information bottleneck [5] is used to ensure it contains only this information [126, 177]. After this, the inner-loop can infer the learned represen tation, gθ(cM), from the meta-trajectory, which we write as gˆθ(D) (see Figure 11). For example, in the circle navigation task, after learning the coordinate representation, (xgoal, ygoal), the inferred gˆθ could be represented by the inferred coordinates, (ˆxgoal, yˆgoal). Alternatively, the informed multi-task RL may be performed concurrently with the meta-RL [137]. 
For example, the informed policy regularization algorithm (IMPORT) [137] follows the simultaneous-training paradigm. In this case, the auxiliary inference objective is given by 
Jinfer(θ) = EM[ED|π[(gθ(cM) − gˆθ(D))2]]. 
This inference objective forces the encoding of g and gˆ to look similar. While optimizing this objective, the algorithm also optimizes the normal meta-learning objective (see Equation 3) but with the policy conditioned on the inferred task representation: πθ(a|s, gˆθ(D)). Additionally, IMPORT simultaneously optimizes the meta-RL objective but with the multi-task policy conditioned on the learned task representation: πmulti 
θ(a|s, gθ(cM)). 
Some task distributions even allow for significant shared behavior between the informed multi-task agent and the uninformed meta-RL agent. This sharing is generally possible when little exploration is needed for the meta-RL policy to identify the task. In this case instead of only inferring the privileged task information, the meta-RL agent may imitate the multi-task agent through distillation [327, 215], which we cover in Section 3.6, or by direct parameter sharing of policy layers [137, 230]. For instance, in the IMPORT algorithm, πθ and πmulti 
θshare parameters by using the same 
components of the meta-parameters vector, θ. In this case, the entire policy is shared, such that π = πmulti. Even when the multi-task and meta-RL policies are computed sequentially, the pre trained multi-task agent may still be used as an initialization for the meta-RL agent [24]. In the case that representations of the policies are shareable between multi-task learning and meta-learning, the task may even be represented as weights and biases of the multi-task policies themselves [230, 24]. Moreover, instead of learning to infer the policy directly, it is be possible to learn a value function conditioned on a latent space of inferred tasks and policies, then optimize the value function within just the latent space of policies [238]. 
In contrast, when task distributions require taking sufficiently many exploratory actions to identify the task, sharing policies becomes less feasible. For example, in the circle navigation task, the informed multi-task policy, with access to the goal, never needs to explore, whereas the primary behavior of the meta-RL policy is exploration around the circumference of the circle. If one were to reuse the multi-task policy conditioned on the inferred goal, πθ = πmulti 
θ(a|s, xˆgoal, yˆgoal), as the 
meta-RL policy, then the inference would not have sufficient data to be accurate. The initial inferred goal location, for example, may be (0, 0), which is not in the training distribution of the multi-task agent. In this case, the behavior of the multi-task agent would be undefined. However, sharing some parameters, simultaneously training the shared policy on inferred representation [137], or fine-tuning the shared policy as an initialization for the meta-RL policy [24], can solve this issue. For example, if training the shared policy simultaneously with known and inferred goal locations, (xgoal, ygoal) and (ˆxgoal, yˆgoal), the agent can learn to execute exploratory behavior whenever the inferred task is (0, 0), or has high uncertainty in the posterior distribution. Still, collecting data sufficient for inferring the task correctly may be difficult. Often, intrinsic rewards are additionally needed to even be able to 
23

(a) TI with IB IB 
(b) VariBAD [371] IB 

Figure 12: Task inference with an information bottleneck (IB) (a), task inference with an IB and without privileged information, as in Zintgraf et al. [371] (b). In either case, the parameters of a latent distribution are passed to the policy. When using privileged information, the task represen tation, c, is reconstructed from samples of this distribution; otherwise, the set of trajectories, D, is reconstructed. 
collect the data enabling task inference. Such exploration in task inference methods is discussed in Section 3.4. 
Task inference without privileged information Other task inference methods do not rely on priv ileged information in the form of the known task representation. If the algorithm is allowed to know whether two trajectories where generated by the same task, then one option is contrastive learning between the tasks [105, 83, 206, 46]. In the same setting, one can cluster the latent representations by aiming to reduce variance of the latents within each task, while still incentivizing diversity be tween the tasks [187]. Alternatively, task inference methods may use no information about the task whatsoever. For instance, a task can be represented as a sample from a stochastic latent variable pa rameterizing the value-function [241], or the representation can be (a sample from) a latent variable parameterizing a learned reward function or transition function [367, 372, 358, 371, 115]. All of these use information already observable, and all of these methods train fθ(D) to represent a task distribution, given a set of trajectories in D. 
For example, Zintgraf et al. [372] propose to learn the task parameters, ϕ, as a latent variable pa rameterizing the reward and transition function. In this case, the latent is trained in a self-supervised manner by reconstructing trajectories. This is accomplished using only observable information by predicting rewards and next states for each transition, given each ϕt: 
HT 

Jinfer(θ) = EM[ED|π[X t=0 
HT 
log pθ(D|ϕt)]], 

= EM[ED|π[X t=0 
X 
s,a,r,s′∈D 
[log pθ(s′|s, a, ϕt) + log pθ(r|s, a, ϕt)]]]. 

An extension of this algorithm has even been proposed with a hierarchical latent variable to accom modate the additional structure in distributions of POMDPs [4]. 
Inner-loop representation Generally, task inference is implemented by adding an additional loss function, and not by any particular meta-parameterization of fθ. While task inference methods do not require a particular meta-parameterization, most implementations use a “black box,” such as a recurrent neural network [126, 137, 372, 177, 369]. Since many task inference methods infer a latent variable, it is also common for fθ to explicitly model this distribution using a variational information bottleneck [126, 241, 372, 177, 369]. 
In this case, the variational distribution defining the latent variable conditions on D. We write this as qθ(z|D). This variational distribution has its own parameters inferred from the dataset, such as a 
24
mean, µθ(D), and a covariance matrix diag(σθ(D)). First, the task representation is reconstructed, e.g., with a linear projection of samples from qθ: cˆM = Lcθ(z ∼ qθ). Then, the inference is trained by maximizing Jinfer, also called the reconstruction loss, over samples from the variational distribution: 
Jinfer(θ) = EM[ED|π[pθ(cM|cˆM)]], 
along with a KL-Divergence to a prior, 
Jprior(θ) = EM[ED|π[D(qθ(z)||pθ(z)]]. 
Once the task inference is trained, the parameters passed to the policy, ϕ, must be chosen. One option is to represent ϕ as a projection of the full distribution of the latent variable, e.g., its mean and variance, in order to capture uncertainty about the task [372, 127]. This can be writ ten πθ(a|s, ϕ = Lϕθ(µ, σ)). Combining this representation of the latent distribution with the Jinfer for trajectory reconstruction above, we arrive at a canonical method, variational Bayes-Adaptive Deep RL (VariBAD) [371]. This method is depicted in Figure 12. 
Alternatively, it is possible for the policy to condition on samples from the information bottleneck (i.e., πθ(a|s, ϕ ∼ qθ)), instead of conditioning on the parameters of the information bottleneck directly [241]. In this case, the task inference can be trained entirely from the actor and critic loss given by the meta-learning objective, rather than a distinct task inference objective. Rakelly et al. [241] and Wen et al. [328] use such probabilistic embeddings for actor-critic RL, as introduced by the PEARL method [241]. 
Several methods also make use of the exchangeability, or permutation invariance, of the transitions implied by the Markov property in task inference [241, 147, 238, 127, 23]. PEARL specifically models the task distribution conditioned on D as a product of individual distributions conditioned 

on each transition in D: 
ϕ ∼ qθ(z|D) ∝Y s,a,r,s′∈D 
qθ(z|s, a, r, s′). 

While it is not necessary to model this permutation invariance, standard sequence models are sen sitive to the ordering of their inputs, which can can be detrimental to learning when the order does not matter [21]. For this reason, it may be useful to embed the structure of permutation invariance into the task inference model as an inductive bias. Specifically, PEARL uses a product of Gaussians, qθ(z|s, a, r, s′) = N (z; µθ(s, a, r, s′), diag(σθ(s, a, r, s′))), which has a simple closed form for the joint mean and covariance. However, other methods forgo the product entirely, and use more general permutation invariant representations, such as neural processes [92, 322] and transformers [308]. 
Finally, some work uses hypernetworks in the inner-loop [230, 22, 24]. It is possible to use a neural network to map an inferred task to base-network weights and biases: πhθ(ˆcM)(a|s) [22]. In this case, the mapping is a hypernetwork, and it can be trained with the meta-learning objective. Alternatively, it is also possible to use the parameters of existing experts trained in the multi-task setting as targets for the hypernetwork. In this case, the hypernetwork itself may be (additionally) trained with the task-inference objective [230, 24]. Even in the former case, where the hypernetwork has no direct supervision, it may be possible to make use of pre-training in the multi-task setting. For example, the meta-learned hypernetwork can be pre-trained in the multi-task setting [24]. And, related work in transfer learning shows that learning combinations of existing expert network parameters can enable efficient transfer and suggests that similar inductive biases can be built into the architecture of meta-learning methods [239]. 
Task inference trade-offs Task inference methods present trade-offs in comparison to other meth ods. First, we consider the comparison to PPG methods then we consider the comparison to black box methods. In comparison to PPG methods, task inference methods impose less structure on the inner-loop. On the one hand, PPG methods generalize well to novel tasks because of the additional structure. In the case where a novel task cannot be represented using the task representations learned during meta-training, task inference methods fail [252], whereas PPG methods generally adapt to the novel task using a policy gradient. On the other hand, PPG methods are unlikely to recover an algorithm as efficient as task inference. For distributions where task inference is possible, fitting such a method to the task distribution may enable faster adaptation. If there are not many tasks in the distribution they are easily inferable from a few consecutive transitions, inferring a latent task using a task inference method can be more sample efficient than learning a new policy with a PPG method. This is the trade-off between generalization and specialization depicted in Figure 8. 
25

x x 
A A 
K episodes 
A 
H episodes 
x 
A 
x 
A 
x 
...MDP 1 

Figure 13: Illustration of “free” exploration in first K episodes (yellow), followed by not free ex ploration (gray), followed by exploitation (white). An agent (A) is trying to identify how to get to a goal location (X). The agent has K shots, or free episodes to explore. However, K episodes may not be enough, so in the (K + 1)th episode, the agent is still exploring the map to find the goal, and is penalized for this. In the remaining two episodes, the agent has learned to navigate to the goal optimally, once the goal has been found, and no longer needs to explore. 
In comparison to black box methods, task inference methods impose more structure. On the one hand, task inference methods often add additional supervision through the use of privileged infor mation [126, 137, 177] or the use of self-supervision [372, 369], which may make meta-training more stable and efficient [126, 372]. This is particularly useful since recurrent policies, often used in task inference and black box methods, are difficult to train in RL [21]. On the other hand, there is evidence that black-box training can be stabilized in meta-RL with the use of a hypernetwork ar chitecture and reasonable initialization [24], and training the inner-loop to accomplish any objective that is not Equation 3 may be suboptimal with respect to that meta-RL objective over the given task distribution. Beyond comparisons to PPG methods and black box methods, task inference methods provide some additional advantages. Task inference methods that model the reward and dynamics can be used to sample additional (imagined) tasks [252], which can be seen as a type of model-based meta-RL, discussed later in Section 3.7. And, as we show in Section 3.4, task inference methods also are useful for exploration. 
3.4 Exploration and Meta-Exploration 
Exploration is the process by which an agent collects data for learning. In standard RL, exploration should work for any MDP and may consist of random on-policy exploration, epsilon-greedy ex ploration, or methods to find novel states. In meta-RL, this type of exploration still occurs in the outer-loop, which is called meta-exploration. However, there additionally exists exploration in the inner-loop, referred to as just exploration, which is where we begin our discussion. This inner-loop exploration is specific to the distribution of MDPs, p(M). To enable sample efficient adaptation, the meta-RL agent uses knowledge about the task distribution to explore efficiently. For instance, instead of taking random actions, the robot chef may open every cabinet to learn about the location of food items and utensils when first entering a new kitchen. This exploration is targeted and used to provide informative trajectories in D that enable few-shot adaptation to the MDP within the task distribution. 
Recall that in the few-shot adaptation setting, on each trial, the agent is placed into a new task and is allowed to interact with it for a few episodes (i.e., its few shots K), before being evaluated on solving the task in the next few episodes (i.e., over the H − K episodes in Equation 3). An illustration can be seen in Figure 13. Intuitively, the agent must explore to gather information during the first few shots that enables it to best solve the task in later episodes. More generally, there is an exploration exploitation trade-off, where the agent must balance taking exploratory actions to learn about the new task (potentially even beyond the initial few shots) with exploiting what it already knows to achieve high rewards. It is always optimal to explore in the first K episodes, since no reward is given to the agent. However, the optimal amount of exploration in the remaining H − K shots depends on the size of the evaluation period H − K: When H − K is large, more exploration is optimal, as sacrificing short-term rewards to learn a better policy for higher later returns pays dividends, while when H − K is small, the agent must exploit more to obtain any reward it can, before time runs out. In this section, we survey approaches that navigate this trade-off. Table 3 summarizes 
26
Sub-topic Papers 
End-to-end 
components Stadie et al. [284], Garcia et al. [91], and Boutilier et al. [34] Posterior sampling Gupta et al. [108], Rakelly et al. [241], Kveton et al. [152], and Simchowitz et al. [277] 
Task Inference Zhou et al. [367], Gurumurthy et al. [109], Wang et al. [318], Fu et al. [83], Liu et al. [177], and Zhang et al. [358] 
Meta-exploration Grewal et al. [100] and Zintgraf et al. [369] 
Table 3: Few-shot meta-RL research categorized by exploration method as described in Section 3.4. End-to-End methods learn to explore implicitly, by directly maximizing the meta-RL objective. Posterior sampling maintains a distribution over possible tasks and acts optimally with respect to samples from this distribution. Task inference guides exploration in order to enable better inference of the task. Meta-exploration concerns exploration in the outer-loop. 
these categories. As discussed in Section 2, there is an optimal solution to this exploration problem that maximizes the meta-RL objective. In the next section, we discuss a framework that formalizes making this trade-off optimally. 
End-to-end optimization Perhaps the simplest approach is to learn to explore and exploit end to-end by directly maximizing the meta-RL objective (Equation 3) as done by black box meta RL approaches [62, 321, 201, 284, 34]. Approaches in this category implicitly learn to explore, as they directly optimize the meta-RL objective whose maximization requires exploration. More 
specifically, the returns in the later K − H episodes Pτ∈DK:HG(τ ) can only be maximized if the policy appropriately explores in the first K episodes, so maximizing the meta-RL objective can yield optimal exploration in principle. However, when more complicated exploration strategies are required, learning exploration this way can be extremely sample inefficient. One issue is that learning to exploit in the later K − H episodes requires already having explored in the first K episodes, but exploration relies on good exploitation to provide reward signal [177]. For example, in the robot chef task, the robot can only learn to cook (i.e., exploit) when it has already found all of the ingredients, but it is only incentivized to find the ingredients (i.e., explore) if doing so results in a cooked meal. Hence, it is challenging to learn to exploit without already having learned to explore and vice-versa, and consequently, end-to-end methods may struggle to learn tasks requiring sophisticated exploration in a sample-efficient manner, compared to methods with more structure, discussed later in this section. 
Some methods that learn exploration end-to-end add additional components. For example E-RL2 [284] sets all rewards in the first K episodes to zero in the outer-loop. While ignoring these rewards does introduce sparsity, it may be helpful when myopically maximizing an immediate, dense reward prevents exploration needed for a longer-term reward. It has also been shown that variance reduction methods may help in the bandit setting [34], and PPG methods may benefit from learning separate exploration and exploitation policies [91]. In the latter case, which policy gets to select an action is chosen by a fixed ϵ schedule that favors exploration at the beginning of adaptation, the exploration policy is meta-learned, and the exploitation policy is learned from scratch for each MDP. Here, it is still the case that both policies are optimized end-to-end [91]. However, in general, many methods add more structure over end-to-end optimization of the meta-RL objective in order to solve task distributions that demand complicated exploration behavior. 
Algorithm 3 PEARL Inner-Loop 
1: Sample task Mi ∼ p(M) 
2: Initialize empty meta-trajectory Di0:H 
3: for each shot k = 0, ..., H − 1 do 
4: Sample ϕ ∼ qθ(z|Di0:k) 
5: Roll out πθ(a|s, ϕ) to collect trajectory τk 
6: Set Dik = τk 
7: end for 
Posterior sampling To circumvent the challenge of implicitly learning to explore, Rakelly et al. [241] propose to directly explore via posterior sampling, an extension of Thompson sampling [300] 
27

Optimal 0-shot 
x 
A 
A 
x 
A 
x 
Posterior Sampling 
x 
A 
A 
x 
A 
x 

Irrelevant 
Exploration 
A 
x 
A 
x 
A 
x 

Episode 1 Episode 2 ... Episode 30 
Figure 14: Exploration for a robot chef (green) finding a stove (red) along a circular counter. The first two rows compare optimal exploration and posterior sampling. The third row shows excessive exploration (blue) when irrelevant information is available in the sampled MDP. 
to MDPs. The inner-loop of this method, PEARL, is described in Algorithm 3. When the agent is placed in a new task, the general idea is to maintain a distribution over what the identity of the task is, and then to iteratively refine this distribution by interacting with the task until it roughly becomes a point mass on the true identity. Posterior sampling achieves this by sampling an estimate of the task identity from the distribution on each episode, and acting as if the estimated task identity were the true task identity for the episode. Then, the observations from the episode are used to update the distribution either with black box methods [241] or directly via gradient descent [108]. Note that this method can be used both when tasks are MDPs [241] and when tasks are bandits [152, 277]. 
Posterior sampling does, however, also have a drawback. First, since the policy employed is always conditioned on a sampled task, all of the exploration in such a method is executed by a policy that assumes it knows the task it is in. This means that the same policy is used for both exploration and exploitation. This can lead to suboptimal exploration in terms of Equation 3. Consider a robot chef that has to find a stove along a curved kitchen counter. Optimally exploring, the chef walks along the perimeter of the counter until it finds the stove. If the chef must be reset to its initial position, e.g., to charge its battery at the end of an episode, then the robot resumes exploration where it last left off. In contrast, a chef using posterior sampling, in each episode, simply walks to a different point along the counter that it has not yet checked, repeating this process until it finds the stove. 
This comparison is depicted in Figure 14. Other methods for exploration in meta-RL exist that add structure to exploration without this limitation. 
Task inference Another way to avoid the challenges of implicitly learning to explore is to directly learn to explore using task inference objectives that encourage exploration [367, 109, 318, 83, 177, 358]. Some, but not all, task inference methods make use of such an objective to encourage ex ploration. Exploration methods that use task inference generally add an intrinsic reward to gather information that removes uncertainty from the task distribution. In other words, these methods train the policy to explore states that enable predicting the task. Specifically, these intrinsic rewards gen erally incentivize improvement in transition predictions (i.e., in adapting the dynamics and reward function) [367] or incentivize information gain over the task distribution [83, 177, 358]. The idea is that recovering the task is sufficient to learn the optimal policy, and hence achieve high returns in the later episodes. While task inference rewards may incentivize more exploration than necessary, as we discuss in this section in the context in meta-exploration, the rewards can be annealed so that the policy is ultimately optimized end-to-end [369]. 
Most of these methods use separate policies for exploration and for exploitation, particularly when exploration episodes are freely available [367, 109, 83, 177]. The intrinsic reward is used to train the exploration policy, while the standard meta-RL objective given by Equation 3 is used to train the ex ploitation policy. The exploration policy explores for the first K episodes, and then the exploitation 
28
policy exploits for the remaining H − K exploitation methods, conditioned on the data collected by the exploration policy. 
One method, DREAM [177], first identifies the information that is useful to the exploitation policy, and then trains the exploration policy directly to recover only this information. This is critical when the number of shots K is too small to exhaustively explore all of the dynamics and reward function, much of which may be irrelevant. For example, exploring the decorations on the wall may pro vide information about the task dynamics, but are irrelevant for a robot chef trying to cook a meal. Learning the task representation in this way can be seen as multi-task training, as described in the Section 3.3, which addresses uninformative (e.g., a one-hot encoding), or irrelevant task informa tion. This multi-task training is particularly beneficial in the context of exploration since the policy used in the multi-task phase to learn the task representation can also be reused as an exploitation policy. While, the exploration and exploitation policies could be meta-trained entirely in sequence, in practice they are trained simultaneously for stability [83, 177]. Meta-testing and meta-training for DREAM are described in Algorithms 4 and 5, respectively. Still, DREAM has some drawbacks. For instance, DREAM requires privileged information in the form of a known task representation, and DREAM may be suboptimal when it is not possibly to explore sufficiently in the given K shots (e.g., in a 0-shot setting). 
Algorithm 4 DREAM Meta-Testing 
1: Sample task Mi ∼ p(M) 
2: Initialize empty meta-trajectory Di0:K 
3: for each exploration k = 0, ..., K do 
4: Roll out exploration policy πθ(a|s, Dik) to collect trajectory τk 
5: Set Dik = τk 
6: end for 
7: Infer ϕˆ ∼ qθ(ϕˆ|Di0:K) 
8: for each exploitation episode h = K, ..., H − 1 do 
9: Roll out exploitation policy πmulti 

10: end for 
θ(a|s, ϕˆ) 

Algorithm 5 DREAM Meta-Training 
1: while not done meta-training do 
2: Sample tasks Mi ∼ p(M) 
3: for each task Mi do 
4: Initialize meta-parameters, θ, and empty meta-trajectory Di0:K 
5: Sample ϕ ∼ gθ(z|cM), a task embedding with IB 
6: for each exploitation episode h = K, ..., H − 1 do 
7: Every other episode, replace ϕ with ϕˆ ∼ qθ, to make the meta-training task distribution closer to the meta-test distribution for stability 
8: Roll out exploitation policy πmulti 

9: Update πmulti 
θ(a|s, ϕ) 
10: end for 
θand gθ using task reward and prior 

11: for each exploration episode k = 0, ..., K − 1 do 
12: Roll out exploration policy πθ(a|s, Dik) to collect trajectory τk 
13: Set Dik = τk 
14: Update qθ(ϕˆ|Di0:k) to infer ϕ using variational inference 
15: Compute rexplore = −||ϕ − ϕˆt||22 + ||ϕ − ϕˆt−1||22 − c for constant c, and all timesteps t 16: Update πθ using exploration reward, rexplore, i.e. the information gain in q for each transition 
17: end for 
18: end for 
19: end while 
Meta-exploration Finally, in meta-RL, there is still the process of acquiring data for the outer-loop learning, just as in standard RL. This is called meta-exploration, since it must explore the space of 
29
exploration strategies. While meta-exploration can be considered exploration in the outer-loop, both loops share data, and exploration methods may affect both loops, so the distinction may be blurry. Often, sufficient meta-exploration occurs simply as a result of the exploration of the standard RL algorithm in the outer-loop. However, a common method to specifically address meta-exploration is the addition of an intrinsic reward. In fact, the addition of any task inference reward, discussed in the previous paragraph, can be considered meta-exploration. This is particularly apparent when considering that this intrinsic reward can be used to train a policy exclusively for off-policy data collection during meta-training. However, sometimes adding a task inference reward is not enough. In this case, intrinsic rewards can be added that function similarly to those in standard RL. For example, using random network distillation (RND) [37], a reward may add an incentive for novelty [369]. In this case, the novelty is measured in the joint space of the state and task representation, instead of just in the state representation, as in standard RL. For example, HyperX [369] adds the reward, 
rhyper(ϕ, s) = ||f(ϕ, s) − h(ϕ, s)||, 
where ϕ = Lθ(µ, σ) represents a projection of the task distribution, as in VariBAD; f represents the predictor network in RND; and h represents the random prior network in RND. 
Additionally, an intrinsic reward may add an incentive for getting data where the task inference has high error and is still not well trained [369], or add an incentive for getting data where TD-error is high [100]. For example, HyperX [369] adds the reward: 
rerror(ϕ, s) = − log pθ(s′|s, a, ϕ) − log pθ(r|s, a, ϕ) 
∝ ||s′ − sˆ′||22 + ||r′ − rˆ′||22 
Many of these rewards incentivize behavior that should not occur at test time, and in any case, the additional reward changes the optimal policy as suggested by Equation 3. To address this, the reward bonus can be annealed, letting the portion incentivizing meta-exploration go to zero [369], so that learning is still ultimately optimized end-to-end, or meta-training could occur off-policy. 
3.5 Bayes-Adaptive Optimality 
Our discussion so far reveals two key intuitions about exploration. First, exploration reduces the uncertainty about the dynamics and reward function of the current task. Crucially, it is not optimal to indiscriminately reduce all uncertainty. Instead, optimal exploration only reduces uncertainty that increases expected future returns, and does not reduce uncertainty over distracting or irrelevant parts of the state space. Second, there is a tension between exploration and exploitation: gathering information to decrease uncertainty and increase future returns can sacrifice more immediate returns. In these cases, it can be worthwhile for the agent to fall back on behaviors shared across all tasks, instead of adapting to the task, particularly when time for exploration is limited [155]. Therefore, maximizing the returns across a period of time requires carefully balancing information gathering via exploration and exploiting this information to achieve high returns. These raise an important question: What is an optimal exploration strategy? To answer this question, we next introduce the Bayes-adaptive Markov decision process [63, 95], a special type of MDP whose solution is a Bayes optimal policy, which optimally trades off between exploration and exploitation. Then, we discuss practical methods for learning approximate Bayes-optimal policies, and analyze the behavior of algorithms introduced in the previous section from the perspective of Bayes-optimality. 
Bayes-adaptive Markov decision processes. To determine the optimal exploration strategy, we need a framework to quantify the expected returns achieved by a policy when placed into an MDP with unknown dynamics and reward function. From a high level, the Bayes-adaptive Markov deci sion process (BAMDP), a special type of MDP, models exactly this: At each timestep, the BAMDP quantifies the current uncertainty about an MDP and returns next states and rewards based on what happens in expectation under the uncertainty. Then, the policy that maximizes returns under the BAMDP maximizes returns when placed into an unknown MDP. Crucially, the dynamics of the BAMDP satisfy the Markov property by augmenting the states with the current uncertainty. In other words, the optimal exploration strategy explicitly conditions on the current uncertainty to determine when and what to explore and exploit. 
More formally, the BAMDP characterizes the current uncertainty as a distribution over potential transition dynamics and reward functions based on the current observations. Intuitively, a peaky distribution that places most of its mass on only a few similar dynamics and reward functions en codes low uncertainty, while a flat distribution encodes high uncertainty, as there are many different 
30
dynamics and reward functions that the agent could be in. Specifically, the belief at the tth timestep bt = p(r, p | τ:t) is a posterior over dynamics p and reward functions r consistent with the observa tions τ:t = (s0, a0, r0, . . . , st) so far, and the initial belief b0 is a prior p(r, p). Then, the states of the BAMDP are hyperstates s+t = (st, bt) composed of a state st and a belief bt, which effectively augments the state with the current uncertainty. Conditioned on the hyperstate, the policy can sat isfy the Markov property, making the hyperstate a sufficient statistic of the history seen by the agent. The hyperstate is a natural sufficient statistic for Meta-RL, both for its relevance in the BAMDP, and since it corresponds to the belief-state of the meta-RL POMDP described in 2.3. However, other sufficient statistics for meta-RL exist as well [287, 46]. 
As previously mentioned, the transition dynamics and reward function of the BAMDP are governed by what happens in expectation under the current uncertainty. Specifically, the BAMDP reward function is the expected rewards under the current belief: 
R+(st, bt, at) = ER∼bt[R(st, at)] . 
The transition dynamics of the BAMDP are similar. The probability of ending up at a next state st+1 is the expected probability of ending up at that state under the belief, and the next belief is updated according to Bayes rule based on the next state and reward from the underlying MDP and not the BAMDP: 
P+(st+1, bt+1 | st, bt, at) = ER,P ∼bt[P(st+1 | st, at)δ (bt+1 = p(R, P | τ:t+1))] . In other words, the BAMDP can be interpreted as interacting with an unknown MDP and maintain ing the current uncertainty (i.e., belief). Taking an action at at timestep t yields a next state st+1 and reward rt+1 from the MDP, which are used to update the belief bt+1. The next state in the BAMDP is then s+t+1 = (st+1, bt+1), but the BAMDP reward is the expected reward under the current belief r+t = R+(st, bt, at) = ER∼bt[R(st, at)]. 
The standard objective on a BAMDP is to maximize the expected rewards over some horizon of H timesteps: 

J (π) = Eb0,π 
"HX−1 t=0 
R+(st, bt, at) 
# 
. (5) 

As H increases, the agent is incentivized to explore more, as there is more time to reap the benefits of finding higher reward solutions. Notably, this objective exactly corresponds to the standard meta-RL objective (Equation 3), where the number of shots K is set to 0. 
Learning an approximate Bayes-optimal policy Directly computing Bayes-optimal policies re quires planning through hyperstates. As the hyperstates include beliefs, which are distributions over dynamics and reward functions, this is generally intractable for all but the simplest problems. However, there are practical methods for learning approximately Bayes-optimal policies [126, 158, 372, 15]. The main idea is to learn to approximate the belief and simultaneously learn a policy conditioned on the belief to maximize the BAMDP objective (Equation 5). 
As a concrete example, variBAD [372] learns to approximate the belief with variational inference. Since directly maintaining a distribution over dynamics and reward functions is generally intractable, variBAD represents the approximate belief with a distribution bt = p(m | τ:t) over latent variables m. This distribution and the latent variables m can be learned by rolling out the policy to obtain trajectories τ = (s0, a0, r0, . . . , sH) and maximizing the likelihood of the observed dynamics and rewards p(s0, r0, s1, r1, . . . , sH | a0, . . . , aH−1) via the evidence lower bound. Simultaneously, a policy π(at | st, bt = p(m | τ:t)) is learned to maximize returns via standard RL. 
Connections with other exploration methods While not all the methods described in Section 3.4 aim to learn Bayes-adaptive optimal policies, the framework of BAMDPs can still offer a helpful perspective on how these methods explore. We discuss several examples below. 
First, black box meta-RL algorithms such as RL2learn a recurrent policy that not only condi tions on the current state st, but on the history of observed states, actions, and rewards τ:t = (s0, a0, r0, . . . , st), which is typically processed through a recurrent neural network to create some hidden state ht at each timestep. Notably, this history is sufficient for computing the belief state bt = p(r, p | τ:t), and hence black box meta-RL algorithms can in principle learn Bayes-adaptive optimal policies by encoding the belief state in the hidden state ht. Indeed, variBAD can be seen as 
31
adding an auxiliary loss to a black box meta-RL algorithm that encourages the hidden state to be pre dictive of the belief state, though practical implementations of these approaches differ, as variBAD typically does not backpropagate through its hidden state, whereas RL2 does. Moreover, it has been shown both theoretically and empirically that meta-learned agents learn Bayes-optimal behavior in bandits [200]. However, in practice, black box meta-RL algorithms may struggle to efficiently learn Bayes-adaptive optimal policies in domains requiring exploration that is temporally-extended and qualitatively different from the exploitation behavior. Learning such sophisticated exploration is difficult due to challenges in end-to-end optimization. While Team et al. [298] demonstrate that is possible to learn complicated exploration strategies end-to-end, doing so may also require the use of curriculum learning, distillation, and a large number of samples for meta-training [298]. Moreover, the meta-exploration problem in their task distribution may not have presented a difficult meta-exploration problem as their agent “does not use the trial conditioning information it observes to adjust its behaviour” [298]. End-to-end training on the meta-RL objective alone presents a dif ficult optimization problem. Liu et al. [177] highlight one such optimization challenge for black box meta-RL algorithms, where learning to explore and gather information is challenging without already having learned how to exploit this information, and vice-versa. 
Second, many exploration methods discussed in the previous section consider the few-shot setting, where the agent is given a few “free” episodes to explore, and the objective is to maximize the re turns on the subsequent episodes. Likewise, the BAMDP objective can be modified to include free exploration episodes by setting initial rewards to zero. Depending on the amount of free exploration, the optimal policies for the BAMDP can encourage fairly different exploration behaviors. For ex ample, in the robot chef task, the optimal few-shot exploration may be to first exhaustively look through the drawers and pantry for the best culinary utensils and ingredients in the initial few shots before beginning to cook. In contrast, optimal zero-shot behavior may attempt to locate the utensils and ingredients while cooking (e.g., as a pot of water boils), as spending the upfront time may be too costly. This may result in using less suitable utensils or ingredients, though, especially when optimized at lower discount factors. 
More generally, methods designed for the few-shot setting attempt to reduce the uncertainty in the belief state in the initial few free episodes, and then subsequently exploit the relative low uncertainty to achieve high returns. This contrasts behavior in the zero-shot setting, which may involve inter leaving exploration and exploitation. For example, the posterior sampling exploration in PEARL maintains a posterior over the current task, which is equivalent to the belief state. Then, explo ration occurs by sampling from this distribution and pretending the sampled task is the current task, and then updating the posterior based on the observations, which aims to collapse the belief state’s uncertainty. Similarly, by learning an exploration policy that gathers all the task-relevant informa tion during the few free exploration episodes, DREAM also attempts to collapse the belief state to include only dynamics and rewards that share the same optimal exploitation policy. 
3.6 Supervision 
In this section, we discuss most of the different types of supervision considered in meta-RL. In the standard setting discussed so far, the meta-RL agent receives reward supervision in both the inner- and outer-loops of meta-training, as well as meta-testing. However, this might not always be the case. Many variations have been considered, from the unsupervised case (i.e., complete lack of rewards during meta-training or testing), to stronger forms of supervision (e.g., access to expert trajectories or other privileged information during meta-training and/or testing). Each of these presents a different problem setting, with unique methods, visualized in Table 4. Below, we discuss these settings, from those with the least supervision to those with the most. 
Unsupervised meta-RL The first problem setting provides the least supervision: no reward in formation is available at meta-train time, but it is available at meta-test time [107, 129, 208]. For example, a robot chef may be meta-trained in a standardized kitchen and then sold to customers, who may each have their own reward functions for the chef. However, the company training the robot may not know the desires of the customers. In this case, it is difficult to design the reward functions for the distribution of MDPs needed for meta-training. It is difficult even to define a distribution under which we might expect the test tasks to have support. One solution is simply to create rewards that encourage maximally diverse trajectories in the environment. Then, it is likely that what an end user desires is similar to one of these trajectories and reward functions. Gupta et al. [107] and Jabri et al. [129] attempt to learn a set of diverse reward functions by rewarding behaviors that are distinct 
32
Meta-Train Meta-Test Papers 
Standard Meta-RL Rewards Rewards See, e.g., Table 2 Unsupervised 
Meta-RL Unsupervised Rewards Gupta et al. [107], Jabri et al. [129], and 
Mutti et al. [208] 
Sung et al. [289], Miconi et al. [198, 
197], Yang et al. [345], Najarro et al. 

Meta-RL with Unsupervised Meta-Testing 
Meta-RL via 
Rewards (Sparse) 
Unsupervised (Sparse) 
[213], and Yan et al. [342] 
Sparse: Gupta et al. [108], Rakelly et al. [241], Packer et al. [221], Zhao et al. [363], Guo et al. [104], and Wan et al. [317] 
Mendonca et al. [194], Sharaf et al. 

Imitation Supervised Rewards 
[272], Weihs et al. [327], and Nguyen et al. [215] 

Mixed 
Supervision Meta-Imitation 
Rewards and/or Supervision 
Rewards and/or Supervision 
Zhou et al. [366], Dance et al. [52], Prat et al. [234], and Rengarajan et al. [247] Duan et al. [60], Finn et al. [76], James et al. [132], Paine et al. [222], Xie et al. [336], Yu et al. [350], Devlin et al. [58], 
Goo et al. [97], Seyed Ghasemipour et al. [270], Xu et al. [338], Yu et al. 

Learning Supervised (Un)- supervised 
[349], Bonardi et al. [31], Brown et al. [36], Dasari et al. [53], Singh et al. [278], Wu et al. [332], Jang et al. [134], Li et al. [163], Li et al. [166], 
Chowdhery et al. [47], Gao et al. [87], and Hejna III et al. [118] 

Table 4: Problem settings by supervision available at meta-training and meta-testing time as cate gorized in Section 3.6. Most of the literature in few-shot meta-RL considers the problem setting with rewards provided at meta-train and meta-test time. There are three additional variations on this supervision in meta-RL addressed in this section. Meta-imitation learning is a related but separate problem, briefly covered in this survey. 
from one another. In general, a set of tasks can be created using an off-the-shelf unsupervised RL method [67, 226]. After this set of tasks is created, meta-RL can easily be performed as normal. 
For example, Gupta et al. [107] use the method, Diversity is All You Need (DIAYN) [67], to create this distribution over reward functions. Specifically, DIAYN first learns a latent variable, Z, to parameterize the reward function, along with a multi-task policy, πmulti(a|s, z). DIAYN does so by maximizing the mutual information between the state and the latent variable, ensuring partitioning of the states, and the entropy of the policy: 
Jθ = H(A|S, Z) + I(S, Z), 
= H(A|S, Z) + H(Z) − H(Z|S) 
In practice, this optimization amounts to using soft actor critic [111] to maximize H(A|S, Z), while setting r(s, z) = log qθ(z|s) − log p(z) to maximize the other terms in expectation [67]. Here, qθ, is a learned discriminator predicting the probability of the latent variable given the state, and p(z) is a known latent variable distribution. After learning qθ(z|s), this discriminator is then reused as a reward function for meta-RL. Specifically, a separate meta-RL agent (MAML) is trained over this latent distribution using r(s, z) = log qθ(z|s). 
Once trained, such meta-RL agents can adapt more quickly than RL from scratch and are competi tive, on several navigation and locomotion tasks, with meta-training over a hand-designed training distribution. Still, these domains are simple enough that diverse trajectories cover the task space, and a gap remains between these domains and more realistic domains such as the robot chef. Meth ods from reward-free RL, such as successor features [303], are highly relevant here, since they can 
33
also make use of access to the dynamics alone to learn representations. However, in that setting, the reward function is given to the agent at test-time, or estimated manually, rather than being inferred by a meta-learned algorithm. 
Meta-RL with unsupervised meta-testing A second setting assumes rewards are available at meta-train time but none are available at meta-test time. For example, perhaps the company pro ducing a robot chef is able to install many expensive sensors in several kitchens in a lab for meta training. These sensors detect when a counter is scratched, or water damage occurs, or furniture is broken. All of these collectively are used to define the reward function. However, it may be prohibitively expensive to install these sensors in the house of each customer. In this case, rewards are not available at meta-test time. Rewards are in the outer-loop, but they are never used in the inner-loop, and it is assumed that reward information is not needed to identify the tasks. In fact, only the dynamics vary across tasks in this setting. 
In order to learn without rewards at meta-test time, many methods remove rewards from the inner loop entirely. While the inner-loop cannot condition on reward, it may learn to maximize reward by maximizing its correlates. While black box methods are applicable in this setting off the shelf, as suming other inputs to the inner-loop correlate with reward, specific methods have been investigated. For example, in PPG normally the inner-loop requires sampling returns; however, without rewards, this is not possible at meta-test time. One solution is use a learned estimate of return, conditional on the data collected so far, to replace these returns in the policy gradient estimate. Here, the returns can be replaced with a learned advantage function, Aθ(st, at, st+1) [345] or learned critic Qθ(st, at, D) [289]. For example, No-Reward Meta Learning (NoRML) modifies the MAML update to include a learned advantage function (in addition to offset parameters, ϕoffset, that allow the meta-learned initialization to focus exclusively on exploration): 
ϕi1 = ϕ0 + ϕoffset + αEst,at,st+1∈Di0Aθ(st, at, st+1)∇ϕ0log πϕ0(at|st). 
Another approach is to leverage manually designed features in a black-box self-supervised inner loop, while using a fully supervised outer-loop [342]. Finally, yet another approach is to leverage Hebbian learning, [116], a biologically inspired method for unsupervised learning in which weight updates are a function of the associated activations in the previous and next layers. The update to the weight (wki,j ) from the ith activation in layer k (xki), to the jth activation in layer k + 1 (xk+1 

generally has the form 
j + bxki + cxk+1 
j) 

wki,j := wki,j + α(axki xk+1 
j + d), 

where α is a learning rate and α, a, b, c, d are all meta-learned parameters in θ. Since weight updates only condition on adjacent layer activations, if no rewards are passed as input to the policy, this function is both local and unsupervised. Hebbian learning can be applied both to feed-forward [213] and recurrent neural networks [198, 197]. Variants of Hebbian learning for meta-RL, which pass rewards as an input to the policy are investigated by Chalvidal et al. [39] and Rohani et al. [256]. 
Alternatively, instead of having no rewards at meta-test time, we may have only sparse rewards. If dense rewards are available at meta-training, standard meta-RL methods can be applied directly by using the dense rewards in the outer-loop and sparse rewards in the inner-loop [108, 241, 363]. In the case only sparse rewards are available for both meta-training and meta-testing, one approach is to alter the reward function at meta-training. A common method for this is a type of experience relabelling called hindsight task relabelling [221, 317]. Assuming tasks differ only in rewards, trajectories can be relabeled with rewards from other tasks and still be consistent with the MDP. Training can proceed by using a standard off-policy meta-RL algorithm [241]. This is particularly useful if, for instance, the trajectory did not reach a goal state in the original task, but did under the relabeled task. How to choose such a task is one area of investigation [317]. Alternatively, if the dynamics differ, one few-shot method allows for policies to be explicitly transferred between tasks, when helpful, by learning to map actions between tasks such that they produce similar state transitions in each task [104]. Yet another way of addressing sparse rewards at meta-test time is by introducing auxiliary rewards that encourage exploration, as discussed in Section 3.4. 
Meta-RL via imitation A third setting assumes access to expert demonstrations at meta-training time, which provide more supervision than standard rewards. For example, a robot chef may have access to labeled supervision provided by human chefs. This setting can increase sample efficiency 
34
and reduce the burden of online data collection. This problem setting requires access to expert poli cies or expert data. If experts are not known in advance, they can be trained, per task. Alternatively, in the many-shot setting, policies optimized by existing RL algorithms can provide supervision, in a process known as algorithm distillation [144, 156]. In order to improve over the existing RL algorithm, the learning speed can be increased by subsampling the demonstration data instead of learning on the full traces of expert data [144]. One method, Guided Meta-Policy Search (GMPS) [194], proposes to imitate task experts in the outer-loop. In this case, the outer-loop can make use of supervised learning, while the inner-loop still learns a reinforcement learning algorithm that condi tions on rewards at meta-test time. They specifically investigate the use of expert labels for the final policy of a MAML-style algorithm. If an expert is not available, the simultaneous training of task specific experts can also result in stable meta-training [194]. 
GMPS illustrates meta-RL via imitation. GMPS rolls out an initial policy to compute a policy gradient for the inner-loop, and then uses supervised learning in the outer-loop for the adapted policy. The action labels for the supervised learning are given by expert policies for each task, which are assumed to be known. For the outer-loop, GMPS specifically uses behavioral cloning with dataset aggregation, as in DAgger [258], to mitigate distribution shift in the training of the final policy of MAML. Here, the dataset aggregation entails using trajectories from the adapted policy but with expert actions labeled by the expert policies. This cloning loss can be written JBC(Diagg, πiϕ1), where Diagg is the aggregated dataset with expert actions for the ith task, πiϕ1is the policy adapted to the ith task, and JBC is defined as 
JBC(D, π) = Es,a∼D[log π(a|s)]. (6) 
This method is described in Algorithm 6. For bandits, using AggreVaTe [257], an extension of DAgger [258], in the outer-loop has also been proposed [272]. 
Algorithm 6 GMPS Meta-Training 
1: Initialize meta-parameters ϕ0 = θ 
2: Initialize outer-loop datasets, Diagg, with states and actions from roll-outs of multi-task experts πmulti(a|s,Mi), for each task Mi 
3: while not done do 
4: Sample tasks Mi ∼ p(M) 
5: for each task Mi do 
6: Collect data Di0 using the initial policy πϕ0 
7: Adapt policy parameters using a policy gradient step: ϕi1 = ϕ0 + α∇ϕ0 JˆRL(Di0, πϕ0) 8: end for 
9: Update ϕ0 using the meta-gradient: ϕ′0 = ϕ0 + β∇ϕ0PMi∼p(M)JBC(Diagg, πiϕ1) 10: Update Diagg by adding states from roll-outs of adapted meta-RL agents, πiϕ1, but actions from πmulti, for each task Mi 
11: end while 
Meta-RL via imitation is still relatively unexplored. This may be due to difficulty in learning the correct supervision for exploratory actions. Obtaining a meta-RL expert requires knowing how to optimally explore in a meta-RL problem, which is generally hard to compute [372]. Instead of ob taining such a general expert, these papers often make use of task-specific experts, which can be easily obtained by standard RL on each task. However, these task-specific experts can only provide supervision for the post-exploration behavior, e.g., for the actions taken by the final policy produced by the sequence of inner-loop adaptations in a MAML-style algorithm. In this case, credit assign ment for the exploration policy may be neglected. Some papers use the same actions to provide supervision for both uninformed policies that must explore and informed policies that must exploit [366, 234]; however, in most environments actions are generally not the same for both exploration and exploitation. To get around this, the agent can adaptively switch between optimizing the meta RL objective end-to-end and cloning the informed multi-task expert into [327]. Additionally, it may be possible to use the multi-task policy to generate a reward encouraging similar state-action distri butions between the multi-task policy and the meta-RL policy, particularly in maximum-entropy RL [215]. 
Meta-imitation learning A fourth commonly studied problem setting is meta-imitation learning (meta-IL) [60, 76, 132, 222, 350, 97, 270, 349, 31, 53, 278, 134, 163, 87]. While meta-imitation 
35
learning is technically not meta-RL, because the inner-loop is not an RL algorithm, it is a closely related problem setting. Although an extensive survey of meta-IL methods is out of scope for this article, we briefly cover meta-IL here. This setting assumes access to a fixed set of demonstrations for each task in the inner-loop. Most methods additionally train the outer-loop through behavioral cloning on fixed data, in a process also called meta-behavioral cloning (meta-BC). Alternatively, the outer-loop may also perform inverse RL, which we call meta-IRL [270, 338, 349, 332, 118]. Another approach for using meta-learning for IRL is to train success classifiers via few-shot learning [336, 166]. Meta-IRL is generally performed online and so often requires a simulated environment, in contrast to meta-behavioral cloning. For both BC and IRL, the inner- and outer-loops generally also assume access to expert-provided actions, but one line of work considers inner-loops that use only sequences of states visited by an expert, potentially a human, despite being deployed on robotic system [76, 350, 31, 53, 134]. 
There are many similarities between meta-RL research and meta-IL research. In meta-IL, there exist analogues to black-box methods [60, 132, 31, 53], PPG methods [76, 350], and task-inference methods [134], as well as sim-to-real methods [132, 31]. For example, to adapt MAML to meta-BC, the inner-loop and outer-loop are each computed with a behavioral cloning loss over offline data, instead of using policy gradients. An adaptation of MAML for meta-BC, similar to [76], is shown in Algorithm 7. Likewise, to adapt RL2to meta-BC, the RNN summarizes an offline dataset, instead of online data, while the outer-loop uses behavioral cloning. An adaptation of RL2for meta-BC is shown in Algorithm 8. 
Many contemporary papers discuss the meta-IL problem setting under the name of in-context learn ing [36]. In-context learning generally refers to learning in the activations of a sequence model, and frequently implies the use of transformers [242, 159]. Black box methods and task inference methods can be seen as two ways to learn to in-context learn. In-context learning can be used both in reference to learning to perform in-context IL and learning to perform in-context RL [159]. Large language models in particular, having gained significant traction recently [58, 36, 47], per form in-context learning [36], which can be seen as meta-IL where each prompt is treated as a task. Moreover, these models are capable of emergent meta-IL, i.e., meta-IL learning that is not explicitly in the training procedure, where novel datasets are passed in text as a prompt to the model along with a query, e.g., in few-shot prompting [47], which suggests a similar capacity to be investigated in meta-RL. An early investigation of language guided meta-RL [249] conditions a policy on a lan guage instruction. However, instead of a language model it uses a hard-coded generator for the instructions. 
Algorithm 7 Meta-Training MAML for Meta-BC 
Require: A dataset of expert demonstrations per task for adaptation Dinner, and validation, Douter 1: Initialize meta-parameters ϕ0 = θ 
2: while not done do 
3: Sample tasks Mi ∼ p(M) 
4: for each task Mi do 
5: Adapt policy parameters using a step of behavioral cloning on the offline inner-loop data: ϕi1 = ϕ0 + α∇ϕ0 JBC(Diinner, πϕ0) 
6: end for 
7: Update ϕ0 using the meta-gradient: ϕ′0 = ϕ0 + β∇ϕ0PMi∼p(M)JBC(Diouter, πiϕ1) 8: end while 
Mixed supervision Beyond the most commonly studied settings above, Zhou et al. [366], Dance et al. [52], Prat et al. [234], and Rengarajan et al. [247] consider a few related, but different settings. In these settings, there is a phase in which the inner-loop receives a demonstration, followed by a phase in which the inner-loop performs trial-and-error reinforcement learning. The demonstration generally comes from a fixed dataset collected offline, but may additionally be supplemented by online data from a separate policy trained via meta-IL on the fixed dataset [366]. The demonstration data itself may [234] or may not [52] provide supervision (actions and rewards), or a combination of the two [366]. Finally, the agent may use reinforcement learning for supervision in the outer loop during the trial-and-error phase [52], it may use imitation learning [366], or it may use some combination of the two [234]. 
36
Algorithm 8 Meta-Training RL2for Meta-BC 
Require: A dataset of expert demonstrations per task for adaptation Dinner of length T, and valida tion, Douter 1: Initialize meta-parameters θ (RNN and other neural network parameters) 2: while not done do 
3: Sample tasks Mi ∼ p(M) 
4: for each task Mi do 
5: Initialize RNN hidden state ϕ0 
6: Adapt task parameters using RNN on offline inner-loop dataset: ϕiT = fθ(Diinner) 7: end for 
8: Update θ using the meta-BC objective: θ′ = θ + β∇θPMi∼p(M)JBC(Diouter, πθ(·|ϕTi)) 9: end while 
Sub-topic Papers 

Parameterized gradient descent 
Nagabandi et al. [210], Kaushik et al. [139], Mendonca et al. [193], and Co-Reyes et al. [248] 

Recurrent network Nagabandi et al. [210] and Seo et al. [269] 
Recurrent 
Hypernetwork Xian et al. [335] 
Finite context Lee et al. [160] 
Bayesian linear 
regression Harrison et al. [113] 
Permutation 
invariance Galashov et al. [86] and Wang et al. [324] 
Variational inference Sæmundsson et al. [260] and Perez et al. [231] 
Model-predictive 
control Nagabandi et al. [210], Lee et al. [160], and Shin et al. [275] 
Additional Data Hiraoka et al. [120], Rimon et al. [252], Seo et al. [269], and Shin et al. [275] 
Sub-procedure Clavera et al. [48] and Hiraoka et al. [120] 
Continual learning Nagabandi et al. [212] 
Additional Objective Alet et al. [6] 
Adversarial training Lin et al. [172] 
Table 5: Categories of model-based meta-RL methods from Section 3.7. Many themes are similar to those in the model-free section, such as the use of parameterized gradient descent, recurrent networks, permutation invariance, and hypernetworks. The top half of the table shows key concepts in the representation of the inner-loop or the model adaptation, while the bottom half shows what the learned adaptive model is used for. 
3.7 Model-Based Meta-RL 
So far, most of the algorithms discussed have been model-free, in that they do not learn a model of the MDP dynamics and reward. Alternatively, such models can be learned explicitly, and then be used for defining a policy via planning or be used for training a policy on the data generated by the model. Such an approach is called model-based RL, and can play a helpful role in meta-RL [113, 260, 86, 210, 212, 139, 160, 172, 193, 231, 120, 248, 335, 269, 275, 324]. Model-based meta RL methods are summarized in Table 5. In this section, we briefly survey model-based meta-RL methods. We keep the discussion limited for several reasons: most model-based methods have an analogous model-free method already discussed in Section 3, most trade-offs between model-based and model-free methods that exist in RL are the same in meta-RL, and most of the meta-RL literature considers model-free RL. 
Many different types of model-based meta-RL have been investigated. In order to adapt the parame ters of the model, some model-based meta-RL papers use gradient descent, as in MAML [210, 139, 193, 248], and some use black-box RNNs, as in RL2 [210]. Alternatively, a fixed number of past transitions can be encoded [160], or variational inference can be used [260, 231]. Similar themes arise in model-based meta-RL as in model-free meta-RL, such as the use of hypernetworks [335] and permutation invariance of transitions [86, 324], as discussed earlier in Sections 3.2 and 3.3. Some methods plan with the model directly using model-predictive control with an off-the-shelf 
37
Algorithm 9 Black-Box Model-Based Meta-RL 
1: Initialize an empty dataset for the environment model Dmodel 
2: Initialize an empty dataset for the policy Dpolicy 
3: Initialize meta-parameters, θm, for the model and meta-parameters, θp, policy 4: while not done do 
5: Sample tasks Mi ∼ p(M) 
6: for each task Mi do 
7: Replace old data in Dipolicy with roll-outs from πϕtusing recurrent policy in real environ ment 8: Add the same roll-outs to Dimodel 
9: end for 
10: Add roll-outs from πϕtin simulated environment model to Dpolicy, optionally starting from real trajectories 
11: Update policy θp using the meta-RL objective: 
θ′p = θp + β∇θpPMi∼p(M)JˆRL(Dipolicy, πθp(·|ϕt)) 
12: Update model θm using the supervised objective: 
θ′m = θm + β∇θmPMi∼p(M) Es,a,r,s′,ϕt∼Dimodellog pθm(s′|s, a, ϕt) + log pθm(r|s, a, ϕt)) 13: end while 
planner [210, 160], some use the model simply for additional data when training a policy [120, 252, 269], some use the model for both model-predictive control and policy learning [275], and others use the adapted parameters from the model as an input to a policy that is then trained using standard RL [160, 193, 372, 363]. However, in the last case there is significant overlap between model based meta-RL and task-inference methods because learning to infer the task often means learning a latent-variable dynamics model. We discuss task-inference methods with model-free methods in Section 3.3, due to their otherwise similar assumptions. Moreover, we categorize methods that use a model with a black-box inner-loop for planning or sampling as black-box model-based methods. This leaves task-inference model-based methods as a sparse category. 
A simple method, for example, would learn a black-box environment model, without a separate planner, and run a standard RL algorithm, but use the model for additional data in the policy gradient estimation. The model itself would be trained using a maximum likelihood estimate of the transition function and reward function over the same trajectories collected by the policy. For pseudocode of such a model, see Algorithm 9. Methods that condition a policy on parameters adapted for a model can also be seen as task inference methods. The model may reconstruct dynamics or reward functions. Models can be used to find adversarial tasks that improve generalization when optimizing the worst-case return [172]. If access to the underlying Markovian states is not possible, dynamics models over the latent states can also be fit by reconstructing observations [363]. Closely related to the meta-RL problem setting, one approach uses model-based meta-RL for continual learning [212], while another uses it as a sub-procedure in a standard RL algorithm to make learning more sample efficient [48, 120]. Finally, using meta-gradients to adapt a model using an additional or unsupervised inner-loop, as opposed to the standard supervised loss, can be beneficial [6]. 
Overall, there are trade-offs between model-free meta-RL and model-based meta-RL. On one hand, model-based meta-RL methods can be extremely sample efficient when it is possible to learn an accurate model [210]. Model-based methods may also be learned off-policy, because the model can be trained using supervised learning. On the other hand, model-based meta-RL may require the implementation of additional components, especially for longer-horizon tasks that require more than an off-the-shelf planner, and can have lower asymptotic performance [48, 333]. Finally, in the meta-learning context, model-based RL may confer two unique advantages. First, when there are too few tasks in the meta-training distribution, model-based meta-RL can allow for supplemental (imagined) tasks to be sampled [252]. Second, model-based meta-RL may be easier when an off-the shelf planner is viable and complicated exploration policies are necessary. In model-free meta-RL, the meta-learning needs to learn what data to collect and how. However, in model-based RL, the exploration may be offloaded to a planning algorithm. It may be easier to allow the planner to deal with complicated exploration in the inner-loop rather than learn this directly. 
38
Area Papers 
Meta-SL Baxter [19], Denevi et al. [56], Finn et al. [75], Khodak et al. [140], Tripuraneni et al. [305], and Collins et al. [50] 

Multi-task Representation Learning for RL 
Jin et al. [135], Yang et al. [344], Hu et al. [124], Cheng et al. [45], and Lu et al. [184] 

Meta-RL Simchowitz et al. [277], Chen et al. [43], Rimon et al. [252], Tamar et al. [295], and Ye et al. [346] 
Table 6: Categories of theory papers from section 3.8. Papers relating to theory of meta-SL and representation learning for RL are included in addition to pure meta-RL theory papers. 
3.8 Theory of Meta-RL 
The theory of meta-RL aims to understand and formalize the principles governing the learning of RL algorithms. As meta-RL is a relatively new area, its theoretical exploration is closely informed by insights from the related areas of meta-SL and multi-task representation learning for RL. However, meta-RL poses additional challenges due to the inner-loop learning an RL algorithm. For a theoret ical understanding of these challenges, many researchers have used the Bayesian framework. This subsection integrates central insights from the theory of meta-SL and representation learning and overviews specific theoretical developments within meta-RL. The papers discussed in this section are summarized in Table 6. 
Insights from supervised meta-learning Meta-learning, both meta-RL and meta-SL, focus on designing algorithms capable of efficiently learning new tasks from a given task distribution. A key aspect of this learning process is the development of representations or inductive biases that generalize across tasks. As an early theoretical contribution, Baxter [19] laid the groundwork by studying the learning of inductive biases in a PAC (probably approximately correct [306]) setting, where the learner operates on a family related tasks and the goal is to find a hypothesis space that is appropriate for all of the tasks in the family. More recently, Tripuraneni et al. [305] provide a fundamental result on how features learned across tasks improve the efficiency of learning in a new task. A related result specialized to MAML [73] is shown by Collins et al. [50]. MAML is further studied in the online convex optimization framework by Denevi et al. [56], Finn et al. [75], and Khodak et al. [140], who assume a notion of task similarity under which all tasks are close to a single fixed point in the parameter space for which guarantees can be provided. These findings, while not directly addressing meta-RL, offer valuable insights into the representation learning aspect crucial for meta-RL algorithms. 
Multi-task representation learning for RL Multi-task representation learning for RL is closely related to meta-RL in that it considers learning a shared representation across a set of tasks. However, in representation learning, learning an exploration strategy or more generally learning a learning al gorithm are not considered. Nevertheless, the settings are close enough that the theoretical results for multi-task representation learning can provide helpful guidance for developing a theoretical un derstanding of meta-RL. 
Jin et al. [135], Yang et al. [344], and Hu et al. [124] consider representation learning in linear ban dits and MDPs. They show regret bounds across the set of parallel tasks that improve as the number of related tasks in the set increases. Lu et al. [184] extend these results to general representations in contextual bandits. Additionally, Cheng et al. [45] reveal that multitask representation learning in low-rank MDPs [1] can significantly enhance sample efficiency when the total number of tasks surpasses a certain threshold. Their findings emphasize the efficiency of employing learned repre sentations in downstream tasks, thereby illustrating the tangible benefits of multi-task representation learning in reinforcement learning. 
Meta-RL specific theoretical advances While research specifically investigating the theory of meta-RL is limited, some important results have been obtained. In particular, results relating to the generalization bounds of meta-RL algorithms offer a principled motivation on the division of meta-RL methods into few-shot and many-shot methods introduced in section 2. 
Simchowitz et al. [277], Rimon et al. [252], and Tamar et al. [295] focus on generalization in meta RL. Tamar et al. [295] provide PAC bounds on the number of training tasks required for learning an approximately Bayes-optimal policy, i.e., a policy that optimizes equation 3. Rimon et al. [252] show that the bound depends exponentially on the degrees of freedom of the task distribution p(M). 
39

Jθ 
H, Large φi+1 
= φi + α∇φiJθ 

MDP 1 
πφi 
NNθ φi 
MDP 1 πφi+1 
... 

S0 S1 S2 S0 S1 Episode Episode 
S0 S1 S2 S0 S1 Episode Episode 

Figure 15: In many-shot meta-RL, the meta-parameters θ often parametrize a policy gradient objec tive function Jθ used for updating the parameters of the policy in the inner-loop ϕ. In this pattern, the inner-loop builds on the inductive bias of a policy gradient algorithm, which helps improve policies over long trials (large H. The meta-parameters are updated after a number of inner-loop updates, which may or may not correspond to the trial length depending on specific setting and the algorithm. 
These results give a principled explanation of why meta-RL methods work well for narrow task distributions but not for broader ones. Whereas Tamar et al. [295] and Rimon et al. [252] focus on in-distribution generalization, Simchowitz et al. [277] study the problem of out-of-distribution generalization by considering misspecified priors for Thompson sampling algorithms, which are often considered in meta-RL [241]. 
Continuing from the Bayesian perspective, Chen et al. [43] bound the regret of the Bayes-optimal policy compared to the optimal policy on any MDP instance possible under the prior. While Chen et al. [43] focus on the worst-case regret, Ye et al. [346] bound the expected regret. Furthermore, they propose a pre-training and fine-tuning algorithm based on policy elimination that achieves favorable regret. 
Finally, there are ongoing theoretical investigations targeted at MAML-like methods [73]. Fallah et al. [69] derive the sample complexity of a variant of the MAML algorithm. Liu et al. [176] and Tang [296] investigate the bias and variance of gradient estimators for MAML and the latter propose a new gradient estimator with a favorable bias-variance trade-off. 
4 Many-Shot Meta-RL 
In this section, we consider the many-shot setting where we want to, for example, learn a loss function that is applied on new tasks for thousands of updates, rather than just a handful. In other words, the goal is to learn a general purpose RL algorithm, similar to those currently used in practice. This setting is discussed separately from the few-shot setting presented in Section 3 because in practice it considers different problems and methods, even though increasing the trial length does not change the setting in principle. On one hand, a prototypical few-shot meta-RL problem is goal navigation in MuJoCo environments [302]. In this scenario, the agent adapts to the different rewards defined by the goal but operates within the same environment. On the other hand, a typical task in many-shot meta-RL might involve learning an objective function for a policy-gradient method for Atari [28] games. 
In the few-shot multi-task setting, an adaptive policy can successfully solve unseen tasks in a small number of episodes by making use of a systematic exploration strategy that exploits its knowledge of the task distribution. This strategy works well for narrow task distributions, e.g., changing the goal location in a navigation task. For more complex task distributions with more tenuous relationships between the tasks, the few-shot methods tend to not work as well [193, 252]. Rimon et al. [252] studies the complexity of meta-RL in terms of the degrees of freedom of the task distribution. See Section 3.8 for more details. Meta-RL methods designed for these more complicated conditions often resemble standard RL algorithms, which are in principle capable of solving any task given enough interactions. The drawback of standard RL algorithms is that they require many samples to solve tasks. The meta-RL algorithms building on them aim to improve the sample efficiency by explicitly optimizing for a faster algorithm. This algorithm template is illustrated in Figure 15. We label this setting multi-task many-shot meta-RL and discuss it further in Section 4.1. 
40
Another area where building on standard RL algorithms is helpful is many-shot single-task meta-RL. Instead of seeking greater generalization across complex task distributions, here the aim is to accel erate learning on a single hard task. Solving hard tasks requires many samples and leaves room for meta-learning to improve sample efficiency during the trial. We discuss this setting in Section 4.2. While the task distributions in the multi-task and single-task many-shot meta-RL problems are very different, we discuss the methods for both of them together in Section 4.3 due to the similarity of algorithms used in the settings. 
There is less research on the many-shot meta-RL setting compared to its few-shot counterpart. We believe this disparity is in large part due to the higher computational demands that follow from the very nature of the many-shot meta-RL setting, placing it beyond the reach of many academic groups. Furthermore, optimization in the many-shot setting is challenging due to high inner-loop computational costs, which limit the kinds of inner-loops that can be efficiently trained. For example, RL2could technically be used in the many-shot setting, but optimizing an RNN over millions of timesteps is beyond the current state of the art. These challenges are open problems in the many shot meta-RL setting, and are discussed further in Section 6. 
4.1 Multi-Task Many-Shot Meta-RL 
Many methods in this category are explicitly motivated by the desire to learn RL algorithms that learn quickly on any new task. This requires either a training task distribution that has support for all tasks of interest, or alternatively adaptation to tasks outside the training task distribution. 
The objective for the many-shot multi-task setting is the same as in the few-shot setting and is given by Equation 3. The differences are the much longer length of the trial H and the broader task distribution, which may include differing action and observation spaces between the tasks. There is no strict cutoff separating long and short trial lengths but the algorithms targeting each setting are generally easy to distinguish. On common benchmarks, such as MuJoCo [302], algorithms targeting the few-shot setting reach maximum performance usually after at most tens of episodes, while algorithms for the many-shot setting are tested on benchmarks like Atari [28] may take tens of thousands or more episodes to converge. As in the few-shot setting, both meta-training and meta testing consist of sampling tasks from a task distribution and running the inner-loop for H episodes. 
In practice, since a single trial consists of many inner-loop updates, optimizing the meta-parameters over full trials can be challenging. Running a complete trial in the inner-loop requires a lot of computation by itself, which can make the meta-parameter updates too expensive to compute. Fur thermore, even if it is feasible to sample full trials for each meta-parameter update, computing such update may not be possible due to memory requirements or optimization problems due to vanishing and exploding gradients. Instead, the meta-learner is commonly updated to maximize a surrogate objective: the performance of the policy after a small number of inner-loop updates starting from the current parameters. This surrogate objective is a biased estimator of the policy performance at the end of the trial but can still provide gains in practice. The bias from considering a truncated hori zon has been analyzed both in supervised learning [334, 196, 195] and meta-RL [314]. We discuss mitigation strategies for the bias from truncation in Section 4.3. 
A pseudocode template for a many-shot meta-RL algorithm is provided in algorithm 10. In the algorithm, the trials may or may not finish depending on the number of iterations N between each outer-loop update. Updating the meta-parameters in the middle of a trial corresponds to the truncated objective described above. 
In the multi-task many-shot setting, the meta-parameters are most commonly parameters of the objective function, which is differentiated with respect to the policy parameters. This results in a similar algorithm to MAML [73] described in Section 2.4. However, unlike in MAML, where the initialization of the policy is learned, the meta-parameters are in the update function, which enables considering arbitrary policy parameterizations in the inner-loop. Furthermore, while learning the initialization is a general meta-parameterization, the parametrized update function can be easier to optimize, especially in the many-shot setting. 
4.2 Single-Task Many-Shot Meta-RL 
For some tasks in deep RL, useful training distributions of tasks may not be available. Moreover, even when they are, the individual tasks may require too many resources to solve on their own making multi-task training across them too hard. These tasks have motivated researchers to look at whether meta-RL can accelerate learning even when the task distribution consists of a single task, 
41
Algorithm 10 Many-Shot Meta-Learning for Reinforcement Learning 
1: Initialize meta-parameters θ 
2: Sample a batch of tasks Mi ∼ p(M) 
3: Initialize policy parameters ϕi0for each task Mi 
4: Set j = 0 
5: while not done do 
6: for N iterations do 
7: for each task Mi do 
8: Collect data Dijusing the policy πϕij 
9: Update policy parameters: ϕij+1 = ϕij + α∇ϕijJθ(Dij, πϕij) 
10: end for 
11: j = j + 1 
12: end for 
13: Update θ to maximize the returns of the newest policies: θ′ = θ + β∇θPiJ(Dij, πϕij) 14: For tasks i for which the trial has concluded, re-initialize the policy parameters ϕi, and sample new tasks Mi ∼ p(M). 
15: end while 
i.e., accelerating learning online during single-task RL training. Note that this setting is sometimes additionally referred to as online cross-validation [291]. 
The goal in the single-task setting is to maximize the final performance of the policy after train ing has completed. The final performance is given by the meta-RL objective in Equation 3 when we choose a task distribution with only a single task, set the trial length H to a large number, and choose K such that only the last episode counts. Compared to the few-shot multi-task setting, when meta-learning on a single task, the optimization cannot wait until the inner-loop training has concluded and update only then because there is no further learning on which to use the updated meta-parameters. Therefore, single-task meta-learning necessarily follows a truncation pattern sim ilar to the one described in Algorithm 10. In contrast to the many-shot multi-task meta-RL algorithm described in the pseudocode, in the single-task setting there is only one task and as a consequence only one set of policy parameters ϕj . Those parameters are usually never reset. Instead, the inner loop keeps updating a single set of agent parameters throughout the lifetime of the agent. Therefore, Algorithm 10 is a fairly good representation of single-task meta-RL methods when choosing a task distribution with a single task and setting the trial length to the full length of training. 
This meta-learning problem is inherently non-stationary, as the data distribution changes with the changing policy. The ability of the meta-learners to react to the non-stationary training conditions for the policy is often considered a benefit of using meta-learning to accelerate RL, but the non stationarity itself results in a challenging meta-learning problem. 
Many of the methods for single-task meta-RL are closely related to methods for online hyperparam eter tuning. Indeed, there is no clear boundary between single-task meta-RL and online hyperparam eter tuning, though generally hyperparameters refer to the special case where the meta-parameters θ are low-dimensional, e.g., corresponding to the learning rate, discount factor, or λ in TD(λ). While such hyperparameters are often tuned by meta-RL algorithms [341, 355], to limit the scope of our survey, we only consider methods that augment the standard RL algorithm in the inner-loop by in troducing meta-learned components that do not have a direct counterpart in the non-meta-learning case. For a survey of methods for online hyperparameter tuning, see Parker-Holder et al. [227]. 
4.3 Many-Shot Meta-RL Methods 
Algorithms for many-shot meta-RL aim to improve over the plain RL algorithms they build upon by introducing meta-learned components. The choice of meta-parameterization depends on the problem. The meta-parameterizations differ in how much structure they can capture from the task distribution, which matters in the multi-task case, and what aspects of the RL problem they address. The meta-parameterization may tackle problems such as credit assignment, representation learning, etc. The best choice of meta-parameterization depends on what aspect of the problem is the primary challenge. 
42
Meta 
parameterization Multi-task Single-task 

Intrinsic rewards 
Auxiliary tasks 
Hyperparameter functions 
Hierarchies 
Objective functions directly 
Alet et al. [7], Zheng et al. [364], Veeriah et al. [312], Zou et al. [373], and Meier et al. [191] 
Frans et al. [82], Fu et al. [84], Veeriah et al. [312], and Nam et al. [214] 
Houthooft et al. [123], Kirsch et al. [146], Oh et al. [220], Bechtle et al. [20], and Jackson et al. [130] 
Zheng et al. [365] and Rajendran et al. [240] 
Lin et al. [171], Veeriah et al. [311], Zahavy et al. [355], and Flennerhag et al. [78] 
Almeida et al. [9], Flennerhag et al. [78], Lu et al. [181], and Luketina et al. [185] 
Xu et al. [340] 

Optimizers Chen et al. [44] and Lan et al. 
[154] 
Black-box Kirsch et al. [143] 
Table 7: Many-shot meta-RL methods discussed in Section 4.3 categorized by the task distribution considered and meta-parameterization. 
Many of the topics discussed below, such as intrinsic rewards and hierarchical RL, are active research areas in RL on their own. Most of the research on these topics does not consider the bi-level structure present in meta-RL. We provide a concise description of each topic in general terms but do not to provide details on the bodies of research outside of their intersection with meta-RL. For learning more about these topics, we provide references to foundational papers and surveys. 
In the following, we discuss the different meta-parameterizations considered both in the single-task and multi-task settings. A summary of the methods discussed in this section is presented in Table 7, where the different methods are categorized by task distribution and meta-parameterization. The empty categories such as single-task meta-RL for learning hierarchical policies may be promising directions for future work. We also discuss the different outer-loop algorithms considered for many shot meta-RL. 
Learning intrinsic rewards The reward function of an MDP defines the task we want the agent to solve. However, the task-defining rewards may be challenging to learn from because maximizing them may not result in good exploratory behavior, e.g., when rewards are sparse [279]. One approach for making the RL problem easier is to introduce a new reward function that can guide the agent in learning how to explore. These additional rewards are called intrinsic motivation or intrinsic rewards [16]. While intrinsic rewards are often designed manually, recently many-shot meta-RL methods have been developed to automate their design. The learned intrinsic reward functions can be represented as functions of the state and action rin(st, at) [365, 240, 312], potential-based shaping functions γrin(st+1) − rin(st) [373], or functions of the entire episode so far rin(τ:t) [7, 364]. Learning an intrinsic reward in the multi-task case can help the agent learn to explore the new environment more quickly [7, 364, 373]. They can also help define skills as part of a hierarchical policy [312] or be used as general reward shaping in the single-task case [365]. Beyond the standard settings, Rajendran et al. [240] learn intrinsic rewards for practicing in extrinsic reward-free episodes in-between evaluation episodes. Finally, Meier et al. [191] present an unsupervised reward learning approach, which learns complex skills in Atari games. 
Learning auxiliary tasks In some RL problems, learning a good representation of the observa tions is a significant challenge for which the RL objective alone may provide poor supervision. One approach for better representation learning is introducing auxiliary tasks, defined as unsupervised or self-supervised objectives optimized alongside the RL task [131]. With auxiliary tasks the inner loop objective then becomes 
Jθ(D, πϕ) = JRL(D, πϕ) + Jaux 
θ(D, πϕ). 
43
When a set of candidate auxiliary tasks is known, the best ones to use can be chosen by meta learning the weight associated with each task, such that only the tasks that improve the outer-loop objective are assigned high weights [171]. Even when auxiliary tasks are not known in advance, meta-learning can be useful. Veeriah et al. [311] use meta-gradients to discover a set of generalized value functions (GVFs) [292], which define the prediction targets for the auxiliary loss of the policy network in the inner-loop. They show that the learned auxiliary tasks can improve sample efficiency over the base algorithm without auxiliary tasks and over handcrafted auxiliary tasks. The same approach for auxiliary task learning is used by Zahavy et al. [355] and Flennerhag et al. [78], who further improve the performance by tuning the hyperparameters of the inner-loop RL algorithm online. At the time of publication both achieved the state-of-the-art performance of model-free RL on the Atari benchmark [28]. 
Learning functions that output hyperparameters Methods that learn functions that output hy perparameters of an inner-loop algorithm straddle the gap between hyperparameter optimization and meta-learning. Almeida et al. [9], Flennerhag et al. [78], and Luketina et al. [185] learn functions that take as inputs summary statistics of the inner-loop performance such as rewards and TD-error, and output the values of hyperparameters such as the λ-coefficient used in estimating returns. Fur thermore, Lu et al. [181] show that parametrizing the policy update size coefficient featured in algorithms such as proximal policy optimization (PPO) [267] by a meta-learned function can be beneficial. The parameters of these functions are themselves optimized by meta-gradients. 
Modifying the RL objective directly Learning intrinsic rewards and auxiliary tasks shows that adding meta-learned terms to the RL objective can accelerate RL. These successes raise the question whether, instead of adding terms to the objectives, modifying the RL objectives directly via meta-RL can improve performance. To answer this question Houthooft et al. [123], Oh et al. [220], and Xu et al. [340] propose algorithms that replace the return or advantage estimator in a policy gradient algorithm with a learned function of the episode 

∇ϕJθ(τ, πϕ) ∝X at,st∈τ 
∇ϕ log πϕ(at|st)fθ(τ ), 

where fθ(τ ) is some meta-learned function of the trajectory. An alternative to replacing the ad vantage estimator is proposed by Kirsch et al. [146] and Bechtle et al. [20], who consider a deep deterministic policy gradient (DDPG)-style [169] objective function, where the critic is learned via meta-RL instead of temporal difference (TD) learning. 

∇ϕJθ(τ, πϕ) = X at,st∈τ 
∇ϕQθ(st, πϕ(at|st)), 

where Qθ is the meta-learned critic. Similar inner-loop is proposed for meta-IL by Yu et al. [350]. These learned RL objectives produce promising results in both multi-task and single-task meta-RL. In the multi-task setting, Oh et al. [220] demonstrate that an objective function learned on simple tasks such as gridworld can generalize to much more complicated tasks such as Atari [28]. Jackson et al. [130] improve the generalization of the approach further by replacing the hand-designed training environments with an automated environment design component. Whereas in the single-task setting, Xu et al. [340] show that the learned objective function can eventually outperform the standard RL algorithm (IMPALA [66]) it builds upon. 
Learning Optimizers In most learning systems, the optimizer producing the parameter updates is manually designed. However, it is also possible to meta-learn an optimizer. Typically, the inner loop of meta-learned optimizers conditions on losses and gradients, and outputs parameter updates. There has been some success in both many-shot and few-shot supervised learning to meta-learn the optimizer [12, 165, 243]. Some of these meta-learned optimizers use RL for meta-training [165], and some deploy on MDPs [44]. Recently, a many-shot method has been proposed to meta-train and meta-test on MDPs, making it the first proper meta-RL method to learn an optimizer [154]. 
Learning hierarchies A central problem in RL is making decisions when the consequences only become apparent after a long delay. Temporal abstraction is a potential solution to this problem [54, 293, 209]. These abstractions are commonly represented as a hierarchy consisting of a set of op tions or skills and a manager policy that chooses what skills to use [293, 17, 229]. When options are available for a given environment, the manager policy can be learned with algorithms closely related to standard RL algorithms targeting the hierarchical setting. However, manually designing 
44

Figure 16: To estimate an update to the meta-parameters, ES samples a set of them and computes the inner-loop for each of them independently. This requires more evaluations of the inner-loop but can work better when the task-horizon is long. 
good options is challenging. Instead, meta-RL can be used in the discovery of options [82, 312] by parametrizing the functions defining the objectives for learning the option with meta-learned functions. While meta-RL can help hierarchical RL the opposite is also true as a hierarchical struc ture can help with the central problem of meta-RL, i.e., learning policies that generalize across task distributions. Fu et al. [84] and Nam et al. [214] propose methods that make use of the temporal abstraction provided by hierarchical policies to solve task distributions with longer horizons. 
Black-box meta-learning In few-shot meta-RL, black-box methods that use RNNs or other neu ral networks instead of stochastic gradient descent (SGD) tend to learn faster than the SGD-based alternatives. Kirsch et al. [143] argue that many black-box meta-RL approaches, e.g., [62, 321] cannot generalize well to unseen environments because they can easily overfit to the training en vironments. To combat overfitting, they introduce a specialized RNN architecture, which reuses the same RNN cell multiple times, making the RNN weights agnostic to the input and output di mensions and permutations. The proposed method requires longer trials to learn a policy for a new environment, making it a many-shot meta-RL method, but in return it can generalize to completely unseen environments. 
Outer-loop algorithms Regardless of the inner-loop parameterization chosen, by definition, algo rithms for many-shot meta-RL have to meta-learn over long task-horizons. Directly optimizing over these long task horizons is challenging because it can result in vanishing or exploding gradients and has infeasible memory requirements [290, 195]. Instead, as described above, most many-shot meta RL algorithms adopt a surrogate objective, which considers only one or a few update steps in the inner-loop [365, 146, 311, 220, 240, 355, 364, 20, 312]. These algorithms use either A2C [204]-style [220, 364, 20, 312] or DDPG [169]-style [146] actor-critic objectives in the outer-loop. Flennerhag et al. [78] present a different kind of surrogate objective, which bootstraps target parameters for the inner-loop by computing several updates ahead and then optimizing its earlier parameters to mini mize distance to that later target using a chosen metric. This allows optimizing over more inner-loop updates, and with the right choice of metric, it can be used for optimizing the behavior policy in the inner-loop, which is difficult using a standard actor-critic objective. This surrogate objective has also been used to meta-learning how to prioritize the task distribution for policy improvement in model-based methods [38]. 
Alternatively evolution strategies (ES) [244, 329, 261], which are black-box optimization algo rithms, are used by [123, 143, 181]. ES works by sampling a set of parameters from a distribution, evaluating the set by running the inner-loop, and updating the parameters of the distribution. This can be seen as applying REINFORCE [330] on the parameter distribution. The general approach of using ES in the outer-loop is illustrated in Figure 16. ES suffers less from the vanishing and explod ing gradients problem and has more favorable memory requirements at the cost of high variance and sample complexity compared to SGD-based methods [195]. Finally, genetic algorithms [263] and 
45

Figure 17: Example meta-RL application domains, including building energy control [186, 100], traffic signal control [356], multi-agent RL [79, 102, 274, 223, 41, 72, 106, 141, 359, 370, 93, 115, 183, 343], robot manipulation [353, 3, 14, 266, 94, 361], robot locomotion [353, 268, 283, 352, 148, 150], and automatic code grading in education [178]. Image credit to [310, 42, 309, 351, 148, 64, 178]. 
random search are used by Alet et al. [7], Co-Reyes et al. [250], and Garau-Luis et al. [90], who consider discrete parameterizations of the inner-loop objective. 
5 Applications 
In many application domains, fast adaptation to unseen situations during deployment is a critical consideration. By meta-learning on a set of related task, meta-RL provides a promising solution in these domains (Figure 17), such as traffic signal control [356], building energy control [186, 100], and automatic code grading in education [178]. Meta-RL has also been used as a subroutine to address non-stationarity in the sub-field of continual RL [274, 212, 251, 30, 174, 331]. Moreover, curriculum learning and unsupervised environment design (UED) have been used to provide the distribution of tasks for a meta-RL agent [190] or an otherwise adaptive agent [57]. In this section, we consider where meta-RL has been most widely applied, as well as intersections with other sub fields where meta-RL has been used successfully to solve problems. Specifically, we discuss robotics and multi-agent RL. 
5.1 Robotics 
One important application domain of meta-RL is robotics, as a robot needs to quickly adapt to different tasks and environmental conditions during deployment, such as manipulating objects with different shapes, carrying loads with different weights, etc. Training a robot from scratch for each possible deployment task may be unrealistic, as RL training typically requires millions of steps, and collecting such a large amount of data on physical robots is time-consuming, and even dangerous when the robots make mistakes during learning. Meta-RL provides a promising solution to this challenge by meta-learning inductive biases from a set of related tasks to enable fast adaptation in a new task. 
However, while meta-RL enables efficient adaptation during deployment, it comes at the expense of sample-inefficient meta-training on multiple tasks, which is bottlenecked by the cost of collecting an even larger amount of real-world data for meta-training. Nonetheless, some methods meta-train in the real world [211, 25, 363, 316, 361], and even learn to manually reset their tasks while doing so [316]. However, most methods train a meta-RL agent in simulation instead of on physical robots, where different tasks can be easily created by changing the simulator parameters, and then deploy the meta-trained robot in the real world. This process, known as sim-to-real transfer [362], significantly 
46
Black-box Task-inference PPG 

Model-free 
Sim-to-real 
Real-world meta 
training 
Akkaya et al. [3] and Schwartzwald et al. [268] 
- 
Yu et al. [353], Schoettler et al. [266], Kumar et al. [148], Kumar et al. [150], and He et al. [115] 
Zhao et al. [363], Walke et al. [316], and Zhao et al. [361] 
Gao et al. [89], Arndt et al. [14], Song et al. [283], 
Yu et al. [352], and Ghadirzadeh et al. [94] 
- 

Sim-to-real Cong et al. [51] - Kaushik et al. [139] and Anne et al. [13] 

Model-based 
Real-world meta 
training 
Nagabandi et al. [211] and Belkhale et al. [25] 
- - 

Table 8: Taxonomy of the meta-RL papers that apply meta-RL to robotics from Section 5.1. Methods for meta-RL can be directly applied to robotics and robotics span the range of meta-RL methods. Task-inference and Model-based methods are common in this section for their sample efficiency. Real-world meta-training is relatively less common. 
reduces the meta-training cost and is adopted by most methods that apply meta-RL to robotics [353, 3, 14, 51, 139, 266, 268, 283, 352, 94, 148, 150, 115]. We give a taxonomy of these robotic meta-RL papers in Table 8 and introduce them in more detail below. 
Model-free meta-RL methods directly adapt the control policy to handle unseen situations during deployment, such as locomotion of legged robots with different hardware (mass, motor voltages, etc.) and environmental conditions (floor friction, terrain, etc.) [353, 268, 283, 352, 148, 150], and manipulation with different robot arms or variable objects [353, 3, 14, 266, 94, 361]. On one hand, black-box methods [3, 268] (see Section 3.2) and task-inference methods [353, 266, 352, 148, 150, 115, 361] (see Section 3.3), when used in robotics, generally condition the policy on a context vector inferred from historical data to represent the current task. They mainly differ in what kind of loss function is used to train the task inference. On the other hand, model-free PPG methods (see Section 3.1) in robotics [89, 14, 283, 94] mainly build upon MAML [73]. However, they usually require more rollouts for adaptation compared to model-free black-box methods, which is consistent with our discussion on the efficiency of different methods in Section 3. Moreover, instead of directly using MAML, these methods introduce further modifications to enable sample-efficient training [14, 94] and handle the high noise of the real world [283]. 
Another line of works adopts model-based meta-RL (Section 3.7). Model-based methods may be more suitable for robotic tasks for the following reasons: (1) Model-based RL can be more effi cient than model-free methods, a key consideration for robot deployment [232]. (2) Adapting the dynamics model may be much easier than adapting the control policy in some cases, such as in tasks where task difference is defined by various dynamics parameters. Similar to the model-free case, both black-box methods [211, 51, 25] and PPG methods [211, 139, 13] have been considered adapt ing the dynamics model for model-based control. (We generally avoid classification of methods as task-inference and model-based. See Section 3.7.) 
5.2 Multi-Agent RL 
Meta-learning has been applied in multi-agent RL to solve a number of problems, from learning with whom to communicate [359], to automating mechanism design by learning agent-specific re ward functions [343], to meta-learning an agent for computing Stackelberg equilibria, by rapidly providing an adaptive best-response policy when training a fixed leader policy [93]. All problems and solutions are summarized in Table 9. However, in this section we focus on two main problems that meta-RL can address in the multi-agent setting. First, we introduce the problems of generaliza tion to unseen agents and non-stationarity, and discuss how meta-RL can address them in general. Then, we discuss the types of meta-RL methods that have been used to address each problem, elab orating on PPG methods that propose additional mechanisms for each problem. 
47
Meta-Training Meta-Testing 
Figure 18: Illustration of using meta-RL to address generalization over unseen agents. By general izing over other agents at meta-train time, we can adapt to new agents at meta-test time. 
Sub-topic Papers 
PPG meta-gradients for non-stationarity Foerster et al. [79], Al-Shedivat et al. [274], and Kim et al. [141] 
PPG for opponent curriculum Feng et al. [72] and Gupta et al. [106] PPG for meta-learning reward functions Yang et al. [343] 
PPG for meta-learning communication 
topology Yang et al. [343] 
Black box for generalization to teammates Charakorn et al. [41] Black box for generalization to opponents Lu et al. [183] 
Task inference for generalization to 
teammates Grover et al. [102] and Zintgraf et al. [370] 

Task inference for generalization to opponents 
Grover et al. [102], Papoudakis et al. [223], and Zintgraf et al. [370] 

Task inference for generalization to humans He et al. [115] 
Non-adaptive agent for learning cheap talk Lu et al. [182] 
Non-adaptive agent for Stackelberg 
equilibria Gerstgrasser et al. [93] 
Table 9: Summary of meta-RL papers used for multi-agent RL from Section 5.2. Methods use PPG, black-box, and task-inference agents, in addition to non-adaptive agents. As examples, the problems addressed include generalization over other agents policies, non-stationarity induced by other agents, computing Stackelberg equilibria, and learning communication patterns. 
The first multi-agent problem we consider is generalization over other agents. In multi-agent RL, many agents act in a shared environment. Often, it is the case that other agents’ policies vary greatly. This creates a problem of generalization to unseen agents. This generalization may occur over opponents [223, 183], or over teammates [41, 106, 115], which is sometimes called ad hoc teamwork [286]. The other agents may be learned policies [370] or even humans [115]. By viewing other agents as (part of) the task, and assuming a distribution of agents available for practice, meta RL is directly applicable. Using meta-RL to address generalization over other agents is visualized in Figure 18. 
The second multi-agent problem we consider is non-stationarity. In multi-agent RL, from the per spective of any one agent, all other agents change as they learn. This means that from one agent’s perspective, if all of the other agents are modeled as part of the environment, then the environment changes – i.e., the problem is non-stationary [79, 274, 141]. Meta-RL likewise can address non stationary by treating other learning agents as (part of) the task. In this case, the learning algorithm of each agent, and what each agent has learned so far, collectively defines the task. By repeatedly re setting the other learning agents during meta-training, we can meta-learn how to handle the changes introduced by the other agents. From the perspective of the meta-learning agent, the distribution over other agents remains stationary. This effectively resolves the non-stationarity of multi-agent RL, which is depicted in Figure 19. 
Solutions in multi-agent RL make use of all different types of meta-RL methods: PPG methods [79, 274, 141], black-box [41, 183], and task-inference methods [102, 223, 370, 115]. These methods are discussed in Sections 3.1, 3.2, and 3.3, respectively. The agent may even use a Markovian (i.e., 
48
Non-Stationarity Meta-Learning 
Figure 19: Illustration of using meta-RL to train over another agent’s learning in multi-agent RL. By resetting the learning process of other agents, and training over it, we can learn to address the non-stationarity created by the learning of other agents. 
non-adaptive) policy, if the other agents are learning [182]. Most of these methods can be applied without modification to the underlying meta-RL problem to address both generalization over other agents and non-stationarity. In the case that other-agents form the entirety of the task, then the 

meta-RL objective can be written: J (θ) = Eπ′i∼p(π′i)∀i=0...A
  
ED 
  X τ∈DK:H 
G(τ ) 
    fθ, π′0...π′A  , (7) 

where A is the number of other agents. However, several PPG papers investigate additional mecha nisms for both such generalization and non-stationarity that we discuss next. 
Some PPG papers focus on improving the distribution of other agents using meta-gradients, in order to improve generalization to new agents. See Section 3.1 for a discussion of meta-gradient estima tion. These papers focus on the curriculum of opponents to best support learning in a population based training setting. Gupta et al. [106] iterates between PPG meta-learning over a distribution of fixed opponents and adding the best-response to the meta-learned agent back to the population. Alternatively, Feng et al. [72] uses meta-gradients or evolutionary strategies to optimize a neural network to produce opponent parameters. The opponent parameters are chosen to maximize the learning agent’s worst case performance. Both methods use the distribution of agents to create a robust agent capable of generalizing across many other agents. 
Finally, some PPG papers introduce additional mechanisms to handle non-stationarity. In order to resolve all non-stationarity, all other (adaptive or non-adaptive) agents must repeatedly reset to their initial policies eventually. While this has been explored [223, 41, 370], PPG methods tend to allow other agents to continue to learn [79, 274], or even to meta-learn [141], without resetting. Each PPG method addresses the non-stationary learning of other agents in a different way. For example, Al-Shedivat et al. [274] propose meta-learning how to make gradient updates such that performance improves against the subsequent opponent policy, over various pairs of opponent policies. In con trast, Foerster et al. [79] derive a policy gradient update such that one agent meta-learns assuming the rest follow an exact policy gradient, and Kim et al. [141] derive a policy gradient update assuming all agents sample data for meta-learning. 
6 Open Problems 
Meta-RL is an active area of research with an increasing number of applications, and in this nascent field there are many opportunities for future work. In this section, we discuss some of the key open questions in meta-RL. Following the method categories in this survey, we first discuss some important directions for future work in few-shot and many-shot meta-RL in Section 6.1 and 6.2 respectively. Following this, we discuss how to utilize offline data in Section 6.3 in order to eliminate the need for expensive online data collection during adaptation and meta-training. Finally, we take a critical stance and evaluate some potential limitations of meta-RL research. 
6.1 Few-Shot Meta-RL: Generalization to Broader Task Distributions 
As discussed in Section 3, few-shot meta-RL methods are a promising solution to fast adaptation on new tasks. However, so far their success is mainly achieved on narrow task distributions, while 
49

Narrow  
Meta-Training 
Broad 
Meta-Training 
Meta-Training Meta-Testing xOOD Task 
Distribution Shift 

Figure 20: Illustration of broad task distributions (left) and OOD generalization (right). There is a clear need for meta-training distributions with both more tasks and more diverse tasks. Novel methods may be needed for such distributions, including methods that fail gracefully when out of distribution. 
the ultimate goal of meta-RL is to enable fast acquisition of entirely new behaviors. Consequently, future work should focus more on generalization of few-shot meta-RL methods to broader task dis tributions. A straightforward way to achieve this goal is to meta-train on a broader task distribution to learn an inductive bias that can generalize to more tasks. However, training on a broader task distribution also introduces new challenges (such as a harder exploration problem) that are beyond the scope of existing methods. For an overview of methods for generalization in RL, beyond meta RL, see Kirk et al. [142]. Moreover, even when trained on a broad task distribution, the agent may still encounter test tasks that lie outside the training distribution. Generalization to such out-of distribution (OOD) tasks is important since a meta-RL agent is likely to encounter unexpected tasks in the real world. For a summary of the task distributions in existing benchmarks, see Table 10. We discuss these two open problems, (illustrated in Figure 20), in more detail below. 
Training on broader task distributions for better generalization The meta-training task distri bution plays an important role in meta-RL, as it determines what inductive bias we can learn from data, while the meta-RL algorithms determine how well we can learn this inductive bias. The task distribution should be both diverse enough so that the learned inductive bias can generalize to a wide range of new tasks, and clear enough in task structure so that there is indeed some shared knowledge we can utilize for fast adaptation. 
However, existing few-shot meta-RL methods are frequently meta-trained on narrow task distribu tions, where different tasks are simply defined by varying a few parameters that specify the reward function or environment dynamics [62, 73, 241, 372]. While the structure between such tasks is clear, the narrowness of the distribution poses problems. For example, in this setting, task inference is generally trivial. Often, the agent can infer the parameters that define the task based on just a few transitions. This makes it hard to evaluate if a meta-RL method can learn systematical explo ration behaviors to infer more sophisticated task structures. In general, the inductive bias that can be learned from a narrow task distribution is highly tailored to the specific training distribution, which provides little help for faster acquisition of entirely new behaviors in a broader task domain. Indeed Zhao et al. [360] find that meta-RL methods are no better than multi-task pre-training when tested on generalization to complex visual tasks like Atari games. 
Consequently, we need to design benchmarks that are diverse, in addition to having clear task struc tures. Such a benchmark would better reflect the complex task distribution in real-world problems, and promote the design of new methods that can generalize over these challenging tasks. For ex ample, some recent robotic benchmarks [133, 351] introduce a wide range of manipulation tasks in simulation with not only parametric diversity (such as moving an object to different goals), but also non-parametric diversity (such as picking an object and opening a window). Game benchmarks with procedurally generated environments provide another good testbed to evaluate meta-RL on broader task distributions [218, 49, 151, 285, 319], as novel environments with both diverse and clear task structures can be easily generated by following different game rules. For example, Alchemy [319] is a video game benchmark with the goal of transforming stones with potions into more valuable forms, which requires the agent to strategically experiment with different hypotheses for efficient exploration and task inference. As another example, the Adaptive Agent (Ada) evaluated on XLand 2.0 learns to experiment, use tools, and navigate in a 3D open-ended environment. In addition 
50
Benchmark Name Citation Setting Number of Tasks XLand 2.0 (private) Team et al. [298] Game (Diverse) > 1040 Alchemy Wang et al. [319] Game (Diverse) > 167, 424 NetHack Küttler et al. [151] Game (Diverse) >> 200, 000 Procgen Cobbe et al. [49] Game (Diverse) >> 200, 000 Sonic Nichol et al. [218] Game (Diverse) 58 discrete Meta Arcade Staley et al. [285] Game (Diverse) 24; continuous (e.g., ball size) DreamerGrader Wang et al. [319] Game (Narrow) 3,556 discrete 2D Navigation e.g., [73, 372] Game (Narrow) 25 discrete or continuous goal RLBench James et al. [133] Robotic (Diverse) 100 discrete Meta-World Yu et al. [351] Robotic (Diverse) 45 discrete; continuous (e.g., goal) MuJoCo[302] Locomotion e.g., [73, 372] Robotic (Narrow) e.g., 2 discrete or continuous goal 
Table 10: Summary of existing benchmarks in meta-RL by setting and number of tasks. MuJoCo and 2D Navigation are perhaps the most common, but also the narrowest meta-training distributions. XLand, Alchemy, NetHack, and Procgen are procedurally generated and so contain many tasks. NetHack and Procgen, however, were not designed to evaluate adaptation in meta-RL, and XLand is private. Additionally, benchmarks that contain both discrete tasks and diverse behavior generally require many tasks in order to generalize to new tasks. 
to these simulation benchmarks, Liu et al. [178] introduce a real-world benchmark which requires systematic exploration to discover the errors in different programs for coding feedback. Existing meta-RL methods cannot yet achieve satisfactory performance on many of these more challenging benchmarks [10, 351, 319, 188], which shows that generalization to a wider task distribution is still an open problem, and more attention should be paid to these more challenging benchmarks to push the limit of meta-RL algorithms. 
Still, both new methods and new benchmarks are still needed in meta-RL. Novel methods, such as the use of curriculum learning [190, 298] and active selection of tasks in the distribution based on task descriptions [136], will likely be needed as well to address sufficiently broad distributions. Many of these benchmarks are insufficient as well, with more work needed to improve the task distribution itself. For example, some of benchmarks with the widest task distributions are private [298], or not designed to test adaptation in meta-RL [49, 151]. It is also possible that some of these benchmarks simply need to be more densely populated with tasks in order for few-shot adaptation to be learnable. For example, Meta-World [351] has benchmarks with 1, 10, or 45 discrete tasks; however, meta-learning an algorithm that can reliably adapt to unseen meta-test tasks may require tens of thousands of meta-training tasks [145]. Adapting benchmarks [29] from the related Con textual MDP literature [112] in order to remove the task identity from the state is a path forward. Procedurally generating tasks is also one promising path for benchmarks [49, 151, 319, 298]. When tasks have manually designed discrete variation, it can be difficult to determine whether meta-test tasks even have support within the meta-training distribution. 
Generalization to OOD tasks In few-shot meta-RL, it is commonly assumed that the meta training and meta-test tasks are drawn from the same task distribution. However, in real-world problems, we usually do not know a priori all the situations the agent may face during deployment, and the RL agent will likely encounter test tasks that lie outside the meta-training task distribution. 
One key challenge here is that we do not know to what extent the learned inductive bias is still helpful for solving the OOD tasks. For example, in the navigation task in Figure 14, the learned inductive bias is an exploration policy that traverses the edge of the semicircle to find the goal first. If we consider OOD tasks of navigation to goals on the edge of a semicircle with a larger radius, then the learned exploration policy is no longer optimal, but may still help the agent explore more efficiently than from scratch. However, if the OOD tasks are navigation to goals on a semicircle in the opposite direction, then the learned inductive bias may be actively harmful and slow down learning. 
Consequently, simply using the learned inductive bias, which is commonly adopted by existing few-shot meta-RL methods, is not sufficient for OOD generalization. Instead, the agent needs to adaptively choose how to utilize or adjust the learned inductive bias according to what kinds of OOD tasks it is solving. On one hand, we want to ensure that the agent can generalize to any OOD task given enough adaptation data, even if the learned inductive bias is misspecified. In principle, 
51
PPG methods can satisfy this requirement while black-box methods cannot, echoing our discus sion on the trade-off between generalization and specialization of few-shot meta-RL methods in Section 3.1. However, in practice, if the meta-testing task distribution is sufficiently dissimilar to the meta-training task distribution, then any sort of meta-learning can be catastrophic [337]. On the other hand, we want to utilize as much useful information from the learned inductive bias as possible to improve learning efficiency on OOD tasks. Although recent works investigate how to improve generalization [153, 173, 100, 337, 115, 127, 99, 323, 2] or adaptation efficiency [68, 193, 161] on OOD tasks with small distribution shifts, more work remains to be done on how to adaptively handle larger distribution shifts between training and test tasks. Ideally, OOD methods make use of their inductive bias where possible, and fail gracefully (e.g., defaulting to an engineered learning algorithm), where it is not possible. 
6.2 Many-Shot Meta-RL: Optimization Issues and Standard Benchmarks 
For many-shot meta-RL (see Section 4), outer-loop optimization poses significant problems, some of which remain open. Moreover, there is a lack of standard benchmarks to compare different many shot meta-RL methods, an important gap to fill for future work. 
Optimization issues in many-shot meta-RL In many-shot meta-RL, the inner-loop updates the policy many times, which leads to a challenging optimization problem in the outer-loop due to not smooth objective surfaces [195] and high computation cost. To deal with these challenges in practice, most methods use the performance after relatively few inner-loop updates compared to the full inner-loop optimization trajectory as a surrogate objective. Updating the inner-loop after only a fraction of the lifetime leads to bias in the gradient estimation, which can be detrimental to meta learning performance [334]. How to tackle this optimization issue remains an open problem. One approach is to use gradient-free optimization methods such as evolution strategies [244] as was done by Kirsch et al. [143], but its sample complexity is much worse than gradient-based optimization in settings where those are applicable. 
Truncated optimization in many-shot single-task meta-RL Even if optimizing over long life times was possible in the multi-task setting, in the single-task setting we still need to update the inner-loop before learning has finished in order for it to be useful. One approach to improve trun cated optimization on a single task is the bootstrapped surrogate objective [78], which approximates an update for a longer truncation length with a bootstrapping objective on a shorter truncation length. However, this also introduces a biased meta-gradient estimation. Another solution computes meta gradients with different numbers of updates in the inner-loop and then computes a weighted average, similar to TD(λ) [32]. Additionally, there are still more sources of bias in this setting, such as bias from the reuse of critics between the inner- and outer-loops [33]. More research is required to choose the optimal bias-variance trade-off for meta-gradient estimators under the single-task setting. 
Non-stationary optimization in many-shot single-task meta-RL Another central challenge in many-shot single-task meta-RL is the non-stationarity of the inner-loop. In multi-task meta-RL, the inner-loop revisits the same tasks multiple times, allowing the meta-learner to fit to the stationary training task distribution. In the single-task case, however, the agent parameters keep changing, making the meta-learning problem non-stationary. Learning in a non-stationary problem is an open area of research extending beyond online meta-RL. 
Benchmarks for many-shot meta-RL Many-shot meta-RL methods are mainly evaluated on a wide range of commonly used RL tasks, such as Atari [28], classic control [35] and continuous control [61], to show that the learned RL algorithms have good generalization. However, some papers test generalization across different domains while others evaluate within a single domain, and there are no unified criteria on how to split the training and test tasks on the chosen domain(s). In the single-task setting, a benchmark for hyperparameters tuning in RL has been proposed, but focuses on a fixed and small number of discrete meta-parameters [271]. In the multi-task setting, to better evaluate generalization of the learned algorithms, it could be helpful to design and adopt benchmarks that choose the meta-train and meta-test tasks based on some unified standard. In this direction, a useful benchmark may even provide multiple groups of train and test tasks in order to gradually increase the difficulty and degree of transfer required, where ideally the degree of transfer is quantifiable by some measure of similarity across the MDPs [11]. Moreover, the ultimate goal of many-shot meta-RL is to design general-purpose RL algorithms that can work well on any reasonable MDP. However, the exact task structure in such a distribution over all “reasonable” MDPs 
52

Online Outer-Loop 
MDP 1 
S0 S1 S2 S0 S1 
Online Inner-Loop 
Online Outer-Loop 
Offline Outer-Loop 
MDP 1 
S0 S1 S2 S0 S1 
Online Inner-Loop 
Offline Outer-Loop 

MDP 1 
π 
MDP 1 
π 

S0 S1 S2 S0 S1 Offline Data Offline Inner-Loop 
S0 S1 S2 S0 S1 Offline Data Offline Inner-Loop 

Figure 21: The four different settings using online or offline data. Using an online outer-loop and online inner-loop is the standard setting. Using an offline outer-loop with offline inner-loop allows for safe meta-learning and safe-adaptation by forgoing all exploration, but places additional de mands on the data provided. Using an offline outer-loop with an online inner-loop allows for safe meta-learning of exploration behavior, but introduces difficulties such as extensive distribution shift. Using an online outer-loop with offline inner-loop allows for meta-learning offline RL algorithms without distribution shift, but still requires online samples during meta-learning. 
is still unclear and needs clarification, as some structure must be shared across these MDPs to allow for meta-learning in the first place. 
6.3 Utilizing Offline Data in Meta-RL 
So far the majority of research in meta-RL focuses on the setting of using online data for both the outer-loop and the inner-loop. However, when offline data is available, utilizing it properly is an effective way to reduce the need for expensive online data collection during both meta-training (the outer-loop) and adaptation (the inner-loop). Depending on which kind of data is available for the outer-loop and inner-loop respectively, we have four different settings, depicted in Figure 21. Apart from fully online meta-RL, the remaining three settings are still underexplored, and we discuss them below. 
Offline outer-loop and offline inner-loop Under this setting, the agent can only adapt with of fline data, and learn to optimize its adaptation strategy also with offline data in the outer-loop [157, 202, 167, 203, 170, 354]. This is particularly suitable for scenarios where online exploration and data collection is costly and even dangerous, while offline logs of historical behaviors are abundant, such as in robotics and industrial control [162]. However, the offline setting also introduces new challenges. First, as in standard offline RL, R(τ ) must be estimated solely from offline data, creat ing issues such as the overestimation of returns [85, 162]. Second, and unique to meta-RL, instead of doing online exploration to collect D:K for adaptation, we can only adapt with whatever offline data is provided on each task. Consequently, the adaptation performance critically depends on how informative the offline data is about the task. Existing works mainly investigate this offline setting on simple task distributions where task identity can be easily inferred from a few randomly sam pled transitions in the offline data. However, how to adapt with offline data on more complicated task distributions (such as those discussed in Section 6.1), and how different offline data collection schemes may influence the performance of offline adaptation, remain open questions. 
Offline outer-loop and online inner-loop In this setting, the agent learns to adapt with online data, by meta-training on purely offline data [59, 96, 233]. Compared to the fully offline setting, this setting is more suitable for the scenarios where few-shot online adaptation is allowed during deployment. Online adaptation tackles the aforementioned problem of limited exploration when adapting with only offline data, but it also introduces a new challenge: how can we learn a systematic exploration policy from offline data collected by some unknown policies? Usually, the desired behaviors of the exploration policy are not covered in the offline data, which is generally collected in a task-specific way. This creates a distribution shift between the exploration policy we want to learn and the offline data we can learn from. This shift can be especially problematic if the offline data was collected only with an expert that has access to the ground-truth task [236]. In particular, it can introduce a problem of ambiguity in the identity of the MDP [59]. To tackle these challenges, existing works make additional assumptions on the data collection scheme, which strictly speaking, 
53
violate the offline meta-training setting and thus limit their application. For example, Pong et al. [233] allow for some online meta-training, but only with unsupervised interaction, and they learn a reward function to generate supervision for this unsupervised online interaction. Additionally, Rafailov et al. [236] make assumptions about the reward that enable easy reward relabeling when swapping offline trajectories between tasks. How to relax offline assumptions to make this setting more applicable remains an open problem. 
Online outer-loop and offline inner-loop Under this setting, the agent can only adapt with offline data. However, online data is available in the outer-loop to help the agent learn what is a good offline adaptation strategy, which may be easier to learn than in an offline outer-loop setting. In other words, the agent is learning to do offline RL via online RL. This setting is appealing for two reasons: (1) It maintains the benefits of using solely offline data for adaptation, while being easier to meta-train than the fully offline setting. (2) It is difficult to design good offline RL algorithms [162], and meta-RL provides a promising approach to automating this process. A couple of existing methods do combine an offline inner-loop with an online outer-loop [52, 234]. However, these approaches both allow for additional online adaptation after the offline adaptation in the inner-loop, and the offline data either contains only observations [52], or conditions on additional expert actions [234]. One method uses permutation-invariant memory to enable an off-policy inner-loop, but only evaluates with data collected from prior policies [127]. Consequently, offline RL via online RL remains an interesting setting for future work with the potential for designing more effective offline RL algorithms by meta-learning in both the few-shot and many-shot settings. 
6.4 Limitations 
So far we have presented a positive case for the use and development of meta-RL. While meta-RL can confer many advantages, as a tool for solving problems, it also presents several trade-offs. At this point, we take a step back to discuss four limitations of meta-RL from a more critical perspective. 
While meta-RL can enable sample-efficient learning at deployment, it does so at the expense of increased sample complexity during meta-training. For this reason, meta-RL is only applicable when upfront data collection is relatively cheap, or adaption during deployment is prohibitively expensive. For example, the trade-off presented by meta-learning makes sense when a simulator is available for meta-training, or when adaptation needs to be efficient and frequent after meta-training. While this limitation is restrictive, it is not significantly more restrictive than the standard use case for reinforcement learning. 
Meta-RL additionally presents a trade-off between transferability and interpretability on one hand and sample efficiency and engineering burden on the other. While meta-RL methods may enable adaptation that is more sample-efficient than a manually engineered alternative, the insights pro duced by such systems may be less interpretable and transferable to novel problems. For example, in the many-shot setting, it may be possible to meta-learn a general-purpose RL algorithm that is bet ter than state-of-the-art algorithms, but even so, the exact mechanisms improving performance may be unclear. Consider meta-learning an objective function that is parameterized as a black-box. The meta-learned objective function may have a simple and interpretable form, but extricating this form from the weights and biases of a neural network is difficult. Meta-RL replaces engineering effort and domain expertise with computation, and in doing so, trades-off interpretability for performance. 
In the few-shot setting (Section 3), it is common to adapt to tasks using gradient updates in the inner loop. However, whether meta-learning to account for this procedure is worthwhile can depend on the particular task distribution. For example, when there is a limited number of tasks or limited amount of data in the inner-loop, it may even be preferable to have MAML perform no task adaptation during meta-training and only fine-tune at meta-test time [88]. Additionally, if the task distribution never requires different actions for the same state in different tasks, e.g., because the state-space does not overlap, then adaptation may not be needed and multi-task training may be sufficient. In such a setting, even if more adaptation is useful for generalization, meta-RL may confer little to no advantage compared to fine-tuning [188]. Moreover, even if adaptation to each task is needed, and sufficient data is provided, as is typically assumed, meta-RL can actually confer a disadvantage. Specifically, test-time adaptation can decrease performance on some tasks in practice [55], and training from scratch can yield higher returns [337]. (Similar observations have been made in the meta-supervised setting as well [304].) In fact, if the meta-testing task distribution is sufficiently dissimilar to the meta-training task distribution, then any sort of meta-learning can be catastrophic, since the agent learns incorrect inductive biases [337]. 
54
Finally, while the meta-RL literature develops many specialized methods, the problem setting may not require such methods. Consider that the meta-RL problem can be considered a particular type of POMDP, as discussed in Section 3.5. Given this, it is reasonable to question whether special ized methods are needed for meta-RL at all. In the zero-shot setting, recent work has suggested that many complex and specialized methods may be unnecessary. For example, some task-inference methods can be nearly matched by well-tuned end-to-end recurrent networks [216]; complicated task-inference parameterizations and supervision can be outperformed by hypernetworks trained end-to-end [22]; some task inference methods [126, 83] can be seen as applications of more general POMDP methods for inferring a hidden state [105, 205]; and complicated task-inference reward bonuses for exploration may be just as effectively replaced by more general exploration methods for POMDPs [347] applied to the meta-RL problem setting [369]. Even for complicated task dis tributions, methods designed for MDPs and more general POMDPs, such as curriculum learning, distillation, and transformer architectures, can be sufficient to enable meta-learning [298]. Still, even if specialized methods are not strictly necessary, such methods can enable more efficient meta learning. Moreover, whether meta-learning methods are developed explicitly or not, meta-learning will at the very least be an emergent phenomenon of any capable and general agent. For this reason, insights from meta-RL should assist practitioners in developing and reasoning about such systems. 
7 Conclusion 
In this article, we presented a survey of meta-RL research focused on two major categories of algo rithms as well as applications. We found the majority of research focused on the few-shot multi-task setting, where the objective is to learn an RL algorithm that adapts to new tasks from a known task distribution rapidly using as few samples as possible. We discussed the strengths and weaknesses of the few-shot algorithms, which generally fall in the categories of parameterized policy gradient, black box, and task inference methods. A central topic in using these methods is how to explore the environment to collect that data. We identified the different exploration strategies in the liter ature and discussed when each of them are applicable. Besides meta-RL in the few-shot setting, a rising topic in meta-RL looks at algorithms in the many-shot setting, where two distinct prob lems are considered: the generalization to broader task distributions and faster learning on a single task. We found the methods for these two seemingly opposite problems to be surprisingly similar, as they are often based on augmenting standard RL algorithms with learned components. We pre sented promising applications of meta-RL, especially those in robotics, where meta-RL is starting to enable significant sample efficiency gains in e.g., sim-to-real transfer. The sample efficiency of RL algorithms is a major blocker in learning controllers for real-world applications. Therefore, if meta-RL delivers on the promise of sample efficient adaptation, it would enable a wide variety of applications. In order to push meta-RL further and enable new applications, we found that broader and more diverse task distributions need to be developed for training and testing the meta-RL algo rithms. With promising applications in sight and a range of open problems awaiting solutions, we expect meta-RL research to continue to actively grow. 
55
References 
[1] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. “Flambe: Structural complexity and representation learning of low rank mdps”. In: Advances in neural informa tion processing systems 33 (2020), pp. 20095–20107. 
[2] Anurag Ajay, Dibya Ghosh, Sergey Levine, Pulkit Agrawal, and Abhishek Gupta. “Distribu tionally Adaptive Meta Reinforcement Learning”. In: Decision Awareness in Reinforcement Learning Workshop at ICML 2022. 
[3] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. “Solving rubik’s cube with a robot hand”. In: arXiv preprint arXiv:1910.07113 (2019). 
[4] Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. “Estimating Disentangled Belief about Hidden State and Hidden Task for Meta-Reinforcement Learning”. In: Learning for Dynam ics and Control. PMLR. 2021, pp. 73–86. 
[5] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. “Deep Variational Information Bottleneck”. In: International Conference on Learning Representations. 2017. URL: https://openreview.net/forum?id=HyxQzBceg. 
[6] Ferran Alet, Kenji Kawaguchi, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. “Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time”. In: 4th Workshop on Meta-Learning at NeurIPS. 2020. 
[7] Ferran Alet, Martin F Schneider, Tomas Lozano-Perez, and Leslie Pack Kaelbling. “Meta learning curiosity algorithms”. In: International Conference on Learning Representations. 2020. 
[8] Badr AlKhamissi, Muhammad ElNokrashy, and Michael Spranger. “The Emergence of Ab stract and Episodic Neurons in Episodic Meta-RL”. In: Learning to Learn - Workshop at ICLR 2021. 2021. URL: https://openreview.net/forum?id=LZCLt6Yx_px. 
[9] Diogo Almeida, Clemens Winter, Jie Tang, and Wojciech Zaremba. “A generalizable ap proach to learning optimizers”. In: arXiv preprint arXiv:2106.00958 (2021). 
[10] Safa Alver and Doina Precup. “A Brief Look at Generalization in Visual Meta Reinforcement Learning”. In: 4th Lifelong Machine Learning Workshop at ICML 2020. 2020. URL: https://openreview.net/forum?id=WrCFtzVEwn. 
[11] Haitham Bou Ammar, Eric Eaton, Matthew E Taylor, Decebal Constantin Mocanu, Kurt Driessens, Gerhard Weiss, and Karl Tuyls. “An automated measure of mdp similarity for transfer in reinforcement learning”. In: Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence. 2014. 
[12] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. “Learning to learn by gradient descent by gradient descent”. In: Advances in neural information processing systems 29 (2016). 
[13] Timothée Anne, Jack Wilkinson, and Zhibin Li. “Meta-Learning for Fast Adaptive Loco motion with Uncertainties in Environments and Robot Dynamics”. In: 2021 IEEE/RSJ In ternational Conference on Intelligent Robots and Systems (IROS). IEEE. 2021, pp. 4568– 4575. 
[14] Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. “Meta Reinforcement Learning for Sim-to-real Domain Adaptation”. In: 2020 IEEE International Conference on Robotics and Automation (ICRA). 2020, pp. 2725–2731. DOI: 10.1109/ICRA40945.2020. 9196540. 
[15] Dilip Arumugam and Satinder Singh. “Planning to the Information Horizon of BAMDPs via Epistemic State Abstraction”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=7eUOC9fEIRO. 
[16] Arthur Aubret, Laetitia Matignon, and Salima Hassas. “A survey on intrinsic motivation in reinforcement learning”. In: arXiv preprint arXiv:1908.06976 (2019). 
[17] Pierre-Luc Bacon, Jean Harb, and Doina Precup. “The option-critic architecture”. In: Pro ceedings of the AAAI Conference on Artificial Intelligence. Vol. 31. 1. 2017. 
56
[18] André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Has selt, and David Silver. “Successor features for transfer in reinforcement learning”. In: Ad vances in neural information processing systems 30 (2017). 
[19] Jonathan Baxter. “A model of inductive bias learning”. In: Journal of artificial intelligence research 12 (2000), pp. 149–198. 
[20] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. “Meta Learning via Learned Loss”. In: 2020 25th International Conference on Pattern Recognition (ICPR). IEEE Computer Society. 2021, pp. 4161–4168. 
[21] Jacob Beck, Kamil Ciosek, Sam Devlin, Sebastian Tschiatschek, Cheng Zhang, and Katja Hofmann. “AMRL: Aggregated Memory For Reinforcement Learning”. In: International Conference on Learning Representations. 2020. URL: https://openreview.net/forum? id=Bkl7bREtDr. 
[22] Jacob Beck, Matthew Jackson, Risto Vuorio, and Shimon Whiteson. “Hypernetworks in Meta-Reinforcement Learning”. In: CoRL (2022). 
[23] Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, and Shimon Whiteson. “SplAg ger: Split Aggregation for Meta-Reinforcement Learning”. In: arXiv (2024). eprint: 2403. 03020. URL: http://arxiv.org/abs/2403.03020. 
[24] Jacob Beck, Risto Vuorio, Zheng Xiong, and Shimon Whiteson. “Recurrent Hypernetworks are Surprisingly Strong in Meta-RL”. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. URL: https://openreview.net/forum?id=pefAAzu8an. 
[25] Suneel Belkhale, Rachel Li, Gregory Kahn, Rowan McAllister, Roberto Calandra, and Sergey Levine. “Model-based meta-reinforcement learning for flight with suspended pay loads”. In: IEEE Robotics and Automation Letters 6.2 (2021), pp. 1471–1478. 
[26] Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. “Long short-term memory and Learning-to-learn in networks of spiking neurons”. In: NeurIPS (2018). 
[27] Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Subhodeep Moitra, Sameera S Ponda, and Ziyu Wang. “Autonomous navigation of strato spheric balloons using reinforcement learning”. In: Nature 588.7836 (2020), pp. 77–82. 
[28] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. “The arcade learning environment: An evaluation platform for general agents”. In: Journal of Artificial Intelli gence Research 47 (2013), pp. 253–279. 
[29] Carolin Benjamins, Theresa Eimer, Frederik Schubert, André Biedenkapp, Bodo Rosen hahn, Frank Hutter, and Marius Lindauer. CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning. 2021. 
[30] Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn, and Sergey Levine. “Comps: Con tinual meta policy search”. In: arXiv preprint arXiv:2112.04467 (2021). 
[31] Alessandro Bonardi, Stephen James, and Andrew J. Davison. “Learning One-Shot Imitation from Humans without Humans”. In: CVPR (2020). 
[32] Clément Bonnet, Paul Caron, Thomas D. Barrett, Ian Davies, and Alexandre Laterre. “One Step at a Time: Pros and Cons of Multi-Step Meta-Gradient Reinforcement Learning”. In: 5th Workshop on Meta-Learning at NeurIPS (2021). URL: https://arxiv.org/abs/ 2111.00206. 
[33] Clément Bonnet, Laurence Illing Midgley, and Alexandre Laterre. “Debiasing Meta Gradient Reinforcement Learning by Learning the Outer Value Function”. In: Sixth Work shop on Meta-Learning at the Conference on Neural Information Processing Systems. 2022. URL: https://openreview.net/forum?id=HFVmkIRJ4J. 
[34] Craig Boutilier, Chih-wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvari, and Manzil Zaheer. “Differentiable Meta-Learning of Bandit Policies”. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Had sell, M.F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 2122– 2134. URL: https : / / proceedings . neurips . cc / paper / 2020 / file / 171ae1bbb81475eb96287dd78565b38b-Paper.pdf. 
57
[35] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. “Openai gym”. In: arXiv preprint arXiv:1606.01540 (2016). 
[36] Tom Brown et al. “Language Models are Few-Shot Learners”. In: Advances in Neural Infor mation Processing Systems. 2020. 
[37] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. “Exploration by random network distillation”. In: International Conference on Learning Representations. 2019. URL: https://openreview.net/forum?id=H1lJJnR5Ym. 
[38] Bradley Burega, John D Martin, and Michael Bowling. “Learning to Prioritize Planning Updates in Model-based Reinforcement Learning”. In: Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems. 2022. URL: https : / / openreview.net/forum?id=uR7ePjeB6z. 
[39] Mathieu Chalvidal, Thomas Serre, and Rufin VanRullen. “Meta-Reinforcement Learning with Self-Modifying Networks”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=cYeYzaP-5AF. 
[40] Stephanie C. Y. Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K. Lampinen, and Felix Hill. “Transformers generalize differently from information stored in context vs in weights”. In: Workshop on Memory in Artificial and Real Intelligence at NeurIPS. 2022. 
[41] Rujikorn Charakorn, Poramate Manoonpong, and Nat Dilokthanakul. “Learning to Co operate with Unseen Agents Through Meta-Reinforcement Learning”. In: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems. 2021, pp. 1478–1479. 
[42] Chacha Chen, Hua Wei, Nan Xu, Guanjie Zheng, Ming Yang, Yuanhao Xiong, Kai Xu, and Zhenhui Li. “Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic signal control”. In: Proceedings of the AAAI Conference on Artificial In telligence. Vol. 34. 04. 2020, pp. 3414–3421. 
[43] Xiaoyu Chen, Jiachen Hu, Chi Jin, Lihong Li, and Liwei Wang. “Understanding Domain Randomization for Sim-to-real Transfer”. In: International Conference on Learning Repre sentations. 2022. URL: https://openreview.net/forum?id=T8vZHIRTrY. 
[44] Yutian Chen, Matthew W Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P Lillicrap, Matt Botvinick, and Nando Freitas. “Learning to learn without gradient descent by gradient descent”. In: International Conference on Machine Learning. PMLR. 2017, pp. 748–756. 
[45] Yuan Cheng, Songtao Feng, Jing Yang, Hong Zhang, and Yingbin Liang. “Provable bene fit of multitask representation learning in reinforcement learning”. In: Advances in Neural Information Processing Systems 35 (2022), pp. 31741–31754. 
[46] Era Choshen and Aviv Tamar. “ContraBAR: Contrastive Bayes-Adaptive Deep RL”. In: ICML (2023). 
[47] Aakanksha Chowdhery et al. PaLM: Scaling Language Modeling with Pathways. 2022. 
[48] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. “Model-Based Reinforcement Learning via Meta-Policy Optimization”. In: CoRL (2018). 
[49] Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. “Leveraging procedural gener ation to benchmark reinforcement learning”. In: International conference on machine learn ing. PMLR. 2020, pp. 2048–2056. 
[50] Liam Collins, Aryan Mokhtari, Sewoong Oh, and Sanjay Shakkottai. “Maml and anil prov ably learn representations”. In: International Conference on Machine Learning. PMLR. 2022, pp. 4238–4310. 
[51] Lin Cong, Michael Görner, Philipp Ruppel, Hongzhuo Liang, Norman Hendrich, and Jian wei Zhang. “Self-Adapting Recurrent Models for Object Pushing from Learning in Simula tion”. In: IROS (2020). 
58
[52] Christopher R. Dance, Julien Perez, and Théo Cachet. “Demonstration-Conditioned Re inforcement Learning for Few-Shot Imitation”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Pro ceedings of Machine Learning Research. PMLR, 2021, pp. 2376–2387. URL: https : / / proceedings.mlr.press/v139/dance21a.html. 
[53] Sudeep Dasari and Abhinav Gupta. “Transformers for One-Shot Visual Imitation”. In: CoRL abs/2011.05970 (2020). arXiv: 2011. 05970. URL: https: //arxiv .org/ abs/ 2011. 05970. 
[54] Peter Dayan and Geoffrey E Hinton. “Feudal reinforcement learning”. In: Advances in neu ral information processing systems 5 (1992). 
[55] Tristan Deleu and Yoshua Bengio. “The effects of negative adaptation in model-agnostic meta-learning”. In: arXiv preprint arXiv:1812.02159 (2018). 
[56] Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. “Learning-to learn stochastic gradient descent with biased regularization”. In: International Conference on Machine Learning. PMLR. 2019, pp. 1566–1575. 
[57] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, An drew Critch, and Sergey Levine. “Emergent complexity and zero-shot transfer via unsu pervised environment design”. In: Advances in neural information processing systems 33 (2020), pp. 13049–13061. 
[58] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin guistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019. 
[59] Ron Dorfman, Idan Shenfeld, and Aviv Tamar. “Offline Meta Reinforcement Learning– Identifiability Challenges and Effective Data Collection Strategies”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 4607–4618. 
[60] Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, J. Schneider, Ilya Sutskever, P. Abbeel, and W. Zaremba. “One-Shot Imitation Learning”. In: NIPS. 2017. 
[61] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. “Benchmarking deep reinforcement learning for continuous control”. In: International conference on ma chine learning. PMLR. 2016, pp. 1329–1338. 
[62] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. “RL2: Fast reinforcement learning via slow reinforcement learning”. In: arXiv preprint arXiv:1611.02779 (2016). 
[63] Michael O’Gordon Duff and Andrew Barto. “Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes”. PhD thesis. 2002. ISBN: 0493525734. 
[64] Benjamin Ellis, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N Foerster, and Shimon Whiteson. “SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning”. In: arXiv preprint arXiv:2212.07489 (2022). 
[65] David Emukpere, Xavier Alameda-Pineda, and Chris Reinke. “Successor Feature Neural Episodic Control”. In: arXiv preprint arXiv:2111.03110 (2021). 
[66] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. “Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures”. In: International Conference on Machine Learning. PMLR. 2018, pp. 1407–1416. 
[67] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. “Diversity is All You Need: Learning Skills without a Reward Function”. In: ICLR (2019). 
[68] Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. “Meta-Q Learning”. In: International Conference on Learning Representations. 2020. 
[69] Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, and Asuman Ozdaglar. “On the con vergence theory of debiased model-agnostic meta-reinforcement learning”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 3096–3107. 
59
[70] Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, and Asuman Ozdaglar. “Provably con vergent policy gradient methods for model-agnostic meta-reinforcement learning”. In: arXiv preprint arXiv:2002.05135 (2020). 
[71] Gregory Farquhar, Shimon Whiteson, and Jakob Foerster. “Loaded DiCE: Trading off bias and variance in any-order score function gradient estimators for reinforcement learning”. In: Advances in Neural Information Processing Systems 32 (2019). 
[72] Xidong Feng, Oliver Slumbers, Ziyu Wan, Bo Liu, Stephen McAleer, Ying Wen, Jun Wang, and Yaodong Yang. “Neural auto-curricula in two-player zero-sum games”. In: Advances in Neural Information Processing Systems 34 (2021). 
[73] Chelsea Finn, Pieter Abbeel, and Sergey Levine. “Model-agnostic meta-learning for fast adaptation of deep networks”. In: International Conference on Machine Learning. PMLR. 2017, pp. 1126–1135. 
[74] Chelsea Finn and Sergey Levine. “Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm”. In: ICLR (2018). 
[75] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. “Online meta learning”. In: International Conference on Machine Learning. PMLR. 2019, pp. 1920–1930. 
[76] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. “One-Shot Visual Imitation Learning via Meta-Learning”. In: CoRL (2017). 
[77] Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia Hadsell. “Meta-Learning with Warped Gradient Descent”. In: International Confer ence on Learning Representations. 2020. URL: https://openreview.net/forum?id= rkeiQlBFPB. 
[78] Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, and Satinder Singh. “Bootstrapped Meta-Learning”. In: arXiv preprint arXiv:2109.04504 (2021). 
[79] Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. “Learning with Opponent-Learning Awareness”. In: AAMAS (2018). 
[80] Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rocktäschel, Eric Xing, and Shimon Whiteson. “Dice: The infinitely differentiable monte carlo estimator”. In: Interna tional Conference on Machine Learning. PMLR. 2018, pp. 1529–1538. 
[81] Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adrià Puigdomènech Badia, Gavin Buttimore, Charlie Deck, Joel Z. Leibo, and Charles Blundell. “Generalization of Reinforcement Learners with Working and Episodic Memory”. In: NeurIPS (2019). 
[82] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. “META LEARN ING SHARED HIERARCHIES”. In: International Conference on Learning Representa tions. 2018. URL: https://openreview.net/forum?id=SyX0IeWAW. 
[83] Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu. “Towards Effective Context for Meta-Reinforcement Learning: an Approach based on Contrastive Learning”. In: 2021. 
[84] Haotian Fu, Hongyao Tang, Jianye Hao, Wulong Liu, and Chen Chen. “MGHRL: Meta Goal-Generation for Hierarchical Reinforcement Learning”. In: International Conference on Distributed Artificial Intelligence. Springer. 2020, pp. 29–39. 
[85] Scott Fujimoto, David Meger, and Doina Precup. “Off-Policy Deep Reinforcement Learning without Exploration”. In: ICML (2019). 
[86] Alexandre Galashov, Jonathan Schwarz, Hyunjik Kim, Marta Garnelo, David Saxton, Push meet Kohli, S. M. Ali Eslami, and Yee Whye Teh. Meta-Learning surrogate models for sequential decision making. 2019. 
[87] Chongkai Gao, Yizhou Jiang, and Feng Chen. “Transferring Hierarchical Structures with Dual Meta Imitation Learning”. In: 6th Annual Conference on Robot Learning. 2022. URL: https://openreview.net/forum?id=MUcBYHjzqp7. 
60
[88] Katelyn Gao and Ozan Sener. “Modeling and Optimization Trade-off in Meta-learning”. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ran zato, R. Hadsell, M.F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 11154–11165. URL: https : / / proceedings . neurips . cc / paper / 2020 / file / 7fc63ff01769c4fa7d9279e97e307829-Paper.pdf. 
[89] Yuan Gao, Elena Sibirtseva, Ginevra Castellano, and Danica Kragic. “Fast adaptation with meta-reinforcement learning for trust modelling in human-robot interaction”. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2019, pp. 305–312. 
[90] Juan Jose Garau-Luis, Yingjie Miao, John D Co-Reyes, Aaron Parisi, Jie Tan, Esteban Real, and Aleksandra Faust. “Multi-objective evolution for generalizable policy gradient algo rithms”. In: ICLR 2022 Workshop on Generalizable Policy Learning in Physical World. 2022. 
[91] Francisco M. Garcia and Philip S. Thomas. “A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning”. In: NeurIPS (2019). 
[92] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye Teh. “Neural Processes”. In: ICML (2018). 
[93] Matthias Gerstgrasser and David C. Parkes. “Meta-RL for Multi-Agent RL: Learning to Adapt to Evolving Agents”. In: Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems. 2022. URL: https://openreview.net/forum? id=0toY1f8-Iq9. 
[94] Ali Ghadirzadeh, Xi Chen, Petra Poklukar, Chelsea Finn, Mårten Björkman, and Danica Kragic. “Bayesian Meta-Learning for Few-Shot Policy Adaptation Across Robotic Plat forms”. In: 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021, pp. 1274–1280. DOI: 10.1109/IROS51168.2021.9636628. 
[95] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. “Bayesian re inforcement learning: A survey”. In: Foundations and Trends® in Machine Learning 8.5-6 (2015), pp. 359–483. 
[96] Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. “Offline rl policies should be trained to be adaptive”. In: International Conference on Machine Learning. PMLR. 2022, pp. 7513–7530. 
[97] Wonjoon Goo and Scott Niekum. “One-shot learning of multi-step tasks from observation via activity localization in auxiliary video”. In: 2019 international conference on robotics and automation (ICRA). IEEE. 2019, pp. 7755–7761. 
[98] Alex Graves, Greg Wayne, and Ivo Danihelka. “Neural Turing Machines”. In: arXiv abs/1410.5401 (2014). URL: http://arxiv.org/abs/1410.5401. 
[99] Ido Greenberg, Shie Mannor, Gal Chechik, and Eli Meirom. “Train Hard, Fight Easy: Robust Meta Reinforcement Learning”. In: arXiv preprint arXiv:2301.11147 (2023). 
[100] Yashvir Singh Grewal, Frits de Nijs, and Sarah Goodwin. “Variance-Seeking Meta Exploration to Handle Out-of-Distribution Tasks”. In: Deep RL Workshop NeurIPS 2021. 2021. 
[101] Jake Grigsby, Linxi Fan, and Yuke Zhu. “AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents”. In: ICLR (2024). 
[102] Aditya Grover, Maruan Al-Shedivat, Jayesh Gupta, Yuri Burda, and Harrison Edwards. “Learning Policy Representations in Multiagent Systems”. In: Proceedings of the 35th Inter national Conference on Machine Learning. Ed. by Jennifer Dy and Andreas Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 1802–1811. URL: https: //proceedings.mlr.press/v80/grover18a.html. 
[103] Arthur Guez, David Silver, and Peter Dayan. “Scalable and efficient Bayes-adaptive rein forcement learning based on Monte-Carlo tree search”. In: Journal of Artificial Intelligence Research 48 (2013), pp. 841–883. 
[104] Yijie Guo, Qiucheng Wu, and Honglak Lee. “Learning Action Translator for Meta Rein forcement Learning on Sparse-Reward Tasks”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 6. 2022, pp. 6792–6800. 
61
[105] Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A Pires, and Rémi Munos. “Neural predictive belief representations”. In: arXiv preprint arXiv:1811.06407 (2018). 
[106] Abhinav Gupta, Marc Lanctot, and Angeliki Lazaridou. “Dynamic population-based meta learning for multi-agent communication with natural language”. In: NeurIPS (2021). 
[107] Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. “Unsupervised Meta-Learning for Reinforcement Learning”. In: arXiv abs/1806.04640 (2018). 
[108] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. “Meta-Reinforcement Learning of Structured Exploration Strategies”. In: Advances in Neural Information Processing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018. URL: https : / / proceedings . neurips . cc / paper / 2018 / file / 4de754248c196c85ee4fbdcee89179bd-Paper.pdf. 
[109] Swaminathan Gurumurthy, Sumit Kumar, and Katia Sycara. “MAME : Model-Agnostic Meta-Exploration”. In: Proceedings of the Conference on Robot Learning. Ed. by Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura. Vol. 100. Proceedings of Machine Learning Research. PMLR, 2020, pp. 910–922. URL: https : / / proceedings . mlr . press/v100/gurumurthy20a.html. 
[110] David Ha, Andrew Dai, and Quoc V Le. “Hypernetworks”. In: International Conference on Learning Representation (ICLR). 2017. 
[111] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. “Soft actor-critic: Off policy maximum entropy deep reinforcement learning with a stochastic actor”. In: Interna tional conference on machine learning. PMLR. 2018, pp. 1861–1870. 
[112] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual Markov Decision Processes. 2015. 
[113] James Harrison, Apoorva Sharma, Roberto Calandra, and Marco Pavone. “Control adapta tion via meta-learning dynamics”. In: Workshop on Meta-Learning at NeurIPS. Vol. 2018. 2018. 
[114] Matthew J. Hausknecht and Peter Stone. “Deep Recurrent Q-Learning for Partially Observ able MDPs”. In: AAAI Fall Symposia. 2015. 
[115] Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Aditi Raghunathan, and Anca Dra gan. “Learning Representations that Enable Generalization in Assistive Tasks”. In: 6th An nual Conference on Robot Learning. 2022. URL: https://openreview.net/forum?id= b88HF4vd_ej. 
[116] D. O. Hebb. The organization of behavior; a neuropsychological theory. 1949. 
[117] Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, and David Silver. “Memory-based control with recurrent neural networks”. In: NIPS Deep Reinforcement Learning Workshop (2015). 
[118] Donald Joseph Hejna III and Dorsa Sadigh. “Few-Shot Preference Learning for Human in-the-Loop RL”. In: 6th Annual Conference on Robot Learning. 2022. URL: https : / / openreview.net/forum?id=IKC5TfXLuW0. 
[119] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado Van Hasselt. “Multi-task deep reinforcement learning with popart”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 01. 2019, pp. 3796–3803. 
[120] Takuya Hiraoka, Takahisa Imagawa, Voot Tangkaratt, Takayuki Osa, Takashi Onishi, and Yoshimasa Tsuruoka. “Meta-model-based meta-policy optimization”. In: Asian Conference on Machine Learning. PMLR. 2021, pp. 129–144. 
[121] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. “Learning to learn using gradient descent”. In: International conference on artificial neural networks. Springer. 2001, pp. 87– 94. 
[122] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. “Meta-learning in neural networks: A survey”. In: arXiv preprint arXiv:2004.05439 (2020). 
62
[123] Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and Pieter Abbeel. “Evolved Policy Gradients”. In: Advances in Neu ral Information Processing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018. URL: https : / / proceedings . neurips . cc / paper / 2018 / file / 7876acb66640bad41f1e1371ef30c180-Paper.pdf. 
[124] Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang. “Near-optimal represen tation learning for linear bandits and linear rl”. In: International Conference on Machine Learning. PMLR. 2021, pp. 4349–4358. 
[125] Mike Huisman, Jan N Van Rijn, and Aske Plaat. “A survey of deep meta-learning”. In: Artificial Intelligence Review 54.6 (2021), pp. 4483–4541. 
[126] Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye Teh, and Nicolas Heess. “Meta reinforcement learning as task inference”. In: arXiv preprint arXiv:1905.06424 (2019). 
[127] Takahisa Imagawa, Takuya Hiraoka, and Yoshimasa Tsuruoka. “Off-Policy Meta Reinforcement Learning With Belief-Based Task Inference”. In: IEEE Access 10 (2022), pp. 49494–49507. 
[128] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. “A modern self referential weight matrix that learns to modify itself”. In: International Conference on Ma chine Learning. PMLR. 2022, pp. 9660–9677. 
[129] Allan Jabri, Kyle Hsu, Abhishek Gupta, Ben Eysenbach, Sergey Levine, and Chelsea Finn. “Unsupervised curricula for visual meta-reinforcement learning”. In: Advances in Neural Information Processing Systems 32 (2019). 
[130] Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gre gory Farquhar, Shimon Whiteson, and Jakob Nicolaus Foerster. “Discovering General Rein forcement Learning Algorithms with Adversarial Environment Design”. In: arXiv preprint arXiv:2310.02782 (2023). 
[131] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. “Reinforcement learning with unsupervised auxiliary tasks”. In: arXiv preprint arXiv:1611.05397 (2016). 
[132] Stephen James, Michael Bloesch, and Andrew J. Davison. “Task-Embedded Control Net works for Few-Shot Imitation Learning”. In: CoRL. 2018. 
[133] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. “Rlbench: The robot learning benchmark & learning environment”. In: IEEE Robotics and Automation Let ters 5.2 (2020), pp. 3019–3026. 
[134] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. “BC-Z: Zero-Shot Task Generalization with Robotic Imita tion Learning”. In: 5th Annual Conference on Robot Learning. 2021. URL: https : / / openreview.net/forum?id=8kbp23tSGYv. 
[135] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. “Provably efficient reinforce ment learning with linear function approximation”. In: Conference on Learning Theory. PMLR. 2020, pp. 2137–2143. 
[136] Jean Kaddour, Steindor Saemundsson, and Marc Deisenroth (he/him). “Probabilistic Ac tive Meta-Learning”. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 20813–20822. URL: https://proceedings.neurips.cc/paper/2020/ file/ef0d17b3bdb4ee2aa741ba28c7255c53-Paper.pdf. 
[137] Pierre-Alexandre Kamienny, Matteo Pirotta, Alessandro Lazaric, Thibault Lavril, Nicolas Usunier, and Ludovic Denoyer. “Learning adaptive exploration strategies in dynamic en vironments through informed policy regularization”. In: arXiv preprint arXiv:2005.02934 (2020). 
[138] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transform ers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: ICML (2020). 
63
[139] Rituraj Kaushik, Timothée Anne, and Jean-Baptiste Mouret. “Fast online adaptation in robotics through meta-learning embeddings of simulated priors”. In: 2020 IEEE/RSJ In ternational Conference on Intelligent Robots and Systems (IROS). IEEE. 2020, pp. 5269– 5276. 
[140] Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. “Adaptive gradient-based meta-learning methods”. In: Advances in Neural Information Processing Systems 32 (2019). 
[141] Dong Ki Kim, Miao Liu, Matthew D Riemer, Chuangchuang Sun, Marwa Abdulhai, Gol naz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, and Jonathan How. “A policy gradient algorithm for learning to learn in multiagent reinforcement learning”. In: International Con ference on Machine Learning. PMLR. 2021, pp. 5541–5550. 
[142] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. “A survey of zero shot generalisation in deep reinforcement learning”. In: Journal of Artificial Intelligence Research 76 (2023), pp. 201–264. 
[143] Louis Kirsch, Sebastian Flennerhag, Hado van Hasselt, Abram Friesen, Junhyuk Oh, and Yutian Chen. “Introducing Symmetries to Black Box Meta Reinforcement Learning”. In: AAAI (2022). 
[144] Louis Kirsch, James Harrison, C Daniel Freeman, Jascha Sohl-Dickstein, and Jürgen Schmidhuber. “Towards General-Purpose In-Context Learning Agents”. In: NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models. 2023. 
[145] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. “General-purpose in-context learning by meta-learning transformers”. In: arXiv (2022). 
[146] Louis Kirsch, Sjoerd van Steenkiste, and Jürgen Schmidhuber. “Improving generalization in meta reinforcement learning using learned objectives”. In: arXiv preprint arXiv:1910.04098 (2019). 
[147] Iryna Korshunova, Jonas Degrave, Joni Dambre, Arthur Gretton, and Ferenc Huszar. “Ex changeable Models in Meta Reinforcement Learning”. In: 4th Lifelong Machine Learn ing Workshop at ICML 2020. 2020. URL: https : / / openreview . net / forum ? id = TZFlzejFPT. 
[148] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. “RMA: Rapid Motor Adap tation for Legged Robots”. In: RSS abs/2107.04034 (2021). 
[149] Sreejan Kumar, Ishita Dasgupta, Jonathan D. Cohen, Nathaniel D. Daw, and Thomas L. Griffiths. “Meta-Learning of Compositional Task Distributions in Humans and Machines”. In: 4th Workshop on Meta-Learning at NeurIPS. 2020. 
[150] Visak C V Kumar, Sehoon Ha, and C. Karen Liu. “Error-Aware Policy Learning: Zero Shot Generalization in Partially Observable Dynamic Environments”. In: Proceedings of Robotics: Science and Systems. Virtual, 2021. DOI: 10.15607/RSS.2021.XVII.065. 
[151] Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Ed ward Grefenstette, and Tim Rocktäschel. “The nethack learning environment”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 7671–7684. 
[152] Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig Boutilier, and Csaba Szepesvari. “Meta-Thompson Sampling”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 5884–5893. URL: https://proceedings.mlr.press/v139/kveton21a.html. 
[153] Lin Lan, Zhenguo Li, Xiaohong Guan, and Pinghui Wang. “Meta reinforcement learning with task embedding and shared policy”. In: Proceedings of the 28th International Joint Conference on Artificial Intelligence. 2019, pp. 2794–2800. 
[154] Qingfeng Lan, A Rupam Mahmood, Shuicheng Yan, and Zhongwen Xu. “Learning to Opti mize for Reinforcement Learning”. In: arXiv preprint arXiv:2302.01470 (2023). 
[155] Robert Tjarko Lange and Henning Sprekeler. “Learning not to learn: Nature versus nurture in silico”. In: 4th Workshop on Meta-Learning at NeurIPS. 2020. 
[156] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. “In-context reinforcement learning with algorithm distillation”. In: ICLR (2023). 
64
[157] Byungjun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-Eung Kim. “Batch re inforcement learning with hyperparameter gradients”. In: International Conference on Ma chine Learning. PMLR. 2020, pp. 5725–5735. 
[158] Gilwoo Lee, Brian Hou, Aditya Mandalika, Jeongseok Lee, Sanjiban Choudhury, and Sid dhartha S Srinivasa. “Bayesian Policy Optimization for Model Uncertainty”. In: Interna tional Conference on Learning Representations. 2019. URL: https://openreview.net/ forum?id=SJGvns0qK7. 
[159] Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill. “Supervised pretraining can learn in-context reinforcement learning”. In: Advances in Neural Information Processing Systems 36 (2024). 
[160] Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. “Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning”. In: ICML (2020). 
[161] Suyoung Lee and Sae-Young Chung. “Improving Generalization in Meta-RL with Imagi nary Tasks from Latent Dynamics Mixture”. In: Advances in Neural Information Processing Systems 34 (2021). 
[162] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. “Offline reinforcement learning: Tutorial, review, and perspectives on open problems”. In: arXiv preprint arXiv:2005.01643 (2020). 
[163] Jiayi Li, Tao Lu, Xiaoge Cao, Yinghao Cai, and Shuo Wang. “Meta-imitation learning by watching video demonstrations”. In: International Conference on Learning Representations. 2021. 
[164] Ke Li and Jitendra Malik. “Learning to Optimize”. In: ICLR (2017). 
[165] Ke Li and Jitendra Malik. “Learning to optimize”. In: arXiv preprint arXiv:1606.01885 (2016). 
[166] Kevin Li, Abhishek Gupta, Ashwin Reddy, Vitchyr H Pong, Aurick Zhou, Justin Yu, and Sergey Levine. “MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 6346–6356. URL: https : / / proceedings . mlr . press / v139/li21g.html. 
[167] Lanqing Li, Rui Yang, and Dijun Luo. “FOCAL: Efficient Fully-Offline Meta Reinforcement Learning via Distance Metric Learning and Behavior Regularization”. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Aus tria, May 3-7, 2021. OpenReview.net, 2021. URL: https://openreview.net/forum? id=8cpHIfgY4Dj. 
[168] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. “Meta-SGD: Learning to Learn Quickly for Few Shot Learning”. In: CoRR abs/1707.09835 (2017). arXiv: 1707.09835. URL: http: //arxiv.org/abs/1707.09835. 
[169] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yu val Tassa, David Silver, and Daan Wierstra. “Continuous control with deep reinforcement learning.” In: ICLR (Poster). 2016. URL: http://arxiv.org/abs/1509.02971. 
[170] Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, and Junshan Zhang. “Model-Based Offline Meta-Reinforcement Learning with Regularization”. In: International Conference on Learn ing Representations. 2022. URL: https://openreview.net/forum?id=EBn0uInJZWh. 
[171] Xingyu Lin, Harjatin, Singh Baweja, George Kantor, and David Held. “Adaptive Auxiliary Task Weighting for Reinforcement Learning”. In: NeurIPS. 2019. 
[172] Zichuan Lin, Garrett Thomas, Guangwen Yang, and Tengyu Ma. “Model-based Adversarial Meta-Reinforcement Learning”. In: Advances in Neural Information Processing Systems. 2020. 
[173] Zichuan Lin, Garrett Thomas, Guangwen Yang, and Tengyu Ma. “Model-based adversarial meta-reinforcement learning”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 10161–10173. 
65
[174] Pierre Liotet, Francesco Vidaich, Alberto Maria Metelli, and Marcello Restelli. “Lifelong hyper-policy optimization with multiple importance sampling regularization”. In: Proceed ings of the AAAI Conference on Artificial Intelligence. Vol. 36. 7. 2022, pp. 7525–7533. 
[175] Bo Liu, Xidong Feng, Jie Ren, Luo Mai, Rui Zhu, Haifeng Zhang, Jun Wang, and Yaodong Yang. “A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum? id=p9zeOtKQXKs. 
[176] Bo Liu, Xidong Feng, Jie Ren, Luo Mai, Rui Zhu, Haifeng Zhang, Jun Wang, and Yaodong Yang. “A theoretical understanding of gradient bias in meta-reinforcement learning”. In: Advances in Neural Information Processing Systems 35 (2022), pp. 31059–31072. 
[177] Evan Z Liu, Aditi Raghunathan, Percy Liang, and Chelsea Finn. “Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices”. In: International Conference on Machine Learning. PMLR. 2021, pp. 6925–6935. 
[178] Evan Zheran Liu, Moritz Pascal Stephan, Allen Nie, Christopher J Piech, Emma Brun skill, and Chelsea Finn. “Giving Feedback on Interactive Student Programs with Meta Exploration”. In: Advances in Neural Information Processing Systems. 2022. 
[179] Hao Liu, Richard Socher, and Caiming Xiong. “Taming maml: Efficient unbiased meta reinforcement learning”. In: International Conference on Machine Learning. PMLR. 2019, pp. 4061–4071. 
[180] Beicheng Lou, Nathan Zhao, and Jiahui Wang. “Meta-learning from sparse recovery”. In: Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Sys tems. 2021. 
[181] Chris Lu, Jakub Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt, and Jakob Foerster. “Discovered Policy Optimisation”. In: Advances in Neural Informa tion Processing Systems. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 16455–16468. URL: https : / / proceedings . neurips . cc / paper _ files / paper / 2022 / file / 688c7a82e31653e7c256c6c29fd3b438-Paper-Conference.pdf. 
[182] Chris Lu, Timon Willi, Alistair Letcher, and Jakob Nicolaus Foerster. “Adversarial Cheap Talk”. In: Sixth Workshop on Meta-Learning at the Conference on Neural Information Pro cessing Systems. 2022. URL: https://openreview.net/forum?id=VE6FEZc0ppH. 
[183] Christopher Lu, Timon Willi, Christian A Schroeder De Witt, and Jakob Foerster. “Model free opponent shaping”. In: International Conference on Machine Learning. PMLR. 2022, pp. 14398–14411. 
[184] Rui Lu, Andrew Zhao, Simon S Du, and Gao Huang. “Provable General Function Class Rep resentation Learning in Multitask Bandits and MDP”. In: Advances in Neural Information Processing Systems 35 (2022), pp. 11507–11519. 
[185] Jelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom Zahavy, and Satinder Singh. “Meta-Gradients in Non-Stationary Environments”. In: ICLR Workshop on Agent Learning in Open-Endedness. 2022. URL: https://openreview.net/forum?id= SlzBXwZIZ9. 
[186] Ricardo Luna Gutierrez and Matteo Leonetti. “Information-theoretic task selection for meta reinforcement learning”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 20532–20542. 
[187] Fan-Ming Luo, Shengyi Jiang, Yang Yu, Zongzhang Zhang, and Yi-Feng Zhang. “Adapt to environment sudden changes by learning a context sensitive policy”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 7. 2022, pp. 7637–7646. 
[188] Zhao Mandi, Pieter Abbeel, and Stephen James. “On the Effectiveness of Fine-tuning Ver sus Meta-RL for Robot Manipulation”. In: CoRL 2022 Workshop on Pre-training Robot Learning. 2022. 
[189] Jingkai Mao, Jakob Foerster, Tim Rocktäschel, Maruan Al-Shedivat, Gregory Farquhar, and Shimon Whiteson. “A baseline for any order gradient estimation in stochastic computation graphs”. In: International Conference on Machine Learning. PMLR. 2019, pp. 4343–4351. 
66
[190] Bhairav Mehta, Tristan Deleu, Sharath Chandra Raparthy, Christopher J. Pal, and Liam Paull. “Curriculum in Gradient-Based Meta-Reinforcement Learning”. In: Beyond Tabula Rasa in RL Workshop at ICLR. 2020. 
[191] Robert Meier and Asier Mujika. “Open-Ended Reinforcement Learning with Neural Re ward Functions”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https : / / openreview.net/forum?id=NL05_JGVg99. 
[192] Luckeciano C. Melo. Transformers are Meta-Reinforcement Learners. 2022. 
[193] Russell Mendonca, Xinyang Geng, Chelsea Finn, and Sergey Levine. “Meta-reinforcement learning robust to distributional shift via model identification and experience relabeling”. In: arXiv preprint arXiv:2006.07178 (2020). 
[194] Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea Finn. “Guided Meta-Policy Search”. In: NeurIPS (2019). 
[195] Luke Metz, C Daniel Freeman, Samuel S Schoenholz, and Tal Kachman. “Gradients are not all you need”. In: arXiv preprint arXiv:2111.05803 (2021). 
[196] Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl Dickstein. “Understanding and correcting pathologies in the training of learned optimizers”. In: International Conference on Machine Learning. PMLR. 2019, pp. 4556–4565. 
[197] Thomas Miconi, Aditya Rawal, Jeff Clune, and Kenneth O. Stanley. “Backpropamine: train ing self-modifying neural networks with differentiable neuromodulated plasticity”. In: In ternational Conference on Learning Representations. 2019. URL: https://openreview. net/forum?id=r1lrAiA5Ym. 
[198] Thomas Miconi, Kenneth Stanley, and Jeff Clune. “Differentiable plasticity: training plastic neural networks with backpropagation”. In: Proceedings of the 35th International Confer ence on Machine Learning. Ed. by Jennifer Dy and Andreas Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 3559–3568. URL: https://proceedings. mlr.press/v80/miconi18a.html. 
[199] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. “Learning robust perceptive locomotion for quadrupedal robots in the wild”. In: Science Robotics 7.62 (2022), eabk2822. 
[200] Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, and Pedro A Ortega. “Meta-trained agents implement Bayes-optimal agents”. In: arXiv preprint arXiv:2010.11223 (2020). 
[201] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. “A Simple Neural Atten tive Meta-Learner”. In: International Conference on Learning Representations. 2018. URL: https://openreview.net/forum?id=B1DmUzWAW. 
[202] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. “Of fline Meta-Reinforcement Learning with Advantage Weighting”. In: CoRR abs/2008.06043 (2020). arXiv: 2008.06043. URL: https://arxiv.org/abs/2008.06043. 
[203] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. “Offline Meta-Reinforcement Learning with Advantage Weighting”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 7780–7791. URL: https://proceedings.mlr.press/v139/mitchell21a.html. 
[204] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli crap, Tim Harley, David Silver, and Koray Kavukcuoglu. “Asynchronous methods for deep reinforcement learning”. In: International conference on machine learning. PMLR. 2016, pp. 1928–1937. 
[205] Pol Moreno, Jan Humplik, George Papamakarios, Bernardo Avila Pires, Lars Buesing, Nico las Heess, and Theophane Weber. “Neural belief states for partially observed domains”. In: NeurIPS 2018 workshop on reinforcement learning under partial observability. 2018. 
67
[206] Yao Mu, Yuzheng Zhuang, Fei Ni, Bin Wang, Jianyu Chen, Jianye Hao, and Ping Luo. “DOMINO: Decomposed Mutual Information Optimization for Generalized Context in Meta-Reinforcement Learning”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=CJGUABT_COm. 
[207] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. “Met alearned Neural Memory”. In: (2019). 
[208] Mirco Mutti, Mattia Mancassola, and Marcello Restelli. “Unsupervised reinforcement learn ing in multiple environments”. In: Proceedings of the AAAI Conference on Artificial Intelli gence. Vol. 36. 7. 2022, pp. 7850–7858. 
[209] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. “Data-efficient hierar chical reinforcement learning”. In: Advances in neural information processing systems 31 (2018). 
[210] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. “Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning”. In: International Conference on Learning Repre sentations. 2019. URL: https://openreview.net/forum?id=HyztsoC5Y7. 
[211] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. “Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning”. In: International Conference on Learning Repre sentations. 2019. 
[212] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. “Deep Online Learning Via Meta Learning: Continual Adaptation for Model-Based RL”. In: International Conference on Learning Representations. 2019. URL: https : / / openreview . net / forum ? id = HyxAfnA5tm. 
[213] Elias Najarro and Sebastian Risi. “Meta-Learning through Hebbian Plasticity in Random Networks”. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 20719–20731. URL: https : / / proceedings . neurips . cc / paper / 2020 / file / ee23e7ad9b473ad072d57aaa9b2a5222-Paper.pdf. 
[214] Taewook Nam, Shao-Hua Sun, Karl Pertsch, Sung Ju Hwang, and Joseph J Lim. “Skill-based Meta-Reinforcement Learning”. In: International Conference on Learning Representations. 2022. 
[215] Hai Huu Nguyen, Andrea Baisero, Dian Wang, Christopher Amato, and Robert Platt. “Leveraging Fully Observable Policies for Learning under Partial Observability”. In: 6th Annual Conference on Robot Learning. 2022. URL: https://openreview.net/forum? id=pn-HOPBioUE. 
[216] Tianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. “Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs”. In: Proceedings of the 39th International Conference on Machine Learning. 2022. 
[217] Alex Nichol, Joshua Achiam, and John Schulman. “On first-order meta-learning algo rithms”. In: arXiv preprint arXiv:1803.02999 (2018). 
[218] Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. “Gotta learn fast: A new benchmark for generalization in rl”. In: arXiv preprint arXiv:1804.03720 (2018). 
[219] Junhyuk Oh, Valliappa Chockalingam, Satinder P. Singh, and Honglak Lee. “Control of Memory, Active Perception, and Action in Minecraft”. In: ICML (2016). 
[220] Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado van Hasselt, Satinder Singh, and David Silver. “Discovering reinforcement learning algorithms”. In: arXiv preprint arXiv:2007.08794 (2020). 
[221] Charles Packer, Pieter Abbeel, and Joseph E Gonzalez. “Hindsight Task Relabelling: Expe rience Replay for Sparse Reward Meta-RL”. In: Advances in Neural Information Process ing Systems. 2021. URL: https: //proceedings .neurips. cc/paper /2021/ file/ 1454ca2270599546dfcd2a3700e4d2f1-Paper.pdf. 
68
[222] Tom Le Paine, Sergio Gómez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar, Tobias Pfaff, Matt W Hoffman, Gabriel Barth-Maron, Serkan Cabi, David Budden, et al. “One shot high-fidelity imitation: Training large-scale deep nets with rl”. In: arXiv preprint arXiv:1810.05017 (2018). 
[223] Georgios Papoudakis and Stefano V Albrecht. “Variational Autoencoders for Opponent Modeling in Multi-Agent Systems”. In: AAAI 2020 Workshop on Reinforcement Learning in Games. 2020. 
[224] Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Çaglar Gülçehre, Sid dhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. “Stabilizing Transformers for Re inforcement Learning”. In: ICML (2020). 
[225] Eunbyung Park and Junier B. Oliva. “Meta-Curvature”. In: NeurIPS (2019). 
[226] Seohong Park, Oleh Rybkin, and Sergey Levine. “METRA: Scalable Unsupervised RL with Metric-Aware Abstraction”. In: The Twelfth International Conference on Learning Repre sentations. 2024. URL: https://openreview.net/forum?id=c5pwL0Soay. 
[227] Jack Parker-Holder, Raghu Rajan, Xingyou Song, André Biedenkapp, Yingjie Miao, Theresa Eimer, Baohe Zhang, Vu Nguyen, Roberto Calandra, Aleksandra Faust, et al. “Auto mated Reinforcement Learning (AutoRL): A Survey and Open Problems”. In: arXiv preprint arXiv:2201.03916 (2022). 
[228] Razvan Pascanu, Tomás Mikolov, and Yoshua Bengio. “On the difficulty of training recur rent neural networks”. In: ICML (2013). 
[229] Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek. “Hierarchical rein forcement learning: A comprehensive survey”. In: ACM Computing Surveys (CSUR) 54.5 (2021), pp. 1–35. 
[230] Matt Peng, Banghua Zhu, and Jiantao Jiao. Linear Representation Meta-Reinforcement Learning for Instant Adaptation. 2021. URL: https://openreview.net/forum?id= lNrtNGkr-vw. 
[231] Christian Perez, Felipe Petroski Such, and Theofanis Karaletsos. “Generalized hidden pa rameter mdps: Transferable model-based rl in a handful of trials”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. 04. 2020, pp. 5403–5411. 
[232] Athanasios S Polydoros and Lazaros Nalpantidis. “Survey of model-based reinforcement learning: Applications on robotics”. In: Journal of Intelligent & Robotic Systems 86.2 (2017), pp. 153–173. 
[233] Vitchyr H Pong, Ashvin V Nair, Laura M Smith, Catherine Huang, and Sergey Levine. “Offline meta-reinforcement learning with online self-supervision”. In: International Con ference on Machine Learning. PMLR. 2022, pp. 17811–17829. 
[234] Alvaro Prat and Edward Johns. PERIL: Probabilistic Embeddings for hybrid Meta Reinforcement and Imitation Learning. 2021. URL: https://openreview.net/forum? id=BIIwfP55pp. 
[235] Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. “Neural episodic control”. In: International Conference on Machine Learning. PMLR. 2017, pp. 2827–2836. 
[236] Rafael Rafailov, Varun Kumar Vijay, Tianhe Yu, Avi Singh, Mariano Phielipp, and Chelsea Finn. “The Reflective Explorer: Online Meta-Exploration from Offline Data in Realistic Robotic Tasks”. In: Deep RL Workshop NeurIPS 2021. 2021. 
[237] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. “Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML”. In: ICLR (2020). 
[238] Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. “Fast Adaptation via Policy-Dynamics Value Functions”. In: ICML (2020). 
[239] Janarthanan Rajendran, Aravind Lakshminarayanan, Mitesh M. Khapra, Prasanna P, and Balaraman Ravindran. “Attend, Adapt and Transfer: Attentive Deep Architecture for Adap tive Transfer from multiple sources in the same domain”. In: International Conference on Learning Representations. 2017. URL: https : / / openreview . net / forum ? id = Sy6iJDqlx. 
69
[240] Janarthanan Rajendran, Richard Lewis, Vivek Veeriah, Honglak Lee, and Satinder Singh. “How Should an Agent Practice?” In: Proceedings of the AAAI Conference on Artificial Intelligence 34.04 (2020), pp. 5454–5461. DOI: 10 . 1609 / aaai . v34i04 . 5995. URL: https://ojs.aaai.org/index.php/AAAI/article/view/5995. 
[241] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. “Efficient off-policy meta-reinforcement learning via probabilistic context variables”. In: International conference on machine learning. PMLR. 2019, pp. 5331–5340. 
[242] Sharath Chandra Raparthy, Eric Hambro, Robert Kirk, Mikael Henaff, and Roberta Raileanu. “Generalization to New Sequential Decision Making Tasks with In-Context Learn ing”. In: NeurIPS 2023 Workshop on Foundation Models for Decision Making (2023). 
[243] Sachin Ravi and H. Larochelle. “Optimization as a Model for Few-Shot Learning”. In: ICLR. 2017. 
[244] Ingo Rechenberg. “Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. Dr.-Ing”. PhD thesis. Thesis, Technical University of Berlin, Department of Process Engineering, 1971. 
[245] Allen Z. Ren, Bharat Govil, Tsung-Yen Yang, Karthik R Narasimhan, and Anirudha Majum dar. “Leveraging Language for Accelerated Learning of Tool Manipulation”. In: 6th Annual Conference on Robot Learning. 2022. URL: https : / / openreview . net / forum ? id = nPw7jaGBrCG. 
[246] Zhizhou Ren, Anji Liu, Yitao Liang, Jian Peng, and Jianzhu Ma. “Efficient Meta Reinforce ment Learning for Preference-based Fast Adaptation”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=61UwgeIotn. 
[247] Desik Rengarajan, Sapana Chaudhary, Jaewon Kim, Dileep Kalathil, and Srinivas Shakkot tai. “Enhanced Meta Reinforcement Learning via Demonstrations in Sparse Reward En vironments”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https : / / openreview.net/forum?id=kCtnkLv-_W0. 
[248] John D Co-Reyes, Sarah Feng, Glen Berseth, Jie Qui, and Sergey Levine. “Accelerating Online Reinforcement Learning via Model-Based Meta-Learning”. In: Learning to Learn - Workshop at ICLR 2021. 2021. URL: https : / / openreview . net / forum ? id = XCJ5PEkuMkC. 
[249] John D Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, John DeNero, Pieter Abbeel, and Sergey Levine. “Meta-Learning Language-Guided Policy Learning”. In: Inter national Conference on Learning Representations. 2019. URL: https : / / openreview . net/forum?id=HkgSEnA5KQ. 
[250] John D Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Quoc V Le, Sergey Levine, Honglak Lee, and Aleksandra Faust. “Evolving Reinforcement Learning Algorithms”. In: International Conference on Learning Representations. 2021. URL: https : / / openreview.net/forum?id=0XXpJ4OtjW. 
[251] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Ger ald Tesauro. “Learning to Learn without Forgetting By Maximizing Transfer and Minimiz ing Interference”. In: International Conference on Learning Representations. 2019. URL: https://openreview.net/forum?id=B1gTShAct7. 
[252] Zohar Rimon, Aviv Tamar, and Gilad Adler. “Meta Reinforcement Learning with Finite Training Tasks - a Density Estimation Approach”. In: Advances in Neural Information Pro cessing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=Y-sdZLIi9R9. 
[253] Samuel Ritter, Ryan Faulkner, Laurent Sartran, Adam Santoro, Matthew Botvinick, and David Raposo. “Rapid Task-Solving in Novel Environments”. In: International Conference on Learning Representations. 2021. URL: https : // openreview . net /forum ? id = F - mvpFpn_0q. 
[254] Samuel Ritter, Jane Wang, Zeb Kurth-Nelson, Siddhant Jayakumar, Charles Blundell, Razvan Pascanu, and Matthew Botvinick. “Been There, Done That: Meta-Learning with Episodic Recall”. In: ICML. 2018. 
70
[255] Samuel Ritter, Jane Wang, Zeb Kurth-Nelson, Siddhant Jayakumar, Charles Blundell, Razvan Pascanu, and Matthew Botvinick. “Been There, Done That: Meta-Learning with Episodic Recall”. In: Proceedings of the 35th International Conference on Machine Learn ing. Ed. by Jennifer Dy and Andreas Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 4354–4363. URL: https://proceedings.mlr.press/v80/ ritter18a.html. 
[256] Seyed Roozbeh Razavi Rohani, Saeed Hedayatian, and Mahdieh Soleymani Baghshah. “BIMRL: Brain Inspired Meta Reinforcement Learning”. In: 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2022, pp. 9048–9053. 
[257] Stéphane Ross and J. Andrew Bagnell. “Reinforcement and Imitation Learning via Interac tive No-Regret Learning”. In: arXiv (2014). 
[258] Stéphane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning”. In: AISTATS (2011). 
[259] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. “ProMP: Proximal Meta-Policy Search”. In: International Conference on Learning Representations. 2019. URL: https://openreview.net/forum?id=SkxXCi0qFX. 
[260] Steindór Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth. “Meta reinforcement learning with latent variable gaussian processes”. In: arXiv preprint arXiv:1803.07551 (2018). 
[261] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. “Evolu tion strategies as a scalable alternative to reinforcement learning”. In: arXiv preprint arXiv:1703.03864 (2017). 
[262] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. “Meta-learning with memory-augmented neural networks”. In: International conference on machine learning. PMLR. 2016, pp. 1842–1850. 
[263] Jürgen Schmidhuber. “Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook”. PhD thesis. Technische Universität München, 1987. 
[264] Jürgen Schmidhuber. “Gödel machines: Fully self-referential optimal universal self improvers”. In: Artificial general intelligence. Springer, 2007, pp. 199–226. 
[265] Jürgen Schmidhuber, Jieyu Zhao, and Marco Wiering. “Shifting inductive bias with success story algorithm, adaptive Levin search, and incremental self-improvement”. In: Machine Learning 28.1 (1997), pp. 105–130. 
[266] Gerrit Schoettler, Ashvin Nair, Juan Aparicio Ojea, Sergey Levine, and Eugen Solowjow. “Meta-reinforcement learning for robotic industrial insertion tasks”. In: 2020 IEEE/RSJ In ternational Conference on Intelligent Robots and Systems (IROS). IEEE. 2020, pp. 9728– 9735. 
[267] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. “Proximal policy optimization algorithms”. In: arXiv preprint arXiv:1707.06347 (2017). 
[268] Andrew Schwartzwald and Nikolaos Papanikolopoulos. “Sim-to-Real with Domain Ran domization for Tumbling Robot Control”. In: IROS. 2020. 
[269] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. “Masked World Models for Visual Control”. In: 6th Annual Conference on Robot Learning. 2022. URL: https://openreview.net/forum?id=Bf6on28H0Jv. 
[270] Seyed Kamyar Seyed Ghasemipour, Shixiang (Shane) Gu, and Richard Zemel. “SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies”. In: Advances in Neural Information Processing Systems. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc., 2019. URL: https : / / proceedings . neurips . cc / paper / 2019 / file / 2b8f621e9244cea5007bac8f5d50e476-Paper.pdf. 
[271] Gresa Shala, Sebastian Pineda Arango, André Biedenkapp, Frank Hutter, and Josif Grabocka. “AutoRL-Bench 1.0”. In: Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems. 2022. URL: https://openreview.net/forum? id=RyAl60VhTcG. 
71
[272] Amr Sharaf and Hal Daumé III. “Meta-Learning Effective Exploration Strategies for Con textual Bandits”. In: Proceedings of the AAAI Conference on Artificial Intelligence (2021), pp. 9541–9548. 
[273] Maruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. “Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Envi ronments”. In: International Conference on Learning Representations. 2018. URL: https: //openreview.net/forum?id=Sk2u1g-0-. 
[274] Maruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. “Continuous Adaptation via Meta-Learning in Nonstationary and Competitive En vironments”. In: International Conference on Learning Representations. 2018. 
[275] Jaeuk Shin, Astghik Hakobyan, Mingyu Park, Yeoneung Kim, Gihun Kim, and Insoon Yang. “Infusing model predictive control into meta-reinforcement learning for mobile robots in dynamic environments”. In: IEEE Robotics and Automation Letters 7.4 (2022), pp. 10065– 10072. 
[276] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. “Mastering the game of Go with deep neural networks and tree search”. In: nature 529.7587 (2016), pp. 484–489. 
[277] Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel J Hsu, Thodoris Lyk ouris, Miro Dudik, and Robert E Schapire. “Bayesian decision-making under misspecified priors with applications to meta-learning”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 26382–26394. 
[278] Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler, Murtaza Dalal, Sergey Levinev, Mohi Khansari, and Chelsea Finn. “Scalable multi-task imitation learning with autonomous improvement”. In: 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2020, pp. 2167–2173. 
[279] Satinder Singh, Richard L Lewis, and Andrew G Barto. “Where do rewards come from”. In: Proceedings of the annual conference of the cognitive science society. Cognitive Science Society. 2009, pp. 2601–2606. 
[280] Jake Snell, Kevin Swersky, and Richard Zemel. “Prototypical Networks for Few-shot Learn ing”. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. 2017. 
[281] Sungryull Sohn, Hyunjae Woo, Jongwook Choi, and Honglak Lee. “Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies”. In: International Confer ence on Learning Representations. 2020. URL: https://openreview.net/forum?id= HkgsWxrtPB. 
[282] Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and Yunhao Tang. “ES-MAML: Simple Hessian-Free Meta Learning”. In: International Confer ence on Learning Representations. 2020. URL: https://openreview.net/forum?id= S1exA2NtDB. 
[283] Xingyou Song, Yuxiang Yang, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea Finn, and Jie Tan. “Rapidly Adaptable Legged Robots via Evolutionary Meta Learning”. In: IROS (2020). 
[284] Bradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. “Some Considerations on Learning to Explore via Meta-Reinforcement Learning”. In: NeurIPS (2018). 
[285] Edward W Staley, Chace Ashcraft, Benjamin Stoler, Jared Markowitz, Gautam Vallabha, Christopher Ratto, and Kapil D Katyal. “Meta Arcade: A Configurable Environment Suite for Meta-Learning”. In: arXiv preprint arXiv:2112.00583 (2021). 
[286] Peter Stone, Gal A. Kaminka, Sarit Kraus, and Jeffrey S. Rosenschein. “Ad Hoc Au tonomous Agent Teams: Collaboration without Pre-Coordination”. In: Proceedings of the Twenty-Fourth Conference on Artificial Intelligence. 2010. 
[287] Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. “Approximate information state for approximate planning and reinforcement learning in partially observed systems”. In: The Journal of Machine Learning Research 23.1 (2022), pp. 483–565. 
72
[288] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales. “Learning to Compare: Relation Network for Few-Shot Learning”. In: CVPR (2018). 
[289] Flood Sung, Li Zhang, Tao Xiang, Timothy M. Hospedales, and Yongxin Yang. “Learning to Learn: Meta-Critic Networks for Sample Efficient Learning”. In: CoRR abs/1706.09529 (2017). arXiv: 1706.09529. URL: http://arxiv.org/abs/1706.09529. 
[290] Ilya Sutskever. Training recurrent neural networks. University of Toronto Toronto, ON, Canada, 2013. 
[291] Richard S Sutton. “Adapting bias by gradient descent: An incremental version of delta-bar delta”. In: AAAI. San Jose, CA. 1992, pp. 171–176. 
[292] Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. “Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction”. In: The 10th International Conference on Au tonomous Agents and Multiagent Systems-Volume 2. 2011, pp. 761–768. 
[293] Richard S Sutton, Doina Precup, and Satinder Singh. “Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning”. In: Artificial intelligence 112.1-2 (1999), pp. 181–211. 
[294] Jihoon Tack, Jongjin Park, Hankook Lee, Jaeho Lee, and Jinwoo Shin. “Meta-Learning with Self-Improving Momentum Target”. In: Advances in Neural Information Processing Sys tems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=FCNMbF_TsKm. 
[295] Aviv Tamar, Daniel Soudry, and Ev Zisselman. “Regularization guarantees generalization in bayesian reinforcement learning through algorithmic stability”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 8. 2022, pp. 8423–8431. 
[296] Yunhao Tang. “Biased Gradient Estimate with Drastic Variance Reduction for Meta Re inforcement Learning”. In: International Conference on Machine Learning. PMLR. 2022, pp. 21050–21075. 
[297] Yunhao Tang, Tadashi Kozuno, Mark Rowland, Rémi Munos, and Michal Valko. “Unifying gradient estimators for meta-reinforcement learning via off-policy evaluation”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 5303–5315. 
[298] Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al. “Human-Timescale Adaptation in an Open-Ended Task Space”. In: arXiv preprint arXiv:2301.07608 (2023). 
[299] Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. “Distral: Robust multitask reinforcement learning”. In: Advances in neural information processing systems 30 (2017). 
[300] William R Thompson. “On the likelihood that one unknown probability exceeds another in view of the evidence of two samples”. In: Biometrika 25.3-4 (1933), pp. 285–294. 
[301] Sebastian Thrun and Lorien Pratt. “Learning to learn: Introduction and overview”. In: Learn ing to learn. Springer, 1998, pp. 3–17. 
[302] Emanuel Todorov, Tom Erez, and Yuval Tassa. “Mujoco: A physics engine for model-based control”. In: 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEE. 2012, pp. 5026–5033. 
[303] Ahmed Touati and Yann Ollivier. “Learning one representation to optimize all rewards”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 13–23. 
[304] Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. “Meta dataset: A dataset of datasets for learning to learn from few examples”. In: International Conference on Learning Representations (2020). 
[305] Nilesh Tripuraneni, Chi Jin, and Michael Jordan. “Provable meta-learning of linear repre sentations”. In: International Conference on Machine Learning. PMLR. 2021, pp. 10434– 10443. 
73
[306] Leslie G Valiant. “A theory of the learnable”. In: Communications of the ACM 27.11 (1984), pp. 1134–1142. 
[307] Joaquin Vanschoren. “Meta-learning: A survey”. In: arXiv preprint arXiv:1810.03548 (2018). 
[308] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need”. In: NeurIPS (2017). 
[309] José R Vázquez-Canteli, Sourav Dey, Gregor Henze, and Zoltán Nagy. “CityLearn: Stan dardizing research in multi-agent reinforcement learning for demand response and urban energy management”. In: arXiv preprint arXiv:2012.10504 (2020). 
[310] José R Vázquez-Canteli and Zoltán Nagy. “Reinforcement learning for demand response: A review of algorithms and modeling techniques”. In: Applied energy 235 (2019), pp. 1072– 1089. 
[311] Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Janarthanan Rajendran, Richard L Lewis, Junhyuk Oh, Hado P van Hasselt, David Silver, and Satinder Singh. “Discovery of Useful Questions as Auxiliary Tasks”. In: Advances in Neural Information Processing Systems. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc., 2019. URL: https : / / proceedings . neurips . cc / paper/2019/file/10ff0b5e85e5b85cc3095d431d8c08b4-Paper.pdf. 
[312] Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado P van Hasselt, David Silver, and Satinder Singh. “Discovery of Options via Meta Learned Subgoals”. In: Advances in Neural Information Processing Systems. Ed. by M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan. Vol. 34. Curran Associates, Inc., 2021, pp. 29861–29873. URL: https://proceedings.neurips.cc/ paper/2021/file/fa246d0262c3925617b0c72bb20eeb1d-Paper.pdf. 
[313] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu koray, and Daan Wierstra. “Matching Networks for One Shot Learning”. In: Advances in Neural Information Processing Systems. 2016. 
[314] Risto Vuorio, Jacob Austin Beck, Gregory Farquhar, Jakob Nicolaus Foerster, and Shimon Whiteson. “No DICE: An Investigation of the Bias-Variance Tradeoff in Meta-Gradients”. In: Deep RL Workshop NeurIPS 2021. 2021. 
[315] Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph Lim. “Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation”. In: NeurIPS. 2019. 
[316] Homer Rich Walke, Jonathan Heewon Yang, Albert Yu, Aviral Kumar, Jedrzej Orbik, Avi Singh, and Sergey Levine. “Don’t Start From Scratch: Leveraging Prior Data to Automate Robotic Reinforcement Learning”. In: 6th Annual Conference on Robot Learning. 2022. URL: https://openreview.net/forum?id=WbdaYyDkNZL. 
[317] Michael Wan, Jian Peng, and Tanmay Gangwani. “Hindsight Foresight Relabeling for Meta Reinforcement Learning”. In: International Conference on Learning Representations. 2022. URL: https://openreview.net/forum?id=P7OVkHEoHOZ. 
[318] Haozhe Wang, Jiale Zhou, and Xuming He. “Learning Context-aware Task Reasoning for Efficient Meta Reinforcement Learning”. In: Proceedings of the 19th International Confer ence on Autonomous Agents and MultiAgent Systems. 2020, pp. 1440–1448. 
[319] Jane X. Wang, Michael King, Nicolas Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie Deck, Peter Choy, Mary Cassin, Malcolm Reynolds, H. Francis Song, Gavin Buttimore, David P. Reichert, Neil C. Rabinowitz, Loic Matthey, Demis Hassabis, Alexander Lerchner, and Matthew M. Botvinick. “Alchemy: A structured task distribution for meta-reinforcement learning”. In: NeurIPS (2021). 
[320] Jane X Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Demis Hassabis, and Matthew Botvinick. “Prefrontal cortex as a meta reinforcement learning system”. In: Nature neuroscience 21.6 (2018), pp. 860–868. 
[321] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. “Learning to reinforce ment learn”. In: arXiv preprint arXiv:1611.05763 (2016). 
74
[322] Qi Wang and Herke van Hoof. “Learning Expressive Meta-Representations with Mixture of Expert Neural Processes”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https: //openreview.net/forum?id=ju38DG3sbg6. 
[323] Qi Wang, Yiqin Lv, Yanghe Feng, Zheng Xie, and Jincai Huang. “A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm”. In: Advances in Neural Information Processing Systems (2023). 
[324] Qi Wang and Herke Van Hoof. “Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models and Amortized Policy Search”. In: Proceedings of the 39th International Conference on Machine Learning. 2022. 
[325] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. “Generalizing from a few examples: A survey on few-shot learning”. In: ACM computing surveys (csur) 53.3 (2020), pp. 1–34. 
[326] Zhenyi Wang, Yang Zhao, Ping Yu, Ruiyi Zhang, and Changyou Chen. “Bayesian Meta Sampling for Fast Uncertainty Adaptation”. In: International Conference on Learning Rep resentations. 2020. URL: https://openreview.net/forum?id=Bkxv90EKPB. 
[327] Luca Weihs, Unnat Jain, Iou-Jen Liu, Jordi Salvador, Svetlana Lazebnik, Aniruddha Kem bhavi, and Alex Schwing. “Bridging the Imitation Gap by Adaptive Insubordination”. In: Advances in Neural Information Processing Systems. Ed. by A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan. 2021. URL: https://openreview.net/forum?id= Wlx0DqiUTD_. 
[328] Lu Wen, Songan Zhang, H Eric Tseng, Baljeet Singh, Dimitar Filev, and Huei Peng. “Im proved Robustness and Safety for Pre-Adaptation of Meta Reinforcement Learning with Prior Regularization”. In: 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2022, pp. 8987–8994. 
[329] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen Schmid huber. “Natural evolution strategies”. In: The Journal of Machine Learning Research 15.1 (2014), pp. 949–980. 
[330] Ronald J Williams. “Simple statistical gradient-following algorithms for connectionist rein forcement learning”. In: Machine learning 8.3 (1992), pp. 229–256. 
[331] Honguk Woo, Gwangpyo Yoo, and Minjong Yoo. “Structure Learning-Based Task Decom position for Reinforcement Learning in Non-stationary Environments”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 8. 2022, pp. 8657–8665. 
[332] Bohan Wu, Feng Xu, Zhanpeng He, Abhi Gupta, and Peter K. Allen. “SQUIRL: Robust and Efficient Learning from Video Demonstration of Long-Horizon Robotic Manipulation Tasks”. In: IROS. 2020. 
[333] Yanqiu Wu, Xinyue Chen, Che Wang, Yiming Zhang, and Keith W. Ross. “Aggressive Q-Learning with Ensembles: Achieving Both High Sample Efficiency and High Asymp totic Performance”. In: Deep Reinforcement Learning Workshop NeurIPS 2022. 2022. URL: https://openreview.net/forum?id=L9MhPPvPFS. 
[334] Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. “Understanding Short-Horizon Bias in Stochastic Meta-Optimization”. In: International Conference on Learning Represen tations. 2018. URL: https://openreview.net/forum?id=H1MczcgR-. 
[335] Zhou Xian, Shamit Lal, Hsiao-Yu Tung, Emmanouil Antonios Platanios, and Katerina Fragkiadaki. “HyperDynamics: Meta-Learning Object and Agent Dynamics with Hyper networks”. In: International Conference on Learning Representations. 2021. URL: https: //openreview.net/forum?id=pHXfe1cOmA. 
[336] Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. “Few-shot goal inference for vi suomotor learning and planning”. In: Conference on Robot Learning. PMLR. 2018, pp. 40– 52. 
[337] Zheng Xiong, Luisa M Zintgraf, Jacob Austin Beck, Risto Vuorio, and Shimon White son. “On the Practical Consistency of Meta-Reinforcement Learning Algorithms”. In: Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems. 2021. URL: https://openreview.net/forum?id=xwQgKphwhFA. 
75
[338] Kelvin Xu, Ellis Ratner, Anca Dragan, Sergey Levine, and Chelsea Finn. “Learning a Prior over Intent via Meta-Inverse Reinforcement Learning”. In: Proceedings of the 36th Interna tional Conference on Machine Learning. 2019. 
[339] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. “Prompting decision transformer for few-shot policy generalization”. In: in ternational conference on machine learning. PMLR. 2022, pp. 24631–24645. 
[340] Zhongwen Xu, Hado P van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, and David Silver. “Meta-Gradient Reinforcement Learning with an Objective Discovered On line”. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 15254–15264. URL: https : / / proceedings . neurips . cc / paper / 2020 / file / ae3d525daf92cee0003a7f2d92c34ea3-Paper.pdf. 
[341] Zhongwen Xu, Hado P van Hasselt, and David Silver. “Meta-Gradient Reinforcement Learn ing”. In: Advances in Neural Information Processing Systems. Ed. by S. Bengio, H. Wal lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Asso ciates, Inc., 2018. URL: https : / / proceedings . neurips . cc / paper / 2018 / file / 2715518c875999308842e3455eda2fe3-Paper.pdf. 
[342] Liqi Yan, Dongfang Liu, Yaoxian Song, and Changbin Yu. “Multimodal Aggregation Ap proach for Memory Vision-Voice Indoor Navigation with Meta-Learning”. In: IROS. 2020. [343] Jiachen Yang, Ethan Wang, Rakshit Trivedi, Tuo Zhao, and Hongyuan Zha. “Adaptive Incen tive Design with Multi-Agent Meta-Gradient Reinforcement Learning”. In: AAMAS (2022). [344] Jiaqi Yang, Wei Hu, Jason D Lee, and Simon S Du. “Impact of representation learning in linear bandits”. In: arXiv preprint arXiv:2010.06531 (2020). 
[345] Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Jie Tan, and Chelsea Finn. “NoRML: No-Reward Meta Learning”. In: AAMAS (2019). 
[346] Haotian Ye, Xiaoyu Chen, Liwei Wang, and Simon Shaolei Du. “On the power of pre training for generalization in rl: Provable benefits and hardness”. In: International Confer ence on Machine Learning. PMLR. 2023, pp. 39770–39800. 
[347] Haiyan Yin, Jianda Chen, Sinno Jialin Pan, and Sebastian Tschiatschek. “Sequential gener ative exploration model for partially observable reinforcement learning”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. 12. 2021, pp. 10700–10708. 
[348] Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. “Bayesian Model-Agnostic Meta-Learning”. In: Advances in Neural In formation Processing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018. URL: https : / / proceedings . neurips . cc / paper / 2018 / file / e1021d43911ca2c1845910d84f40aeae-Paper.pdf. 
[349] Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. “Meta-Inverse Reinforcement Learning with Probabilistic Context Variables”. In: Advances in Neural Information Pro cessing Systems. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc., 2019. URL: https://proceedings. neurips.cc/paper/2019/file/30de24287a6d8f07b37c716ad51623a7-Paper.pdf. 
[350] Tianhe Yu, Chelsea Finn, Sudeep Dasari, Annie Xie, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. “One-Shot Imitation from Observing Humans via Domain-Adaptive Meta Learning”. In: Proceedings of Robotics: Science and Systems. Pittsburgh, Pennsylvania, 2018. DOI: 10.15607/RSS.2018.XIV.002. 
[351] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. “Meta-world: A benchmark and evaluation for multi-task and meta rein forcement learning”. In: Conference on Robot Learning. PMLR. 2020, pp. 1094–1100. 
[352] Wenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, and Sehoon Ha. “Learning fast adaptation with meta strategy optimization”. In: IEEE Robotics and Automation Letters 5.2 (2020), pp. 2950–2957. 
[353] Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. “Preparing for the Unknown: Learning a Universal Policy with Online System Identification”. In: Proceedings of Robotics: Science and Systems. Cambridge, Massachusetts, 2017. DOI: 10.15607/RSS.2017.XIII.048. 
76
[354] Haoqi Yuan and Zongqing Lu. “Robust task representations for offline meta-reinforcement learning via contrastive learning”. In: International Conference on Machine Learning. PMLR. 2022, pp. 25747–25759. 
[355] Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado van Hasselt, David Silver, and Satinder Singh. “A self-tuning actor-critic algorithm”. In: arXiv preprint arXiv:2002.12928 (2020). 
[356] Xinshi Zang, Huaxiu Yao, Guanjie Zheng, Nan Xu, Kai Xu, and Zhenhui Li. “Metalight: Value-based meta-reinforcement learning for traffic signal control”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. 01. 2020, pp. 1153–1160. 
[357] Hao Zhang and Zhen Kan. “Temporal logic guided meta q-learning of multiple tasks”. In: IEEE Robotics and Automation Letters 7.3 (2022), pp. 8194–8201. 
[358] Jin Zhang, Jianhao Wang, Hao Hu, Tong Chen, Yingfeng Chen, Changjie Fan, and Chongjie Zhang. “MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Explo ration”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 12600–12610. URL: https : / / proceedings . mlr . press / v139 / zhang21w.html. 
[359] Qi Zhang and Dingyang Chen. “A Meta-Gradient Approach to Learning Cooperative Multi Agent Communication Topology”. In: Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems. 2021. 
[360] Mandi Zhao, Pieter Abbeel, and Stephen James. “On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=mux7gn3g_3. 
[361] Tony Z Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan Schaal, and Sergey Levine. “Offline meta-reinforcement learning for industrial inser tion”. In: 2022 International Conference on Robotics and Automation (ICRA). IEEE. 2022, pp. 6386–6393. 
[362] Wenshuai Zhao, Jorge Peña Queralta, and Tomi Westerlund. “Sim-to-real transfer in deep reinforcement learning for robotics: a survey”. In: 2020 IEEE Symposium Series on Compu tational Intelligence (SSCI). IEEE. 2020, pp. 737–744. 
[363] Zihao Zhao, Anusha Nagabandi, Kate Rakelly, Chelsea Finn, and Sergey Levine. “MELD: Meta-Reinforcement Learning from Images via Latent State Models”. In: Conference on Robot Learning. PMLR. 2021, pp. 1246–1261. 
[364] Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado Van Has selt, David Silver, and Satinder Singh. “What can learned intrinsic rewards capture?” In: International Conference on Machine Learning. PMLR. 2020, pp. 11436–11446. 
[365] Zeyu Zheng, Junhyuk Oh, and Satinder Singh. “On Learning Intrinsic Rewards for Pol icy Gradient Methods”. In: Advances in Neural Information Processing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018. URL: https://proceedings.neurips.cc/paper/2018/ file/51de85ddd068f0bc787691d356176df9-Paper.pdf. 
[366] Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. “Watch, Try, Learn: Meta Learning from Demonstrations and Rewards”. In: ICLR. 2020. 
[367] Wenxuan Zhou, Lerrel Pinto, and Abhinav Gupta. “Environment Probing Interaction Poli cies”. In: International Conference on Learning Representations. 2019. URL: https : / / openreview.net/forum?id=ryl8-3AcFX. 
[368] Luisa M. Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. “Fast Context Adaptation via Meta-Learning”. In: ICLR (2019). 
[369] Luisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hof mann, and Shimon Whiteson. “Exploration in Approximate Hyper-State Space for Meta Re inforcement Learning”. In: International Conference on Machine Learning. PMLR. 2021, pp. 12991–13001. 
77
[370] Luisa Zintgraf, Sam Devlin, Kamil Ciosek, Shimon Whiteson, and Katja Hofmann. “Deep Interactive Bayesian Reinforcement Learning via Meta-Learning”. In: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems. 2021, pp. 1712–1714. 
[371] Luisa Zintgraf, Sebastian Schulze, Cong Lu, Leo Feng, Maximilian Igl, Kyriacos Shiarlis, Yarin Gal, Katja Hofmann, and Shimon Whiteson. “VariBAD: Variational Bayes-Adaptive Deep RL via Meta-Learning”. In: Journal of Machine Learning Research 22.289 (2021), pp. 1–39. 
[372] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hof mann, and Shimon Whiteson. “VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning”. In: International Conference on Learning Representation (ICLR). 2020. 
[373] Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, and Jun Zhu. “Learning task distribution reward shaping with meta-learning”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. 12. 2021, pp. 11210–11218. 
[374] Yayi Zou and Xiaoqi Lu. “Gradient-EM Bayesian Meta-Learning”. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Had sell, M.F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 20865– 20875. URL: https : / / proceedings . neurips . cc / paper / 2020 / file / ef48e3ef07e359006f7869b04fa07f5e-Paper.pdf. 
A List of venues surveyed 
This survey is primarily based on the meta-RL research presented in the following conferences and workshops for the years from 2017 to 2022: 
• International Conference on Learning Representations (ICLR) 
• Conference on Neural Information Processing Systems (NeurIPS) 
• International Conference on Machine Learning (ICML) 
• Autonomous Agents and Multiagent Systems (AAMAS) 
• Annual AAAI Conference on Artificial Intelligence (AAAI) 
• Conference on Robot Learning (CoRL) 
• Robotics: Science and Systems (RSS) 
• International Conference on Intelligent Robots and Systems (IROS) 
• NeurIPS Workshop on Meta-Learning 
• ICLR Workshop on Meta-Learning 
78
